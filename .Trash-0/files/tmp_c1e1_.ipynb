{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68edc8a0",
   "metadata": {},
   "source": [
    "# emotional tts quick c1 iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e115d61",
   "metadata": {},
   "source": [
    "## Preparing EmotionalTTS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57657a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the datasets\n",
    "\n",
    "# !(mkdir /DataEmotionalTTS && \\\n",
    "\n",
    "!(cd /DataEmotionalTTS && \\\n",
    "    wget https://www.openslr.org/resources/110/thorsten-emotional_v02.tgz && \\\n",
    "    tar -zxvf thorsten-emotional_v02.tgz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc75dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create manifests, normalization and phonemization\n",
    "\n",
    "# !(cd NeMoEmotionalTTS && \\\n",
    "#     python get_data.py \\\n",
    "\n",
    "!(python scripts/dataset_processing/tts/openslr_emotional/get_data.py \\\n",
    "        --data-root ../DataEmotionalTTS/ \\\n",
    "        --val-size 0.1 \\\n",
    "        --test-size 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4883a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2cc2cc4a34b961ef1657cc82dbd18875 does not exist for whisper category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phonemization\n",
    "## inside the phonemizer container\n",
    "\n",
    "from phonemizer.backend import EspeakBackend\n",
    "import json\n",
    "\n",
    "backend = EspeakBackend('de')\n",
    "unique_symbols = set()\n",
    "input_manifest_filepaths = [\"/DataEmotionalTTS/thorsten-emotional_v02/train_manifest\", \\\n",
    "                            \"/DataEmotionalTTS/thorsten-emotional_v02/test_manifest\", \\\n",
    "                            \"/DataEmotionalTTS/thorsten-emotional_v02/val_manifest\"]\n",
    "\n",
    "for input_manifest_filepath in input_manifest_filepaths:\n",
    "    output_manifest_filepath = input_manifest_filepath+\"_phonemes\"\n",
    "    records = []\n",
    "    n_text = []\n",
    "    with open(input_manifest_filepath + \".json\", \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            d = json.loads(line)\n",
    "            records.append(d)\n",
    "            n_text.append(d['normalized_text'])\n",
    "\n",
    "    phonemized = backend.phonemize(n_text)\n",
    "    for line in n_text:\n",
    "        for char in line:\n",
    "            unique_symbols.add(char)\n",
    "    new_records = []\n",
    "    for i in range(len(records)):\n",
    "        records[i][\"is_phoneme\"] = 0\n",
    "        new_records.append(records[i])\n",
    "        phoneme_record = records[i].copy()\n",
    "        phoneme_record[\"normalized_text\"] = phonemized[i]\n",
    "        phoneme_record[\"is_phoneme\"] = 1\n",
    "        new_records.append(phoneme_record)\n",
    "\n",
    "    with open(output_manifest_filepath + \".json\", \"w\") as f:\n",
    "        for r in new_records:\n",
    "            f.write(json.dumps(r) + '\\n')\n",
    "print(unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a174fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ''.join(unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[char for char in a if char not in \"ʊʃŋɜːɛɾəɪçɔøɡœɑÜ„1Q̃ɒʒÄɹÖʌθàó̈ðéɐáabcdefghijklmnopqrstuvwxyzäöüß\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /DataEmotionalTTS/thorsten-emotional_v02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset config\n",
    "!cat scripts/dataset_processing/tts/openslr_emotional/ds_conf/ds_for_fastpitch_align.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplementary data creation - emotional TTS (optional)\n",
    "!(HYDRA_FULL_ERROR=1 python scripts/dataset_processing/tts/extract_sup_data.py \\\n",
    "        --config-path /NeMoEmotionalTTS/scripts/dataset_processing/tts/openslr_emotional/ds_conf \\\n",
    "        --config-name ds_for_fastpitch_align.yaml \\\n",
    "        manifest_filepath=/DataEmotionalTTS/thorsten-emotional_v02/train_manifest_phonemes.json \\\n",
    "        sup_data_path=/DataEmotionalTTS/thorsten-emotional_v02/phonemes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21713250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PITCH_MEAN=169.59390258789062, PITCH_STD=103.64842224121094\n",
    "# PITCH_MIN=65.4063949584961, PITCH_MAX=2093.004638671875"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9da02",
   "metadata": {},
   "source": [
    "## Preparing NeutralTTS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8782535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the manifest files\n",
    "## path corrections :%s/github\\/datasets/DataEmotionalTTS/g\n",
    "## add emotion_id =0 :%s/, \"is_p/, \"emotion_id\": 0, \"is_p/g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e354ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplementary data creation - Neutral TTS (optional)\n",
    "!(HYDRA_FULL_ERROR=1 PYTHONPATH=. python scripts/dataset_processing/tts/extract_sup_data.py \\\n",
    "        --config-path /NeMoEmotionalTTS/scripts/dataset_processing/tts/openslr/ds_conf \\\n",
    "        --config-name ds_for_fastpitch_align.yaml \\\n",
    "        manifest_filepath=/DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/train_manifest_phonemes.json \\\n",
    "        sup_data_path=/DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/phonemes6/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PITCH_MEAN=132.55270385742188, PITCH_STD=37.61381912231445\n",
    "# PITCH_MIN=65.4063949584961, PITCH_MAX=2093.004638671875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67660d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-train fastpitch (with speaker and emotional embedding) on German neutral tts\n",
    "!(CUDA_VISIBLE_DEVICES=0 HYDRA_FULL_ERROR=1 PYTHONPATH=. python examples/tts/fastpitch.py --config-path conf/de --config-name fastpitch_align_22050 \\\n",
    "    name=\"c1e1-FastPitch-pretrain\" \\\n",
    "    model.train_ds.dataloader_params.batch_size=32 \\\n",
    "    model.validation_ds.dataloader_params.batch_size=32 \\\n",
    "    train_dataset=/DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/train_manifest_phonemes.json \\\n",
    "    validation_datasets=/DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/val_manifest_phonemes.json \\\n",
    "    sup_data_path=/DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/phonemes6/ \\\n",
    "    whitelist_path=nemo_text_processing/text_normalization/de/data/whitelist.tsv \\\n",
    "    exp_manager.exp_dir=/resultGermanTTS \\\n",
    "    trainer.max_epochs=100 \\\n",
    "    pitch_mean=132.524658203125 \\\n",
    "    pitch_std=37.389366149902344 \\\n",
    "    +exp_manager.create_wandb_logger=true \\\n",
    "    +exp_manager.wandb_logger_kwargs.name=\"072720221625_c1e1-FastPitch-pretrain\" \\\n",
    "    +exp_manager.wandb_logger_kwargs.project=\"EmotionalTTS\" | tee -a 072720221625_c1e1-FastPitch-pretrain.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e096df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.7G\n",
      "drwxr-xr-x 2 root root 5.5G Jul 29 04:30  .\n",
      "-rw-r--r-- 1 root root 524M Jul 29 04:30 'c1e1-FastPitch-pretrain--v_loss=0.7493-epoch=74-last.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 04:30 'c1e1-FastPitch-pretrain--v_loss=0.7493-epoch=74.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 04:19 'c1e1-FastPitch-pretrain--v_loss=0.7484-epoch=69.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 04:07 'c1e1-FastPitch-pretrain--v_loss=0.7698-epoch=64.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 03:56 'c1e1-FastPitch-pretrain--v_loss=0.7690-epoch=59.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 03:45 'c1e1-FastPitch-pretrain--v_loss=0.7913-epoch=54.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 03:33 'c1e1-FastPitch-pretrain--v_loss=0.7987-epoch=49.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 03:22 'c1e1-FastPitch-pretrain--v_loss=0.8191-epoch=44.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 03:11 'c1e1-FastPitch-pretrain--v_loss=0.8506-epoch=39.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 02:59 'c1e1-FastPitch-pretrain--v_loss=0.8780-epoch=34.ckpt'\n",
      "-rw-r--r-- 1 root root 524M Jul 29 02:48 'c1e1-FastPitch-pretrain--v_loss=0.9233-epoch=29.ckpt'\n",
      "drwxr-xr-x 3 root root  36M Jul 29 01:51  ..\n"
     ]
    }
   ],
   "source": [
    "# monitor performance of pre-trained fastpitch\n",
    "!ls -alht /resultEmotionalTTS/c1e1-FastPitch-pretrain/2022-07-29_01-38-27/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e82e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_test_path = \"/resultEmotionalTTS/c1e1-FastPitch-pretrain/2022-07-29_01-38-27/checkpoints/c1e1-FastPitch-pretrain--v_loss=0.7493-epoch=74.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0921e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-29 04:37:33 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "      warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n",
      "    \n",
      "[NeMo W 2022-07-29 04:37:36 experimental:27] Module <class 'nemo.collections.tts.torch.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fe6917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-29 04:38:24 tokenize_and_classify:81] Creating ClassifyFst grammars. This might take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-29 04:38:51 modelPT:149] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/train_manifest_phonemes.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/phonemes6/\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - emotion_id\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 14\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 132.524658203125\n",
      "      pitch_std: 37.389366149902344\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 32\n",
      "      num_workers: 12\n",
      "    \n",
      "[NeMo W 2022-07-29 04:38:51 modelPT:156] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/val_manifest_phonemes.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /DataEmotionalTTS/openslr-95-german-neutral-tts/thorsten-de/phonemes6/\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - emotion_id\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 14\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 132.524658203125\n",
      "      pitch_std: 37.389366149902344\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 32\n",
      "      num_workers: 12\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-29 04:38:51 features:200] PADDING: 1\n"
     ]
    }
   ],
   "source": [
    "fpckpt = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_test_path).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141a7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-29 04:39:32 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fpckpt.save_to('/resultEmotionalTTS/c1e1-FastPitch-pretrain/2022-07-29_01-38-27/checkpoints/c1e1-FastPitch-pretrain--v_loss=0.7493-epoch=74.nemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ade33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune the model trained in the last cell\n",
    "!(CUDA_VISIBLE_DEVICES=0 python examples/tts/fastpitch_finetune.py --config-path conf/de --config-name fastpitch_align_v1.05.yaml \\\n",
    "    name=\"c1e1-FastPitch-finetune\" \\\n",
    "    model.train_ds.dataloader_params.batch_size=32 \\\n",
    "    model.validation_ds.dataloader_params.batch_size=32 \\\n",
    "    train_dataset=/DataEmotionalTTS/thorsten-emotional_v02/train_manifest_phonemes.json \\\n",
    "    validation_datasets=/DataEmotionalTTS/thorsten-emotional_v02/val_manifest_phonemes.json \\\n",
    "    sup_data_path=/DataEmotionalTTS/thorsten-emotional_v02/train_manifest_phonemes.json \\\n",
    "    whitelist_path=nemo_text_processing/text_normalization/de/data/whitelist.tsv \\\n",
    "    exp_manager.exp_dir=/resultEmotionalTTS \\\n",
    "    +init_from_nemo_model=/DataEmotionalTTS/tts_de_fastpitchhifigan_v1.10.0/tts_de_fastpitch_align.nemo \\\n",
    "    +trainer.max_steps=1000 ~trainer.max_epochs \\\n",
    "    trainer.check_val_every_n_epoch=25 \\\n",
    "    model.n_speakers=1 \\\n",
    "    model.n_emotions=8 \\\n",
    "    model.optim.lr=2e-4 \\\n",
    "    ~model.optim.sched model.optim.name=adamw trainer.devices=1 trainer.strategy=null \\\n",
    "    pitch_mean=169.59390258789062 \\\n",
    "    pitch_std=103.64842224121094 \\\n",
    "    +exp_manager.create_wandb_logger=true \\\n",
    "    +exp_manager.wandb_logger_kwargs.name=\"072620221311_c1e1-FastPitch-finetune\" \\\n",
    "    +exp_manager.wandb_logger_kwargs.project=\"EmotionalTTS\" | tee -a 072620221311_c1e1-FastPitch-pretrain.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab01ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33853e93",
   "metadata": {},
   "source": [
    "## Generate syn mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b73fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630870e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-08-16 18:28:29 tokenize_and_classify:81] Creating ClassifyFst grammars. This might take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-08-16 18:28:49 modelPT:149] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /DataEmotionalTTS/thorsten-emotional_v02/train_manifest_phonemes.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /DataEmotionalTTS/thorsten-emotional_v02/phonemes/\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - emotion_id\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 14\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 169.59390258789062\n",
      "      pitch_std: 103.64842224121094\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 32\n",
      "      num_workers: 12\n",
      "    \n",
      "[NeMo W 2022-08-16 18:28:49 modelPT:156] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /DataEmotionalTTS/thorsten-emotional_v02/val_manifest_phonemes.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /DataEmotionalTTS/thorsten-emotional_v02/phonemes/\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - emotion_id\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 14\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 169.59390258789062\n",
      "      pitch_std: 103.64842224121094\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 32\n",
      "      num_workers: 2\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-08-16 18:28:49 features:200] PADDING: 1\n",
      "[NeMo I 2022-08-16 18:28:50 save_restore_connector:243] Model FastPitchModel was successfully restored from /DataEmotionalTTS/staging-c1e1-FastPitch-finetune/c1e1-FastPitch-finetune.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "[NeMo W 2022-08-16 18:28:59 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n",
      "[NeMo W 2022-08-16 18:29:17 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n",
      "[NeMo W 2022-08-16 18:29:59 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n",
      "[NeMo W 2022-08-16 18:31:24 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n",
      "[NeMo W 2022-08-16 18:31:45 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n",
      "[NeMo W 2022-08-16 18:31:58 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n",
      "[NeMo W 2022-08-16 18:33:01 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n",
      "[NeMo W 2022-08-16 18:33:33 tts_tokenizers:144] Text: [viː ʃɛtsən ziː diː ʃɑ̃sən aɪn zoː aɪnən pɾoːtsɛs tsuː ɡəvɪnən ] contains unknown char: [̃]. Symbol will be skipped.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nemo.collections.tts.torch.helpers import BetaBinomialInterpolator\n",
    "\n",
    "folder_name = \"synmels999\"\n",
    "fastpitch_model_path = \"/DataEmotionalTTS/staging-c1e1-FastPitch-finetune/c1e1-FastPitch-finetune.nemo\"\n",
    "dataset_parts = [\"test_manifest_phonemes\", \"val_manifest_phonemes\", \"train_manifest_phonemes\"]\n",
    "dataset_base_path = \"/DataEmotionalTTS/thorsten-emotional_v02/\"\n",
    "\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "if \".nemo\" in fastpitch_model_path:\n",
    "    spec_model = FastPitchModel.restore_from(fastpitch_model_path).eval().cuda()\n",
    "else:\n",
    "    spec_model = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_model_path).eval().cuda()\n",
    "\n",
    "spec_model.eval().cuda()\n",
    "    \n",
    "def load_wav(audio_file):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "    return samples.transpose()\n",
    "    \n",
    "for dataset_part in dataset_parts:\n",
    "    # Get records from the manifest\n",
    "    manifest_path = f\"{dataset_base_path}{dataset_part}.json\"\n",
    "    records = []\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            records.append(json.loads(line))\n",
    "\n",
    "    beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "\n",
    "    spec_model.eval()\n",
    "    device = spec_model.device\n",
    "\n",
    "    save_dir = Path(f\"{dataset_base_path}{folder_name}/{dataset_part}\")\n",
    "\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
    "    for i, r in enumerate(records):\n",
    "        audio = load_wav(r[\"audio_filepath\"])\n",
    "\n",
    "        audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "        audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        # Again, our finetuned FastPitch model doesn't use multiple speakers,\n",
    "        # but we keep the code to support it here for reference\n",
    "        if spec_model.fastpitch.speaker_emb is not None and \"speaker\" in r:\n",
    "            speaker = torch.tensor([r['speaker']]).to(device)\n",
    "        else:\n",
    "            speaker = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if \"normalized_text\" in r:\n",
    "                text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
    "            else:\n",
    "                text = spec_model.parse(r['text'])\n",
    "\n",
    "            text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "\n",
    "            # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
    "            attn_prior = torch.from_numpy(\n",
    "                beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "            ).unsqueeze(0).to(text.device)\n",
    "\n",
    "            spectrogram = spec_model.forward(\n",
    "                text=text, \n",
    "                input_lens=text_len, \n",
    "                spec=spect, \n",
    "                mel_lens=spect_len, \n",
    "                attn_prior=attn_prior,\n",
    "                speaker=speaker,\n",
    "              )[0]\n",
    "\n",
    "        save_path = save_dir / f\"mel_{i}.npy\"\n",
    "        np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "        r[\"mel_filepath\"] = str(save_path)\n",
    "\n",
    "        hifigan_manifest_path = f\"{dataset_base_path}{folder_name}/hifigan_{dataset_part}_ft.json\"\n",
    "\n",
    "        with open(hifigan_manifest_path, \"w\") as f:\n",
    "            for r in records:\n",
    "                f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2be85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
