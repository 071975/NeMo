{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f21c2ff-88db-4fc3-a379-dc990aaa4571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-18 10:05:29 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-07-18 10:05:29 experimental:27] Module <class 'nemo.collections.tts.torch.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import time\n",
    "import random\n",
    "import librosa\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from nemo.collections.tts.torch.helpers import BetaBinomialInterpolator\n",
    "from nemo.collections.tts.models.base import SpectrogramGenerator\n",
    "from nemo.collections.tts.models import FastPitchModel, MixerTTSModel\n",
    "from nemo.collections.asr.parts.preprocessing.segment import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b366a2c-bb17-4e3d-8e56-5c5c1a1ee887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25364 512\n"
     ]
    }
   ],
   "source": [
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "train = list(json_reader('/home/chsieh/manifest/mtts-dataset/train-100.json'))\n",
    "val = list(json_reader('/home/chsieh/manifest/mtts-dataset/val-100.json'))\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bd5485-99b4-41ff-9157-f4d579861208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-18 10:05:35 tokenize_and_classify:87] Creating ClassifyFst grammars.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-18 10:05:58 experimental:27] Module <class 'nemo.collections.tts.torch.g2ps.IPAG2P'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-07-18 10:05:58 g2ps:86] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2022-07-18 10:05:58 modelPT:149] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/mtts-dataset/train-100.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /raid/speech_sup/fastpitch_sup_data_folder\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: 20\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.4063949584961\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 170.99537658691406\n",
      "      pitch_std: 76.23028564453125\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 16\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2022-07-18 10:05:58 modelPT:156] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/mtts-dataset/val-100.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /raid/speech_sup/fastpitch_sup_data_folder\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: 20\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.4063949584961\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 170.99537658691406\n",
      "      pitch_std: 76.23028564453125\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 16\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-18 10:05:58 features:200] PADDING: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastPitchModel(\n",
       "  (mel_loss): MelLoss()\n",
       "  (pitch_loss): PitchLoss()\n",
       "  (duration_loss): DurationLoss()\n",
       "  (aligner): AlignmentEncoder(\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (forward_sum_loss): ForwardSumLoss(\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       "  (bin_loss): BinLoss()\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (fastpitch): FastPitchModule(\n",
       "    (encoder): FFTransformerEncoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (word_emb): Embedding(115, 384, padding_idx=112)\n",
       "    )\n",
       "    (decoder): FFTransformerDecoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm(\n",
       "              (weight): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (bias): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (duration_predictor): TemporalPredictor(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (pitch_predictor): TemporalPredictor(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (aligner): AlignmentEncoder(\n",
       "      (softmax): Softmax(dim=3)\n",
       "      (log_softmax): LogSoftmax(dim=3)\n",
       "      (key_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (query_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): ReLU()\n",
       "        (4): ConvNorm(\n",
       "          (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (speaker_emb): Embedding(100, 384)\n",
       "    (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_wav(audio_file, target_sr=None):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "        sample_rate = f.samplerate\n",
    "    if target_sr is not None and sample_rate != target_sr:\n",
    "        samples = librosa.core.resample(samples, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    return samples.transpose()\n",
    "\n",
    "\n",
    "# ckpt_file = '/home/chsieh/chengping-ws/fastpitch/completed/emb-all/FastPitch/2022-07-08_07-44-15/checkpoints/FastPitch--v_loss=0.8746-epoch=199.ckpt'\n",
    "ckpt_file = '/home/chsieh/chengping-ws/fastpitch/completed/emb-all-cln/FastPitch/2022-07-13_21-39-05/checkpoints/FastPitch--v_loss=0.8200-epoch=479.ckpt'\n",
    "spec_model = FastPitchModel.load_from_checkpoint(ckpt_file)\n",
    "spec_model.to('cuda:0')\n",
    "spec_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c55b6d6-78bd-422d-b0eb-4d922194bb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "device = spec_model.device\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31738e9e-4cc2-4130-83c7-fb2716a053d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'Libritts-FP-embadd-cln-mels-22k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64675965-23da-4935-b232-c89160acb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(f\"/home/chsieh/{save_name}/\")\n",
    "save_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebed9afc-a3c4-484f-bc5d-f13631237aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_manifest = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dd41d1-d190-4bf7-a8b2-6514edbe4018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0 in 1.722810983657837 sec\n",
      "done 500 in 74.01316928863525 sec\n",
      "done 1000 in 139.46378183364868 sec\n",
      "done 1500 in 203.0050597190857 sec\n",
      "done 2000 in 266.296373128891 sec\n",
      "done 2500 in 326.1671657562256 sec\n",
      "done 3000 in 386.45816016197205 sec\n",
      "done 3500 in 447.59197425842285 sec\n",
      "done 4000 in 505.961065530777 sec\n",
      "done 4500 in 566.7194142341614 sec\n",
      "done 5000 in 626.1484844684601 sec\n",
      "done 5500 in 683.803308725357 sec\n",
      "done 6000 in 743.7966949939728 sec\n",
      "done 6500 in 808.656904220581 sec\n",
      "done 7000 in 870.1506638526917 sec\n",
      "done 7500 in 930.0228970050812 sec\n",
      "done 8000 in 987.2079255580902 sec\n",
      "done 8500 in 1046.0684022903442 sec\n",
      "done 9000 in 1107.9700260162354 sec\n",
      "done 9500 in 1172.7521977424622 sec\n",
      "done 10000 in 1232.4664885997772 sec\n",
      "done 10500 in 1293.0866334438324 sec\n",
      "done 11000 in 1351.3352448940277 sec\n",
      "done 18000 in 2167.26926612854 sec\n",
      "done 18500 in 2223.825672388077 sec\n",
      "done 19000 in 2281.038801431656 sec\n",
      "done 19500 in 2339.4216430187225 sec\n",
      "done 20000 in 2396.7035009860992 sec\n",
      "done 20500 in 2456.5404963493347 sec\n",
      "done 21000 in 2518.0600638389587 sec\n",
      "done 21500 in 2576.1027748584747 sec\n",
      "done 22000 in 2638.945410013199 sec\n",
      "done 22500 in 2703.450191259384 sec\n",
      "done 23000 in 2765.427059173584 sec\n",
      "done 23500 in 2824.9997692108154 sec\n",
      "done 24000 in 2895.514630794525 sec\n",
      "done 24500 in 2965.031177043915 sec\n",
      "done 25000 in 3036.4257526397705 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i, r in enumerate(cur_manifest):\n",
    "    save_path = save_dir / f\"mel_train_{i}.npy\"\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        r[\"mel_filepath\"] = str(save_path)\n",
    "        continue\n",
    "        \n",
    "    audio = load_wav(r[\"audio_filepath\"], 22050)\n",
    "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Again, our finetuned FastPitch model doesn't use multiple speakers,\n",
    "    # but we keep the code to support it here for reference\n",
    "    if spec_model.fastpitch.speaker_emb is not None and \"speaker\" in r:\n",
    "       speaker = torch.tensor([r['speaker']]).to(device)\n",
    "    else:\n",
    "       speaker = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if \"normalized_text\" in r:\n",
    "           text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
    "        else:\n",
    "           text = spec_model.parse(r['text'])\n",
    "        \n",
    "        text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "        spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "\n",
    "        # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
    "        attn_prior = torch.from_numpy(\n",
    "         beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "        ).unsqueeze(0).to(text.device)\n",
    "            \n",
    "        spectrogram = spec_model.forward(\n",
    "         text=text, \n",
    "         input_lens=text_len, \n",
    "         spec=spect, \n",
    "         mel_lens=spect_len, \n",
    "         attn_prior=attn_prior,\n",
    "         speaker=speaker,\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "        r[\"mel_filepath\"] = str(save_path)\n",
    "        \n",
    "    if i%500 == 0:\n",
    "        tm = time.time() - start_time\n",
    "        print(\"done {} in {} sec\".format(i, tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3916922-24d6-41dc-bcb3-7e40eaa688bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan_manifest_path = f\"/home/chsieh/manifest/mel-dataset/{save_name}\"\n",
    "os.makedirs(hifigan_manifest_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(hifigan_manifest_path, 'hifigan_train.json'), \"w\") as f:\n",
    "    for r in cur_manifest:\n",
    "        f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96101997-f35d-4113-a992-f77b94f3c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_manifest = val\n",
    "start_time = time.time()\n",
    "for i, r in enumerate(cur_manifest):\n",
    "    save_path = save_dir / f\"mel_val_{i}.npy\"\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        r[\"mel_filepath\"] = str(save_path)\n",
    "        continue\n",
    "        \n",
    "    audio = load_wav(r[\"audio_filepath\"], 22050)\n",
    "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Again, our finetuned FastPitch model doesn't use multiple speakers,\n",
    "    # but we keep the code to support it here for reference\n",
    "    if spec_model.fastpitch.speaker_emb is not None and \"speaker\" in r:\n",
    "       speaker = torch.tensor([r['speaker']]).to(device)\n",
    "    else:\n",
    "       speaker = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if \"normalized_text\" in r:\n",
    "           text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
    "        else:\n",
    "           text = spec_model.parse(r['text'])\n",
    "        \n",
    "        text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "        spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "\n",
    "        # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
    "        attn_prior = torch.from_numpy(\n",
    "         beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "        ).unsqueeze(0).to(text.device)\n",
    "            \n",
    "        spectrogram = spec_model.forward(\n",
    "         text=text, \n",
    "         input_lens=text_len, \n",
    "         spec=spect, \n",
    "         mel_lens=spect_len, \n",
    "         attn_prior=attn_prior,\n",
    "         speaker=speaker,\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "        np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "        r[\"mel_filepath\"] = str(save_path)\n",
    "        \n",
    "    if i%500 == 0:\n",
    "        tm = time.time() - start_time\n",
    "        print(\"done {} in {} sec\".format(i, tm))\n",
    "        \n",
    "hifigan_manifest_path = f\"/home/chsieh/manifest/mel-dataset/{save_name}\"\n",
    "os.makedirs(hifigan_manifest_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(hifigan_manifest_path, 'hifigan_val.json'), \"w\") as f:\n",
    "    for r in cur_manifest:\n",
    "        f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bdf357f-0d42-4d04-9830-53250169eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb22dd8-6c60-4591-a77d-8293dfc7ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan_manifest_path = f\"/home/chsieh/chengping-ws/mel-dataset/{save_name}\"\n",
    "os.makedirs(hifigan_manifest_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e52f4098-e9db-4a2b-895c-0f9d6cfa561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f'/home/chsieh/manifest/mel-dataset/{save_name}/hifigan_train.json'\n",
    "dataset_path_ws = f'/home/chsieh/chengping-ws/mel-dataset/{save_name}/hifigan_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "416aee7a-9766-4ca1-b95f-300a9c5a89b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_path) as f:\n",
    "    dataset = [eval(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2ad08fb-9b87-4691-9622-96cd9eb7ec0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_filepath': '/data/speech/LibriTTS/LibriTTS/train-clean-360/8410/278217/8410_278217_000100_000004.wav',\n",
       " 'duration': 3.730083,\n",
       " 'text': 'I have not yet recovered from the shock of that horrible business at Winchester.',\n",
       " 'speaker': 32,\n",
       " 'old_speaker_id': 8410,\n",
       " 'mel_filepath': '/data/speech/Libritts-FP-embadd-cln-mels-22k/mel_train_0.npy'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d14daeba-1a18-4b14-ba13-7f1f43bf9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    d['audio_filepath'] = d['audio_filepath'].replace('/raid/speech', '/data/speech')\n",
    "    d['mel_filepath'] = d['mel_filepath'].replace('/home/chsieh', '/data/speech')\n",
    "\n",
    "with open(dataset_path, 'w') as fout:\n",
    "    for m in dataset: fout.write(json.dumps(m) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbedc415-d2b4-409c-8bd9-b450294bdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    d['audio_filepath'] = d['audio_filepath'].replace('/data/speech', '/raid/speech')\n",
    "    d['mel_filepath'] = d['mel_filepath'].replace('/data/speech', '/raid/speech_mel')\n",
    "\n",
    "with open(dataset_path_ws, 'w') as fout:\n",
    "    for m in dataset: fout.write(json.dumps(m) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905917d-2a93-48fe-b523-04caa9125b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "nemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
