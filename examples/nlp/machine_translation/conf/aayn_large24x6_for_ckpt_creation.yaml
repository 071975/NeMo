name: AttentionIsAllYouNeed
do_training: True # set to False if only preprocessing data
do_testing: False # set to True to run evaluation on test data after training
nemo_file: null

model:
  train_ds:
    src_file_name: /home/apeganov/en_de_shuffled_4M/train.filtered.tokenized.en
    tgt_file_name: /home/apeganov/en_de_shuffled_4M/train.filtered.de
    use_tarred_dataset: false
    tar_files: null
    metadata_file: null
    lines_per_dataset_fragment: 1000000
    num_batches_per_tarfile: 100
    shard_strategy: scatter
    tokens_in_batch: 512
    clean: true
    max_seq_length: 512
    min_seq_length: 1
    cache_ids: false
    cache_data_per_node: false
    use_cache: false
    shuffle: true
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8
  validation_ds:
    src_file_name: ~/en_de_shuffled_4M/IWSLT19/en.tokenized
    tgt_file_name: ~/en_de_shuffled_4M/IWSLT19/de.tokenized
    use_tarred_dataset: false
    tar_files: null
    metadata_file: null
    lines_per_dataset_fragment: 1000000
    num_batches_per_tarfile: 1000
    shard_strategy: scatter
    tokens_in_batch: 512
    clean: false
    max_seq_length: 512
    min_seq_length: 1
    cache_ids: false
    cache_data_per_node: false
    use_cache: false
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8
  test_ds:
    src_file_name: ~/en_de_shuffled_4M/IWSLT19/en.tokenized
    tgt_file_name: ~/en_de_shuffled_4M/IWSLT19/de.tokenized
    use_tarred_dataset: false
    tar_files: null
    metadata_file: null
    lines_per_dataset_fragment: 1000000
    num_batches_per_tarfile: 1000
    shard_strategy: scatter
    tokens_in_batch: 512
    clean: false
    max_seq_length: 512
    min_seq_length: 1
    cache_ids: false
    cache_data_per_node: false
    use_cache: false
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8
  optim:
    name: adam
    lr: 0.0004
    betas:
    - 0.9
    - 0.98
    weight_decay: 0.0
    sched:
      name: InverseSquareRootAnnealing
      min_lr: 0.0
      last_epoch: -1
      warmup_steps: 30000
      warmup_ratio: null
  encoder_tokenizer:
    library: yttm
    tokenizer_model: /home/apeganov/checkpoints/wmt21_en_de_backtranslated_24x6_averaged_finetuned_13.1.21/fcf69a9198864a77b936bb6c73c9f329_shared_tokenizer.32000.BPE.model
    vocab_size: null
    bpe_dropout: null
    vocab_file: null
    special_tokens: null
    training_sample_size: null
  decoder_tokenizer:
    library: yttm
    tokenizer_model: /home/apeganov/checkpoints/wmt21_en_de_backtranslated_24x6_averaged_finetuned_13.1.21/071dff1d034e466f935ef6349ede7648_shared_tokenizer.32000.BPE.model
    vocab_size: null
    bpe_dropout: null
    vocab_file: null
    special_tokens: null
    training_sample_size: null
  encoder:
    library: nemo
    model_name: null
    pretrained: false
    max_sequence_length: 512
    num_token_types: 0
    embedding_dropout: 0.1
    learn_positional_encodings: false
    hidden_size: 1024
    num_layers: 24
    inner_size: 4096
    num_attention_heads: 16
    ffn_dropout: 0.1
    attn_score_dropout: 0.1
    attn_layer_dropout: 0.1
    hidden_act: relu
    mask_future: false
    pre_ln: true
  decoder:
    library: nemo
    model_name: null
    pretrained: false
    max_sequence_length: 512
    num_token_types: 0
    embedding_dropout: 0.1
    learn_positional_encodings: false
    hidden_size: 1024
    inner_size: 4096
    num_layers: 6
    num_attention_heads: 16
    ffn_dropout: 0.1
    attn_score_dropout: 0.1
    attn_layer_dropout: 0.1
    hidden_act: relu
    pre_ln: true
  head:
    num_layers: 1
    activation: relu
    log_softmax: true
    dropout: 0.0
    use_transformer_init: true
  num_val_examples: 3
  num_test_examples: 3
  max_generation_delta: 6
  label_smoothing: 0.1
  beam_size: 4
  len_pen: 0.6
  src_language: en
  tgt_language: de
  find_unused_parameters: true
  shared_tokenizer: true
  preproc_out_dir: null
  target: nemo.collections.nlp.models.machine_translation.mt_enc_dec_model.MTEncDecModel

trainer:
  gpus: 2
  num_nodes: 1
  max_epochs: 10
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  checkpoint_callback: False
  logger: False
  log_every_n_steps: 50  # Interval of logging.
  val_check_interval: 500

exp_manager:
  name: AAYNLarge24x6
  create_checkpoint_callback: true  # Whether you want exp_manager to create a model checkpoint callback
  create_wandb_logger: true
  wandb_logger_kwargs:
    name: finetuning_machine_translation_for_ASR_errors
    project: autoregressive_punctuation_capitalization
    entity: anton_peganov
  checkpoint_callback_params:
    save_best_model: true
    monitor: val_sacreBLEU
    mode: max