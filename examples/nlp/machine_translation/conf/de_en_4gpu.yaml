# Config file for training left-to-right Transformer language model
name: &name TransformerMT

trainer:
  gpus: 4
  num_nodes: 1
  max_epochs: 50
  amp_level: O2 # O1/O2 for mixed precision
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  distributed_backend: ddp
  checkpoint_callback: False
  logger: False
  log_every_n_steps: 50  # Interval of logging.
  val_check_interval: 0.25  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations

model:
  test_checkpoint_path: null
  machine_translation:
    tokenizer:
      tokenizer_name: yttm
      tokenizer_model: /raid2/NMT/wmt16_de_en/tokenizer.BPE.16384
    hidden_size: 512
    num_layers: 6
    num_attn_heads: 8
    inner_size: 2048
    max_seq_length: 600  # maximum allowed length in embedding layer
    embedding_dropout: 0.1
    ffn_dropout: 0.1
    attn_score_dropout: 0.1
    attn_layer_dropout: 0.1
    label_smoothing: 0.1
    beam_size: 4
    len_pen: 0.6
    max_generation_delta: 50

  train_ds:
    src_file_name: /raid2/NMT/wmt16_de_en/train.clean.de.shuffled
    tgt_file_name: /raid2/NMT/wmt16_de_en/train.clean.en.shuffled
    tokens_in_batch: 12500  # This is limit per gpu. Total limit is 50000
    clean: true
    max_seq_length: 512
    shuffle: true
    num_samples: -1 # number of samples to be considered, -1 means all the dataset
    drop_last: false # drops the last last batch if it is smaller than the batch size
    pin_memory: true # enables pin_memory feature of the data loaders
    num_workers: 8 # number of workers for data loaders

  validation_ds:
    src_file_name: /raid2/NMT/wmt16_de_en/wmt14-en-de.ref # path to file with validation data
    tgt_file_name: /raid2/NMT/wmt16_de_en/wmt14-en-de.src
    tokens_in_batch: 512
    shuffle: false
    num_samples: -1 # number of samples to be considered, -1 means all the dataset
    predict_last_k: 0
    drop_last: false # drops the last last batch if it is smaller than the batch size
    pin_memory: true # enables pin_memory feature of the data loaders
    num_workers: 8 # number of workers for data loaders

  optim:
    name: adam
    lr: 7e-4
    betas: [0.9, 0.98]
    weight_decay: 0

    sched:
      name: InverseSquareRootAnnealing
      warmup_steps: null
      warmup_ratio: 0.04
      last_epoch: -1

      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

exp_manager:
  exp_dir: NMT  # where to store logs and checkpoints
  name: *name  # name of experiment
  create_tensorboard_logger: True
  create_checkpoint_callback: True

