name: AttentionIsAllYouNeed
do_training: True # set to False if only preprocessing data
do_testing: True # set to True to run evaluation on test data after training

model:
  beam_size: 4
  len_pen: 0.6
  multilingual: False
  max_generation_delta: 100
  label_smoothing: 0.0
  shared_tokenizer: False # train tokenizer model across src and tgt train data
  preproc_out_dir: preprocessed_data # path to store data preprocessing outputs
  src_language: 'en'
  tgt_language: 'en'
  use_decoder_tips: false
  sum_replacement_with_original_embeddings: true
  detach_decoder_tips: false
  tgt_character_vocabulary: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/cross_char_vocabulary.txt

  train_ds:
    src_file_name: null
    tgt_file_name: null
    tokens_in_batch: 18000
    use_tarred_dataset: true
    metadata_file: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/train_cross_tarred_25000/metadata.tokens.25000.json
    clean: false
    max_seq_length: 4096
    shuffle: true
    drop_last: false
    pin_memory: false
    num_workers: 8

  validation_ds:
    src_file_name: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/IWSLT_tst2019/input.txt
    tgt_file_name: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/IWSLT_tst2019/cross_labels.txt
    tokens_in_batch: 2048
    clean: false
    max_seq_length: 4096
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8
    add_src_num_words_to_batch: true

  test_ds:
    src_file_name: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/IWSLT_tst2019/input.txt
    tgt_file_name: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/IWSLT_tst2019/cross_labels.txt
    tokens_in_batch: 2048
    clean: false
    max_seq_length: 4096
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8
    add_src_num_words_to_batch: true

  optim:
    name: adam
    lr: 0.0004
    betas:
      - 0.9
      - 0.98
    weight_decay: 0.0
    sched:
      name: InverseSquareRootAnnealing
      min_lr: 0.0
      last_epoch: -1
      warmup_ratio: 0.1

  encoder_tokenizer:
    library: yttm
    tokenizer_model: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/input.BPE.25000.model
    bpe_dropout: 0.0
    r2l: false

  decoder_tokenizer:
    library: char
    vocab_file: /media/apeganov/DATA/wiki_wmt_92_128_29.11.2021/for_upload/cross_char_vocabulary.txt
    bpe_dropout: 0.0
    r2l: false
    word_tokens:
      - a
      - b
      - c
      - d
      - e
      - f
      - g
      - h

  encoder:
    library: nemo
    model_name: null
    pretrained: false
    max_sequence_length: 512
    num_token_types: 0
    embedding_dropout: 0.1
    learn_positional_encodings: false
    hidden_size: 512
    num_layers: 6
    inner_size: 2048
    num_attention_heads: 8
    ffn_dropout: 0.1
    attn_score_dropout: 0.1
    attn_layer_dropout: 0.1
    hidden_act: relu
    mask_future: false
    pre_ln: false
    pre_ln_final_layer_norm: true

  decoder:
    library: nemo
    model_name: null
    pretrained: false
    max_sequence_length: 512
    num_token_types: 0
    embedding_dropout: 0.1
    learn_positional_encodings: false
    hidden_size: 512
    inner_size: 2048
    num_layers: 6
    num_attention_heads: 8
    ffn_dropout: 0.1
    attn_score_dropout: 0.1
    attn_layer_dropout: 0.1
    hidden_act: relu
    pre_ln: false
    pre_ln_final_layer_norm: true

  head:
    num_layers: 1
    activation: relu
    log_softmax: true
    dropout: 0.0
    use_transformer_init: true

trainer:
  gpus: 1
  num_nodes: 1
  max_steps: 200000
  amp_backend: native
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  accelerator: gpu
  checkpoint_callback: False
  logger: False
  log_every_n_steps: 50  # Interval of logging.
  check_val_every_n_epoch: 1
  val_check_interval: 2000
  progress_bar_refresh_rate: 0

exp_manager:
  exp_dir: nemo_experiments
  name: AAYNLarge6x6
  create_checkpoint_callback: true  # Whether you want exp_manager to create a model checkpoint callback
  create_wandb_logger: true
  wandb_logger_kwargs:
    name: local_nmt_wiki_wmt_large6x6_bs50000_steps400000_cross
    project: autoregressive_punctuation_capitalization
    entity: anton_peganov
  checkpoint_callback_params:
    save_best_model: true
    monitor: val_CER
    mode: min
    save_top_k: 5
