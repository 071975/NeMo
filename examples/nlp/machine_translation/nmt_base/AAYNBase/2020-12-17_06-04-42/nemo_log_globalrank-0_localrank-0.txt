[NeMo W 2020-12-17 06:04:40 experimental:28] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2020-12-17 06:04:41 enc_dec_nmt:97] 
    
    ************** Experiment configuration ***********
[NeMo W 2020-12-17 06:04:41 nemo_logging:349] /opt/conda/lib/python3.6/site-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.
    Use OmegaConf.to_yaml(cfg)
    
      category=UserWarning,
    
[NeMo I 2020-12-17 06:04:41 enc_dec_nmt:98] Config: model:
      beam_size: 4
      len_pen: 0.6
      max_generation_delta: 5
      label_smoothing: 0.1
      train_ds:
        src_file_name: /mnt/D1/Data/NMT/wmt16_de_en/train.clean.de.shuffled
        tgt_file_name: /mnt/D1/Data/NMT/wmt16_de_en/train.clean.en.shuffled
        tokens_in_batch: 50000
        clean: true
        max_seq_length: 512
        cache_ids: true
        cache_data_per_node: false
        use_cache: true
        shuffle: true
        num_samples: -1
        drop_last: false
        pin_memory: false
        num_workers: 8
      validation_ds:
        src_file_name: /mnt/D1/Data/NMT/wmt16_de_en/wmt14-en-de.ref
        tgt_file_name: /mnt/D1/Data/NMT/wmt16_de_en/wmt14-en-de.src
        tokens_in_batch: 8192
        clean: false
        max_seq_length: 512
        cache_ids: true
        cache_data_per_node: false
        use_cache: true
        shuffle: false
        num_samples: -1
        drop_last: false
        pin_memory: false
        num_workers: 8
      test_ds:
        src_file_name: /mnt/D1/Data/NMT/wmt16_de_en/wmt14-en-de.ref
        tgt_file_name: /mnt/D1/Data/NMT/wmt16_de_en/wmt14-en-de.src
        tokens_in_batch: 512
        clean: false
        max_seq_length: 512
        cache_ids: true
        cache_data_per_node: false
        use_cache: true
        shuffle: false
        num_samples: -1
        drop_last: false
        pin_memory: false
        num_workers: 8
      optim:
        name: adam
        lr: 0.001
        betas:
        - 0.9
        - 0.98
        weight_decay: 0.0
        sched:
          name: InverseSquareRootAnnealing
          min_lr: 0.0
          last_epoch: -1
          warmup_ratio: 0.05
      encoder_tokenizer:
        tokenizer_name: yttm
        tokenizer_model: tokenizer.BPE.8192.model
        vocab_file: null
        special_tokens: null
      decoder_tokenizer:
        tokenizer_name: yttm
        tokenizer_model: tokenizer.BPE.8192.model
        vocab_file: null
        special_tokens: null
      encoder_embedding:
        max_sequence_length: 512
        num_token_types: 2
        embedding_dropout: 0.1
        learn_positional_encodings: false
        _target_: nemo.collections.nlp.modules.common.transformer.TransformerEmbedding
      encoder:
        hidden_size: 512
        num_layers: 6
        inner_size: 2048
        num_attention_heads: 8
        ffn_dropout: 0.1
        attn_score_dropout: 0.1
        attn_layer_dropout: 0.1
        hidden_act: relu
        mask_future: false
        pre_ln: false
        _target_: nemo.collections.nlp.modules.common.transformer.TransformerEncoder
      decoder_embedding:
        max_sequence_length: 512
        num_token_types: 2
        embedding_dropout: 0.1
        learn_positional_encodings: false
        _target_: nemo.collections.nlp.modules.common.transformer.TransformerEmbedding
      decoder:
        hidden_size: 512
        inner_size: 2048
        num_layers: 6
        num_attention_heads: 8
        ffn_dropout: 0.1
        attn_score_dropout: 0.1
        attn_layer_dropout: 0.1
        hidden_act: relu
        pre_ln: false
        _target_: nemo.collections.nlp.modules.common.transformer.TransformerDecoder
      head:
        hidden_size: 512
        num_classes: 37000
        num_layers: 1
        activation: relu
        log_softmax: true
        dropout: 0.0
        use_transformer_init: true
        _target_: nemo.collections.nlp.modules.common.token_classifier.TokenClassifier
    trainer:
      gpus:
      - 0
      - 1
      num_nodes: 1
      amp_level: O2
      precision: 16
      distributed_backend: ddp
      checkpoint_callback: false
      logger: false
      log_every_n_steps: 50
      check_val_every_n_epoch: 1
      _target_: pytorch_lightning.Trainer
      max_steps: 150000
    exp_manager:
      name: AAYNBase
      files_to_copy: []
      create_wandb_logger: true
      wandb_logger_kwargs:
        name: TEST-nmt-base
        project: nmt-de-en
      create_checkpoint_callback: true
      checkpoint_callback_params:
        monitor: val_sacreBLEU
        mode: max
      exp_dir: nmt_base
    
[NeMo I 2020-12-17 06:04:42 exp_manager:184] Experiments will be logged at nmt_base/AAYNBase/2020-12-17_06-04-42
[NeMo I 2020-12-17 06:04:42 exp_manager:519] TensorboardLogger has been set up
[NeMo I 2020-12-17 06:04:42 exp_manager:529] WandBLogger has been set up
[NeMo W 2020-12-17 06:04:42 exp_manager:569] trainer had a weights_save_path of cwd(). This was ignored.
[NeMo W 2020-12-17 06:04:42 modelPT:194] Using /NeMo/examples/nlp/machine_translation/tokenizer.BPE.8192.model instead of tokenizer.BPE.8192.model.
[NeMo W 2020-12-17 06:04:42 modelPT:194] Using /NeMo/examples/nlp/machine_translation/tokenizer.BPE.8192.model instead of tokenizer.BPE.8192.model.
[NeMo W 2020-12-17 06:04:42 nemo_logging:349] /opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py:104: UserWarning: 
    GeForce GT 710 with CUDA capability sm_35 is not compatible with the current PyTorch installation.
    The current PyTorch install supports CUDA capabilities sm_52 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 compute_86.
    If you want to use the GeForce GT 710 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/
    
      warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
    
[NeMo I 2020-12-17 06:04:42 data_preprocessing:374] Loading cached tokenized dataset ...
