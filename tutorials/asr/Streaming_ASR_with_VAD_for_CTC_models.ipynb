{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Add starting block TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming ASR with VAD\n",
    "\n",
    "why asr with vad? TODO\n",
    "\n",
    "\n",
    "This tutorials demonstrate how to integrate VAD to Streaming ASR. To reduce redundancy, this tutorial will not walk you through Streaming ASR step by step,  <b>please have a look at the detailed [Streaming ASR](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Streaming_ASR.ipynb) tutorial first.<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline of streaming ASR with VAD is as below. \n",
    "<!-- <img src=\"https://raw.githubusercontent.com/NVIDIA/NeMo/online_streaming_vad_asr/tutorials/asr/images/streaming_ASR_with_VAD.png\" width=\"500\"> -->\n",
    "\n",
    "<img src=\"./images/streaming_ASR_with_VAD.png\" width=\"700\">\n",
    "\n",
    "There are a few advantages of it:\n",
    "1. NeMo VAD and ASR models are using same input feature, and the only difference is that we do not normalize it in VAD. We could use feature extractor for both modules.\n",
    "2. When VAD detects begining and ending of speech, we will activate and deactivate feature normalizer and streaming ASR. This will introduce less latency and save computation resources as well as improve performance. \n",
    "\n",
    "This is for streaming VAD + ASR with CTC based models and you can easily adopt it to RNNT based models as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in NeMo we simulate this process by performing streaming VAD and generating speech segments first and perform streaming ASR inference on the segments. If you are interested in the inference pipeline and the interaction of each module, please refer to Riva. It will be in Riva soon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a script to help you perform streaming ASR with VAD inference, you can play with it. \n",
    "\n",
    "And we will walk you through it in this tutorial. \n",
    "1. TODO\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# script TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and script for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_manifest=\"chris_demo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"audio_filepath\": \"/mnt/data/freesound/Laughter/id_17121 smallaudiencelaughter.wav\", \"duration\": 2.8583125, \"offset\": 0, \"text\": \"\"}\r\n",
      "{\"audio_filepath\": \"data/chris-sample01_02.wav\", \"duration\": 123.7388125, \"offset\": 0, \"text\": \"this is the first sample using a headset this is the second sample using a headset\"}\r\n",
      "{\"audio_filepath\": \"data/chris-sample03.wav\", \"duration\": 63.464, \"offset\": 0, \"text\": \"this is the third sample using a laptop microphone\"}"
     ]
    }
   ],
   "source": [
    "!head -n 10 $input_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VAD and ASR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4376/2877109157.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# here we use stt_en_conformer_ctc_large as example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0masr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnemo_asr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncDecCTCModelBPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stt_en_conformer_ctc_large\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0masr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mVAD_MODEL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/fjia/code/ngc_workspace/new_experiments/mul_vad/80_b256_lr0.02_min1e4_e50_3x4/80_b256_lr0.02_min1e4_e50_3x4/80_b256_lr0.02_min1e4_e50_3x4/2022-02-18_14-30-33/checkpoints/80_b256_lr0.02_min1e4_e50_3x4.nemo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# here we use stt_en_conformer_ctc_large as example\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(\"stt_en_conformer_ctc_large\", map_location=device)\n",
    "asr_model = asr_model.to(device)\n",
    "VAD_MODEL=\"/home/fjia/code/ngc_workspace/new_experiments/mul_vad/80_b256_lr0.02_min1e4_e50_3x4/80_b256_lr0.02_min1e4_e50_3x4/80_b256_lr0.02_min1e4_e50_3x4/2022-02-18_14-30-33/checkpoints/80_b256_lr0.02_min1e4_e50_3x4.nemo\"\n",
    "# /home/fjia/code/1007/b256_truelr0.02_min1e4_e200_2gpu/b256_truelr0.02_min1e4_e200_2gpu/b256_truelr0.02_min1e4_e200_2gpu/2021-10-06_17-43-59/checkpoints/b256_truelr0.02_min1e4_e200_2gpu--val_loss=0.0932-epoch=192.ckpt\n",
    "vad_model = nemo_asr.models.EncDecClassificationModel.restore_from(VAD_MODEL,  map_location=device)\n",
    "vad_model = vad_model.to(device)\n",
    "asr_model.eval()\n",
    "vad_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from omegaconf import OmegaConf\n",
    "cfg = copy.deepcopy(asr_model._cfg)\n",
    "OmegaConf.set_struct(cfg.preprocessor, False)\n",
    "cfg.preprocessor.dither = 0.0\n",
    "cfg.preprocessor.pad_to = 0\n",
    "\n",
    "if cfg.preprocessor.normalize != \"per_feature\":\n",
    "    logging.error(\"Only EncDecCTCModelBPE models trained with per_feature normalization are supported currently\")\n",
    "OmegaConf.set_struct(cfg.preprocessor, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to AudioChunkIterator which return successive chunks of samples in Streaming ASR tutorial, \n",
    "here we use FeatureFrameBufferer return return an array of feature buffers. As noted above, VAD and ASR will use share it and and the only difference is <code>normalize_feature</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.streaming_utils import AudioBuffersDataLayer, speech_collate_fn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFrameBufferer:\n",
    "    \"\"\"\n",
    "    Class to append each feature frame to a buffer and return\n",
    "    an array of buffers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, asr_model, frame_len=1.6, batch_size=4, total_buffer=4.0, normalize_feature=True):\n",
    "        '''\n",
    "        Args:\n",
    "          frame_len: frame's duration, seconds\n",
    "          frame_overlap: duration of overlaps before and after current frame, seconds\n",
    "          offset: number of symbols to drop for smooth streaming\n",
    "        '''\n",
    "        self.normalize_feature = normalize_feature \n",
    "        self.ZERO_LEVEL_SPEC_DB_VAL = -16.635  # Log-Melspectrogram value for zero signal\n",
    "        self.asr_model = asr_model\n",
    "        self.sr = asr_model._cfg.sample_rate\n",
    "        self.frame_len = frame_len\n",
    "        timestep_duration = asr_model._cfg.preprocessor.window_stride\n",
    "        self.n_frame_len = int(frame_len / timestep_duration)\n",
    "\n",
    "        total_buffer_len = int(total_buffer / timestep_duration)\n",
    "        self.n_feat = asr_model._cfg.preprocessor.features\n",
    "        self.buffer = np.ones([self.n_feat, total_buffer_len], dtype=np.float32) * self.ZERO_LEVEL_SPEC_DB_VAL\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.signal_end = False\n",
    "        self.frame_reader = None\n",
    "        self.feature_buffer_len = total_buffer_len\n",
    "\n",
    "        self.feature_buffer = (\n",
    "            np.ones([self.n_feat, self.feature_buffer_len], dtype=np.float32) * self.ZERO_LEVEL_SPEC_DB_VAL\n",
    "        )\n",
    "        self.frame_buffers = []\n",
    "        self.buffered_features_size = 0\n",
    "        self.reset()\n",
    "        self.buffered_len = 0\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset frame_history and decoder's state\n",
    "        '''\n",
    "        self.buffer = np.ones(shape=self.buffer.shape, dtype=np.float32) * self.ZERO_LEVEL_SPEC_DB_VAL\n",
    "        self.prev_char = ''\n",
    "        self.unmerged = []\n",
    "        self.frame_buffers = []\n",
    "        self.buffered_len = 0\n",
    "        self.feature_buffer = (\n",
    "            np.ones([self.n_feat, self.feature_buffer_len], dtype=np.float32) * self.ZERO_LEVEL_SPEC_DB_VAL\n",
    "        )\n",
    "\n",
    "    def get_batch_frames(self):\n",
    "        if self.signal_end:\n",
    "            return []\n",
    "        batch_frames = []\n",
    "        for frame in self.frame_reader:\n",
    "            batch_frames.append(np.copy(frame))\n",
    "            if len(batch_frames) == self.batch_size:\n",
    "                return batch_frames\n",
    "        self.signal_end = True\n",
    "\n",
    "        return batch_frames\n",
    "\n",
    "    def get_frame_buffers(self, frames):\n",
    "        # Build buffers for each frame\n",
    "        self.frame_buffers = []\n",
    "        for frame in frames:\n",
    "            self.buffer[:, : -self.n_frame_len] = self.buffer[:, self.n_frame_len :]\n",
    "            self.buffer[:, -self.n_frame_len :] = frame\n",
    "            self.buffered_len += frame.shape[1]\n",
    "            self.frame_buffers.append(np.copy(self.buffer))\n",
    "        return self.frame_buffers\n",
    "\n",
    "    def set_frame_reader(self, frame_reader):\n",
    "        self.frame_reader = frame_reader\n",
    "        self.signal_end = False\n",
    "\n",
    "    def _update_feature_buffer(self, feat_frame):\n",
    "        self.feature_buffer[:, : -feat_frame.shape[1]] = self.feature_buffer[:, feat_frame.shape[1] :]\n",
    "        self.feature_buffer[:, -feat_frame.shape[1] :] = feat_frame\n",
    "        self.buffered_features_size += feat_frame.shape[1]\n",
    "\n",
    "    def get_norm_consts_per_frame(self, batch_frames):\n",
    "        norm_consts = []\n",
    "        for i, frame in enumerate(batch_frames):\n",
    "            self._update_feature_buffer(frame)\n",
    "            mean_from_buffer = np.mean(self.feature_buffer, axis=1)\n",
    "            stdev_from_buffer = np.std(self.feature_buffer, axis=1)\n",
    "            norm_consts.append((mean_from_buffer.reshape(self.n_feat, 1), stdev_from_buffer.reshape(self.n_feat, 1)))\n",
    "        return norm_consts\n",
    "\n",
    "    def normalize_frame_buffers(self, frame_buffers, norm_consts):\n",
    "        CONSTANT = 1e-5\n",
    "        for i, frame_buffer in enumerate(frame_buffers):\n",
    "            frame_buffers[i] = (frame_buffer - norm_consts[i][0]) / (norm_consts[i][1] + CONSTANT)\n",
    "\n",
    "    def get_buffers_batch(self):\n",
    "        batch_frames = self.get_batch_frames()\n",
    "\n",
    "        while len(batch_frames) > 0:\n",
    "\n",
    "            frame_buffers = self.get_frame_buffers(batch_frames)\n",
    "            norm_consts = self.get_norm_consts_per_frame(batch_frames)\n",
    "            if len(frame_buffers) == 0:\n",
    "                continue\n",
    "            if self.normalize_feature:\n",
    "                norm_consts = self.get_norm_consts_per_frame(batch_frames)\n",
    "                self.normalize_frame_buffers(frame_buffers, norm_consts)\n",
    "            return frame_buffers\n",
    "        return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to <code>ChunkBufferDecoder</code> in streaming ASR tutorial, we have \n",
    "\n",
    "- class <code>FrameBatchASR</code> for streaming frame-based ASR and \n",
    "- class <code>FrameBatchVAD</code> for streaming frame-based VAD.\n",
    "Since FrameBatchASR has been introduced in streaming ASR, we ignore it in this tutorial, please refer to streaming_utils.py for more information, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.streaming_utils import FrameBatchASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through FrameBatchVAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.streaming_utils import get_samples, AudioFeatureIterator\n",
    "class FrameBatchVAD:\n",
    "    \"\"\"\n",
    "    class for streaming frame-based ASR use reset() method to reset FrameASR's\n",
    "    state call transcribe(frame) to do ASR on contiguous signal's frames\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vad_model, frame_len=0.16, total_buffer=0.63, batch_size=4, patience=25\n",
    "    ):\n",
    "        '''\n",
    "        Args:\n",
    "          frame_len: frame's duration, seconds\n",
    "          frame_overlap: duration of overlaps before and after current frame, seconds\n",
    "          offset: number of symbols to drop for smooth streaming\n",
    "        '''\n",
    "        # Note the feature sent into VAD is none normalized\n",
    "        self.frame_bufferer = FeatureFrameBufferer(\n",
    "            asr_model=vad_model, frame_len=frame_len, batch_size=batch_size, total_buffer=total_buffer, \n",
    "            normalize_feature=False \n",
    "        )\n",
    "        self.patience=patience\n",
    "        self.vad_model = vad_model\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.all_vad_preds = []\n",
    "        self.vad_decision_buffer = [0] * (self.patience+1)\n",
    "        self.speech_segment = set()\n",
    "        self.start = 0\n",
    "        self.end=0\n",
    "        self.is_speech = False\n",
    "\n",
    "        self.frame_buffers = []\n",
    "        self.reset()\n",
    "        cfg = copy.deepcopy(vad_model._cfg)\n",
    "        self.frame_len = frame_len\n",
    "        OmegaConf.set_struct(cfg.preprocessor, False)\n",
    "\n",
    "        # some changes for streaming scenario\n",
    "        cfg.preprocessor.dither = 0.0\n",
    "        cfg.preprocessor.pad_to = 0\n",
    "        cfg.preprocessor.normalize = \"None\"\n",
    "        self.raw_preprocessor = EncDecClassificationModel.from_config_dict(cfg.preprocessor)\n",
    "        self.raw_preprocessor.to(vad_model.device)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset frame_history and decoder's state\n",
    "        \"\"\"\n",
    "        self.data_layer = AudioBuffersDataLayer()\n",
    "        self.data_loader = DataLoader(self.data_layer, batch_size=self.batch_size, collate_fn=speech_collate_fn)\n",
    "\n",
    "        self.all_vad_preds = []\n",
    "        self.vad_decision_buffer = [0] * (self.patience+1)\n",
    "        self.speech_segment = set()\n",
    "        self.start = 0\n",
    "        self.end=0\n",
    "        self.is_speech = False\n",
    "        self.frame_bufferer.reset()\n",
    "\n",
    "    def read_audio_file(self, \n",
    "                        audio_filepath: str, \n",
    "                        offset: float,\n",
    "                        duration: float,\n",
    "                        delay: float, \n",
    "                        model_stride_in_secs: float):\n",
    "        samples = get_samples(audio_filepath, offset, duration)\n",
    "        self.pad_end_len = int(delay * model_stride_in_secs * self.vad_model._cfg.sample_rate)\n",
    "        samples = np.pad(samples, (0, self.pad_end_len))\n",
    "\n",
    "        frame_reader = AudioFeatureIterator(samples, self.frame_len, self.raw_preprocessor, self.vad_model.device)\n",
    "        self.set_frame_reader(frame_reader)\n",
    "\n",
    "    def set_frame_reader(self, frame_reader):\n",
    "        self.frame_bufferer.set_frame_reader(frame_reader)\n",
    "\n",
    "    def online_decision(self, current_pred, current_frame, end_of_seq=False, threshold=0.4):\n",
    "         # binarization\n",
    "        if current_pred >= threshold:\n",
    "            current_decision = 1\n",
    "        else:\n",
    "            current_decision = 0\n",
    "        \n",
    "        # change happens\n",
    "        if not self.is_speech and self.vad_decision_buffer.count(1) > 2:\n",
    "            self.is_speech = True\n",
    "            self.start = current_frame  \n",
    "            \n",
    "        if self.is_speech and self.vad_decision_buffer.count(1) == 0:\n",
    "            self.is_speech = False\n",
    "            self.end = current_frame\n",
    "            self.speech_segment.add((round(self.start,2) , round(self.end, 2)))\n",
    "            \n",
    "        if self.is_speech and end_of_seq:\n",
    "            self.end = current_frame \n",
    "            self.speech_segment.add((self.start, self.end))\n",
    "\n",
    "        self.vad_decision_buffer = self.vad_decision_buffer[1:]\n",
    "        self.vad_decision_buffer.append(current_decision)\n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def infer_logits(self):\n",
    "        frame_buffers = self.frame_bufferer.get_buffers_batch()\n",
    "\n",
    "        while len(frame_buffers) > 0:\n",
    "            self.frame_buffers += frame_buffers[:]\n",
    "            self.data_layer.set_signal(frame_buffers[:])\n",
    "            self._get_batch_preds()\n",
    "            frame_buffers = self.frame_bufferer.get_buffers_batch()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_batch_preds(self):\n",
    "        device = self.vad_model.device\n",
    "        for batch in iter(self.data_loader):\n",
    "\n",
    "            feat_signal, feat_signal_len = batch\n",
    "            feat_signal, feat_signal_len = feat_signal.to(device), feat_signal_len.to(device)\n",
    "\n",
    "            logits = self.vad_model(\n",
    "                processed_signal=feat_signal, \n",
    "                processed_signal_length=feat_signal_len\n",
    "            )\n",
    "                \n",
    "            vad_probs = torch.softmax(logits, dim=-1)\n",
    "            vad_pred = vad_probs[:, 1]\n",
    "            vad_pred = vad_pred.cpu().numpy()\n",
    "            self.all_vad_preds.extend(vad_pred)\n",
    "\n",
    "            del logits\n",
    "\n",
    "    def decode(self, threshold=0.4):\n",
    "        self.infer_logits()\n",
    "        end_of_seq = False\n",
    "        for i in range(len(self.all_vad_preds)):\n",
    "            current_pred = self.all_vad_preds[i]\n",
    "            current_frame = i * self.frame_len\n",
    "            \n",
    "            if i == len(self.all_vad_preds)-1 :\n",
    "                end_of_seq = True\n",
    "\n",
    "            self.online_decision(current_pred, current_frame, end_of_seq, threshold)\n",
    "\n",
    "            if end_of_seq:\n",
    "                break\n",
    "            \n",
    "        return self.all_vad_preds, self.speech_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce `online_decision` in `decode` function. Basically we binarize the VAD output of current frame with `threshold` and append to `vad_decision_buffer` of length (patience+1). \n",
    "\n",
    "The online_decision method here is quite simple and we would recommend to use a more sophiscated method that might improve performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr\n",
    "chunk_len_in_ms = 160\n",
    "total_buffer_in_secs = 4\n",
    "model_stride = 4 # 8 for citrinet\n",
    "# vad\n",
    "total_vad_buffer_in_secs = 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "feature_stride = cfg.preprocessor['window_stride']\n",
    "model_stride_in_secs = feature_stride * model_stride\n",
    "total_buffer = total_buffer_in_secs\n",
    "chunk_len = chunk_len_in_ms / 1000\n",
    "batch_size = 128\n",
    "tokens_per_chunk = math.ceil(chunk_len / model_stride_in_secs)\n",
    "mid_delay = math.ceil((chunk_len + (total_buffer - chunk_len) / 2) / model_stride_in_secs)\n",
    "\n",
    "total_vad_buffer = total_vad_buffer_in_secs\n",
    "vad_mid_delay = math.ceil((chunk_len + (total_vad_buffer - chunk_len) / 2) / 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models.ctc_bpe_models import EncDecCTCModelBPE\n",
    "from nemo.collections.asr.models.classification_models import EncDecClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_vad = FrameBatchVAD(\n",
    "    vad_model=vad_model, frame_len=chunk_len, total_buffer=total_vad_buffer, batch_size=batch_size, patience=9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_asr = FrameBatchASR(\n",
    "    asr_model=asr_model, frame_len=chunk_len, total_buffer=total_buffer, batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_wer_feat(mfst, asr, frame_len, tokens_per_chunk, delay, preprocessor_cfg, model_stride_in_secs, device,vad=None, vad_delay=None,threshold: float = 0.4,\n",
    "    look_back: int = 4):\n",
    "    # Create a preprocessor to convert audio samples into raw features,\n",
    "    # Normalization will be done per buffer in frame_bufferer\n",
    "    # Do not normalize whatever the model's preprocessor setting is\n",
    "    preprocessor_cfg.normalize = \"None\"\n",
    "    preprocessor = nemo_asr.models.EncDecCTCModelBPE.from_config_dict(preprocessor_cfg)\n",
    "    preprocessor.to(device)\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    total_durations_to_asr = []\n",
    "    original_durations = []\n",
    "    total_speech_segments = []\n",
    "    total_streaming_vad_logits = []\n",
    "\n",
    "    with open(mfst, \"r\") as mfst_f:\n",
    "        for l in mfst_f:\n",
    "            asr.reset()\n",
    "            row = json.loads(l.strip())\n",
    "            if vad:\n",
    "                vad.reset()\n",
    "                vad.read_audio_file(row['audio_filepath'], offset=0, duration=0, delay=vad_delay, model_stride_in_secs=1)          \n",
    "                streaming_vad_logits, speech_segments = vad.decode(threshold=threshold)\n",
    "                speech_segments = [list(i) for i in speech_segments]\n",
    "                speech_segments.sort(key=lambda x: x[0])\n",
    "\n",
    "                final_hyp = \" \"\n",
    "                total_duration_to_asr = 0\n",
    "                for i in range(len(speech_segments)): \n",
    "                    asr.reset()\n",
    "                    offset = max(speech_segments[i][0] - frame_len * look_back, 0)\n",
    "\n",
    "                    if row['duration'] and speech_segments[i][1] > row['duration']:\n",
    "                        end = row['duration']\n",
    "                        speech_segments[i][1] = end\n",
    "                    else:\n",
    "                        end = speech_segments[i][1]\n",
    "                    \n",
    "                    duration = end - speech_segments[i][0] + frame_len * look_back\n",
    "                    \n",
    "                    asr.read_audio_file(row['audio_filepath'], offset, duration, delay, model_stride_in_secs)\n",
    "                    hyp = asr.transcribe(tokens_per_chunk, delay) + \" \"\n",
    "                    # there should be some better method to merge the hyps of segments.\n",
    "                    final_hyp += hyp\n",
    "                    total_duration_to_asr += duration\n",
    "\n",
    "                final_hyp = final_hyp[1:-1]\n",
    "                # final_hyp = clean_label(final_hyp, num_to_words)\n",
    "                # ref_clean = clean_label(row['text'], num_to_words)\n",
    "\n",
    "                hyps.append(final_hyp)\n",
    "                refs.append(row['text'])\n",
    "                total_durations_to_asr.append(total_duration_to_asr)\n",
    "                original_durations.append(row['duration'])\n",
    "                total_speech_segments.append(speech_segments)\n",
    "                total_streaming_vad_logits.append(streaming_vad_logits)\n",
    "\n",
    "            else:\n",
    "                asr.read_audio_file(row['audio_filepath'], offset=0, duration=0, delay=delay, model_stride_in_secs=model_stride_in_secs)\n",
    "                hyp = asr.transcribe(tokens_per_chunk, delay)\n",
    "                hyps.append(hyp)\n",
    "                refs.append(row['text'])\n",
    "\n",
    "                total_durations_to_asr.append(row['duration'])\n",
    "                speech_segments = \"ALL\"\n",
    "                total_speech_segments.append(speech_segments)\n",
    "\n",
    "    wer = word_error_rate(hypotheses=hyps, references=refs)\n",
    "    print(f\"Overall word error rate is {wer}\")\n",
    "    if vad:\n",
    "        print(f\"VAD reduces total durations for ASR inference from {int(sum(original_durations))} seconds to {int(sum(total_durations_to_asr))} seconds, by filtering out some noise or music\")\n",
    "    \n",
    "    return hyps, refs, wer, total_durations_to_asr, total_speech_segments, total_streaming_vad_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "look_back=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.metrics.wer import word_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " hyps, refs, wer, total_durations_to_asr, total_speech_segments, total_streaming_vad_logits = get_wer_feat(\n",
    "        input_manifest,\n",
    "        frame_asr,\n",
    "        chunk_len,\n",
    "        tokens_per_chunk,\n",
    "        mid_delay,\n",
    "        cfg.preprocessor,\n",
    "        model_stride_in_secs,\n",
    "        asr_model.device,\n",
    "        frame_vad,\n",
    "        vad_mid_delay,\n",
    "        threshold,\n",
    "        look_back, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_speech_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output\" # the path of output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"streaming_demo_output.json\"\n",
    "hyp_json = os.path.join(output_path, fname)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(\"vad_logits\", exist_ok=True)\n",
    "with open(hyp_json, \"w\") as out_f:\n",
    "    for i, hyp in enumerate(hyps):\n",
    "        vad_output_file = os.path.join(\"vad_logits\", str(i) + \".npy\")\n",
    "        np.save(vad_output_file, total_streaming_vad_logits[i])\n",
    "        record = {\n",
    "            \"pred_text\": hyp,\n",
    "            \"text\": refs[i],\n",
    "            \"wer\": round(word_error_rate(hypotheses=[hyp], references=[refs[i]]) * 100, 2),\n",
    "            \"total_duration_to_asr\": total_durations_to_asr[i],\n",
    "            \"total_speech_segments\": total_speech_segments[i],\n",
    "            \"total_streaming_vad_logits\": vad_output_file,\n",
    "        }\n",
    "        out_f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(hyp_json, \"r\") as fp:\n",
    "    for line in fp:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "with open(input_manifest, \"r\") as fp:\n",
    "    for line in fp:\n",
    "        input_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "audio_filepath = input_data[i]['audio_filepath']\n",
    "print(f\"orginal duration is of {audio_filepath} is {input_data[i]['duration']}\")\n",
    "\n",
    "for speech_segment in data[i]['total_speech_segments']: \n",
    "    print(\"Speech Segment:\", speech_segment)\n",
    "    offset = max(speech_segment[0] - chunk_len * look_back, 0)\n",
    "    duration = speech_segment[1] - speech_segment[0] + chunk_len * look_back\n",
    "    \n",
    "    audio_snipt, sample_rate = librosa.load(audio_filepath, offset=offset, duration=duration, sr=16000)\n",
    "    total_streaming_vad_logits = np.load(data[i]['total_streaming_vad_logits'])\n",
    "    ipd.display(ipd.Audio(audio_snipt, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(audio_filepath, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=[10, 2])\n",
    "ax1 = plt.subplot()\n",
    "ax1.plot(np.arange(len(audio))/16000, audio, 'gray')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_ylabel('Signal')\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_xlim(0, )\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.arange(len(total_streaming_vad_logits))*chunk_len , total_streaming_vad_logits, 'g')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "ax2.set_ylabel('Preds')\n",
    "ax2.set_ylim([-0.1, 1.1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_NeMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
