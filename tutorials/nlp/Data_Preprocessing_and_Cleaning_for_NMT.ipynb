{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0075e98c",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Cleaning for NMT\n",
    "\n",
    "This notebook contains a tutorial of data processing and cleaning for NMT (Neural Machine Translation) to train translation models with the [NeMo framework](https://github.com/NVIDIA/NeMo).\n",
    "\n",
    "A pre-requisite to train supervised neural machine translation systems is the availability of *parallel corpora* of reasonable quality.\n",
    "\n",
    "A parallel corpus is a collection of sentences or documents that are translations of each other in 2 or more languages.\n",
    "\n",
    "For example,\n",
    "\n",
    "| English                                                                            | Russian |\n",
    "| :-: | :-: |\n",
    "| To date, a total of 43 participants from 15 countries have completed the training. | К настоящему времени подготовку прошли в общей сложности 43 участника из 15 стран . |\n",
    "| M-Sport Bentley writes a new piece of Bentley history at Silverstone | M-Sport Bentley открывает новую страницу в истории Bentley в Сильверстоуне |\n",
    "| Information in the application was not true. | Информация в заявлении не была достоверна. |\n",
    "\n",
    "This notebook will cover the following data pre-processing and data cleaning techniques for such corpora.\n",
    "\n",
    "## The importance of data cleaning\n",
    "\n",
    "The presence of noise in the training dataset can adversely affect model quality (https://arxiv.org/abs/1805.12282). Webcrawled and automatically aligned data sources in particular, such as [Paracrawl](https://paracrawl.eu/), [WikiMatrix](https://arxiv.org/abs/1907.05791), [CC-Aligned](https://arxiv.org/abs/1911.06154) and [CC-Matrix](https://arxiv.org/abs/1911.04944) can be extremely noisy.\n",
    "\n",
    "## Cleaning\n",
    "1. Downloading and filtering publicly available datasets based on confidence thresholds (if available). For example, [WikiMatrix](https://arxiv.org/abs/1907.05791) filtering based on [LASER](https://arxiv.org/abs/1812.10464) confidence scores.\n",
    "2. Language ID filtering using a pre-trained [fastText classifier](https://fasttext.cc/docs/en/language-identification.html). This step will remove all sentences from the parallel corpus that our classifier predicts as not being in the appropriate language (ex: sentences in the English column that aren't in English or sentences in Russian column that aren't in Russian).\n",
    "3. Length and Length-ratio filtering. This steps removes all sentences that are 1) too long 2) too short or 3) have a ratio between their lengths greater than a certain factor (this typically removes partial translations).\n",
    "4. [Bicleaner](https://github.com/bitextor/bicleaner) classifier-based cleaning. Bicleaner identifies noisy parallel senteces using a classifier that leverages multiple features such as n-gram language model likelihood scores, word alignment scores and other heuristics.\n",
    "\n",
    "## Pre-processing\n",
    "5. [Moses Punctuation Normalization](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/normalize-punctuation.perl). This step standardizes punctuation. For example the less common way to write apostrophes Tiffany`s will be standardized to Tiffany's.\n",
    "6. Unicode standardization. There exist some unicode characters that aren't punctuation that need to be standardized for example, this step normalizes the number ４ to 4.\n",
    "7. [Moses Tokenization](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl) or text segmentation for Chinese/Japanese with [Jieba](https://github.com/fxsjy/jieba) and [mecab](https://github.com/taku910/mecab). For languages like Chinese and Japanese that do not have explicit word segmentation markers (like spaces), we use these tools to introduce spaces into the text that will let us split the string into words. For other languages, we use Moses to separate punctuation markers from words so that they become separate tokens.\n",
    "8. Deduplication - This step removes duplicate translation pairs from the corpus.\n",
    "9. Shuffling - This step shuffles the order of occurrence of translation pairs.\n",
    "\n",
    "## Tarred Datasets for Large Corpora\n",
    "10. Large datasts with over 50M sentence pairs when batched and pickled can be upto 60GB in size. Loading them entirely into CPU memory when using say 8 or 16 workers with DistributedDataParallel training uses 480-960GB of RAM which is often impractical and inefficient. Instead, we use [Webdataset](https://github.com/webdataset/webdataset) to allow training while keeping datasets on disk and let webddataset handle pre-loading and fetching of data into CPU RAM.\n",
    "\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "The data cleaning techniques used in this notebook are only meant to be loose guidelines and are not guaranteed to produced clean parallel corpora at the end of it. Not all of these steps are a necessity for every dataset, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86db52c",
   "metadata": {},
   "source": [
    "![NMT Data Pipeline](images/nmt_data_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fd8d3",
   "metadata": {},
   "source": [
    "# Downloading Publicly Available Data\n",
    "\n",
    "## WikiMatrix (https://arxiv.org/abs/1907.05791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78984523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data ...\n",
      "--2021-07-13 09:04:45--  https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ru.tsv.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 658252364 (628M) [application/gzip]\n",
      "Saving to: ‘data/WikiMatrix.en-ru.tsv.gz’\n",
      "\n",
      "data/WikiMatrix.en- 100%[===================>] 627.76M  50.3MB/s    in 13s     \n",
      "\n",
      "2021-07-13 09:04:58 (49.9 MB/s) - ‘data/WikiMatrix.en-ru.tsv.gz’ saved [658252364/658252364]\n",
      "\n",
      "---------------------\n",
      "Unzipping file ...\n",
      "---------------------\n",
      "Peek into the file\n",
      "1.2217877209774821\tThe glory of the Lord has risen upon thee\".\tКакую же из милостей вашего Господа вы считаете ложью?».\n",
      "1.2136469670929166\tFear of the Lord is aking to wonder (or awe).\tВоистину, мучений от твоего Господа надлежит остерегаться».\n",
      "1.1979604432731699\tI think I washed his body 50 times.\"\tЯ думаю, что я омыла его тело 50 раз.»\n",
      "1.1954915649299516\tThere has come to you clear evidence from your Lord.\tК вам пришло ясное знамение от вашего Господа.\n",
      "1.1941585356247322\t\"15,000 attend dawn service\".\t15,000 attend dawn service (англ.).\n",
      "1.1916203199426767\tAnd in the mountains they suffer a calamity.\tИ в горах их постигла беда.\n",
      "1.1913226864413053\tAsk anybody, particularly the critics.\"\tСпросите кого угодно, в особенности критиков.»\n",
      "1.1881032124947857\t\"Wiranto – survivor with iron will\".\t«Wiranto — survivor with iron will».\n",
      "1.1865469635358286\t\"They Saved Lisa's Brain\".\t«They Saved Lisa’s Brain» (рус.\n",
      "1.1863071915494687\tHowever, patients liked banknotes or coins of the Japan Bank.\tОднако пациенты предпочитали банкноты или монеты Банка Японии.\n",
      "---------------------\n",
      "File length ...\n",
      "5203872 data/WikiMatrix.en-ru.tsv\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/\n",
    "print('Downloading data ...')\n",
    "!wget https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ru.tsv.gz -O data/WikiMatrix.en-ru.tsv.gz\n",
    "print('---------------------')\n",
    "print('Unzipping file ...')\n",
    "!gunzip -k -f data/WikiMatrix.en-ru.tsv.gz\n",
    "print('---------------------')\n",
    "print('Peek into the file')\n",
    "!head -10 data/WikiMatrix.en-ru.tsv\n",
    "print('---------------------')\n",
    "print('File length ...')\n",
    "!wc -l data/WikiMatrix.en-ru.tsv\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a62f9e",
   "metadata": {},
   "source": [
    "## Filter Based on LASER Confidence\n",
    "\n",
    "LASER (https://arxiv.org/abs/1812.10464) is a multi-lingual neural sentence embedding model that is often used for cross-lingual sentence/document retrieval. Similarities in the embedding space are often used as proxies for cross-lingual similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21608388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "======= TSV Conf Filtering =========\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering file by confidence 1.04: 100%|██████████| 5203872/5203872 [00:12<00:00, 424012.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence score average  : 1.0628594097124588\n",
      "Confidence score variance : 0.0003311031014927841\n",
      "Kept 1661908 out of 5203872 after conversion (31.935989201886596%)\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def num_lines_in_file(fname):\n",
    "    \"\"\"\n",
    "    Returns the number of lines in a file.\n",
    "    \"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        for i, _ in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "def filter_tsv_with_conf(\n",
    "    input_file, output_file_lang_1, output_file_lang_2,\n",
    "    confidence_threshold=None, confidence_column=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters a tsv file that has confidence scores associated with each parallel example.\n",
    "\n",
    "    For example:\n",
    "\n",
    "    1.23 \\t This is a sentence in lang1 \\t This is a sentence in lang2\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print('====================================')\n",
    "    print('======= TSV Conf Filtering =========')\n",
    "    print('====================================')\n",
    "    print()\n",
    "    num_lines = num_lines_in_file(input_file)\n",
    "    scores = []\n",
    "    num_output_lines = 0\n",
    "    lang_1_col = 0\n",
    "    lang_2_col = 1\n",
    "    with open(input_file, 'r') as f, \\\n",
    "        open(output_file_lang_1, 'w') as f_out_1, \\\n",
    "        open(output_file_lang_2, 'w') as f_out_2:\n",
    "        for line in tqdm(f, total=num_lines, desc=f\"Filtering file by confidence {confidence_threshold}\"):\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            if confidence_threshold is not None and float(line[confidence_column]) < confidence_threshold:\n",
    "                continue\n",
    "            else:\n",
    "                if confidence_threshold is not None:\n",
    "                    scores.append(float(line[confidence_column]))\n",
    "                    if confidence_column == 0:\n",
    "                        lang_1_col, lang_2_col = 1, 2\n",
    "                    elif confidence_column == 2:\n",
    "                        lang_1_col, lang_2_col = 0, 1\n",
    "                    elif confidence_column == 1:\n",
    "                        lang_1_col, lang_2_col = 0, 2\n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid Column for confidence {confidence_column}\")\n",
    "                f_out_1.write(line[lang_1_col] + '\\n')\n",
    "                f_out_2.write(line[lang_2_col] + '\\n')\n",
    "                num_output_lines += 1\n",
    "\n",
    "    if confidence_threshold is not None:\n",
    "        print(f'Confidence score average  : {np.mean(scores)}')\n",
    "        print(f'Confidence score variance : {np.var(scores)}')\n",
    "        print(f'Kept {num_output_lines} out of {num_lines} after conversion ({(num_output_lines / num_lines) * 100}%)')\n",
    "        print('====================================')\n",
    "\n",
    "filter_tsv_with_conf(\n",
    "    'data/WikiMatrix.en-ru.tsv',\n",
    "    'data/WikiMatrix.en-ru.en', \n",
    "    'data/WikiMatrix.en-ru.ru',\n",
    "    confidence_threshold=1.04, confidence_column=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a171d1",
   "metadata": {},
   "source": [
    "## Language ID filtering with fastText\n",
    "\n",
    "Noisy parallel corpora often contain sentences that are not in the intended language. A classifier that determines the language in which a sentence is written can be used to filter out sentences that aren't in the appropriate language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d58b7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-13 09:05:38--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 131266198 (125M) [application/octet-stream]\n",
      "Saving to: ‘data/lid.176.bin’\n",
      "\n",
      "data/lid.176.bin    100%[===================>] 125.18M  65.6MB/s    in 1.9s    \n",
      "\n",
      "2021-07-13 09:05:41 (65.6 MB/s) - ‘data/lid.176.bin’ saved [131266198/131266198]\n",
      "\n",
      "\n",
      "====================================\n",
      "====== Language ID Filtering =======\n",
      "====================================\n",
      "\n",
      "processed lines / total number of lines: 100%|▉| 1658535/1661908 [00:12<00:00, 1\n",
      "\n",
      "-----------------------------------------\n",
      "Number of removed sentences:\n",
      "-----------------------------------------\n",
      "\n",
      "33671 data/WikiMatrix.en-ru.langidfilter.removed.ru\n",
      "\n",
      "-----------------------------------------\n",
      "Examples of removed sentences\n",
      "-----------------------------------------\n",
      "\n",
      "Ask Sylvia!\tСпроси Сильвию!\n",
      "Любовь Шутова: Теперь ответственности больше.\tЛюбовь Шутова: теперь ответственности больше.\n",
      "\"Александр Митта: Нужно понимать, как управлять вниманием зрителя\" .\tАлександр Митта: Нужно понимать, как управлять вниманием зрителя.\n",
      "Пышка, я тебя знаю: вкус советского детства.\tПышка, я тебя знаю: вкус советского детства.\n",
      "Ethnologue (\tEthnologue  (англ.)\n",
      "\"Почему «Доброе утро» лишилось двух ведущих\".\tПочему «Доброе утро» лишилось двух ведущих? (неопр.).\n",
      "Time Masters Vol.\tTime Masters Vol.\n",
      "1999 – Which pronunciation do you prefer?.\t1999 — Which pronunciation do you prefer?.\n",
      "1980 – The brogue that isn't.\t1980 — The brogue that isn’t.\n",
      "\"Формы новые нужны, драмы всякие важны.\tФормы новые нужны, драмы всякие важны.\n",
      "paste: write error: Broken pipe\n",
      "paste: write error\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -O data/lid.176.bin\n",
    "print()\n",
    "print('====================================')\n",
    "print('====== Language ID Filtering =======')\n",
    "print('====================================')\n",
    "print()\n",
    "\n",
    "!python ../../scripts/neural_machine_translation/filter_langs_nmt.py \\\n",
    "    --input-src data/WikiMatrix.en-ru.en  \\\n",
    "    --input-tgt data/WikiMatrix.en-ru.ru \\\n",
    "    --output-src data/WikiMatrix.en-ru.langidfilter.en  \\\n",
    "    --output-tgt data/WikiMatrix.en-ru.langidfilter.ru  \\\n",
    "    --source-lang en \\\n",
    "    --target-lang ru \\\n",
    "    --removed-src data/WikiMatrix.en-ru.langidfilter.removed.en  \\\n",
    "    --removed-tgt data/WikiMatrix.en-ru.langidfilter.removed.ru  \\\n",
    "    --fasttext-model data/lid.176.bin\n",
    "\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Number of removed sentences:')\n",
    "print('-----------------------------------------')\n",
    "print()\n",
    "!wc -l data/WikiMatrix.en-ru.langidfilter.removed.ru\n",
    "\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Examples of removed sentences')\n",
    "print('-----------------------------------------')\n",
    "print()\n",
    "\n",
    "!paste -d \"\\t\" \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.removed.en \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.removed.ru \\\n",
    "    | head -10\n",
    "print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb42e92",
   "metadata": {},
   "source": [
    "## Length and Ratio Filtering\n",
    "\n",
    "This step filters out sentences based on their lengths and the ratio between source and target lengths. If (a) src_len / tgt_len or tgt_len / src_len exceed 1.3 or (b) source or target sequence lengths are less than 1 or greater than 250, the sentence pair will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52ff172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'data/mosesdecoder' already exists and is not an empty directory.\n",
      "clean-corpus.perl: processing data/WikiMatrix.en-ru.langidfilter.en & .ru to data/WikiMatrix.en-ru.langidfilter.lengthratio, cutoff 1-250, ratio 1.3\n",
      "..........(100000)..........(200000)..........(300000)..........(400000)..........(500000)..........(600000)..........(700000)..........(800000)..........(900000)..........(1000000)..........(1100000)..........(1200000)..........(1300000)..........(1400000)..........(1500000)..........(1600000)..\n",
      "Input sentences: 1628237  Output sentences:  1138638\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/moses-smt/mosesdecoder data/mosesdecoder\n",
    "!perl data/mosesdecoder/scripts/training/clean-corpus-n.perl -ratio 1.3 \\\n",
    "    data/WikiMatrix.en-ru.langidfilter \\\n",
    "    en ru \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio \\\n",
    "    1 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2b589",
   "metadata": {},
   "source": [
    "## Bicleaner Filtering\n",
    "\n",
    "Bicleaner (https://aclanthology.org/W18-6488/ and https://aclanthology.org/2020.eamt-1.31/) is a tool to identify noisy parallel sentences in translation corpora. It applies 3 different filtering steps:\n",
    "\n",
    "1. Pre-filtering based on 37 rules.\n",
    "2. Language model fluency scores based on n-gram language models trained with kenlm.\n",
    "3. Random forest clasifier that uses all examples filtered out in steps 1 & 2 as \"negative\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9be8d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading En-Ru Bicleaner models.\n",
      "fatal: destination path 'bicleaner' already exists and is not an empty directory.\n",
      "Branch 'bicleaner-0.15' set up to track remote branch 'bicleaner-0.15' from 'origin'.\n",
      "Switched to a new branch 'bicleaner-0.15'\n",
      "--2021-07-13 09:10:13--  https://github.com/bitextor/bicleaner-data/releases/latest/download/en-ru.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/bitextor/bicleaner-data/releases/download/v1.4/en-ru.tar.gz [following]\n",
      "--2021-07-13 09:10:13--  https://github.com/bitextor/bicleaner-data/releases/download/v1.4/en-ru.tar.gz\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/236975479/614a5780-69f8-11eb-831b-11b2dc2c342f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T161122Z&X-Amz-Expires=300&X-Amz-Signature=51e70727e2cdd5ef4202bd41e3d9c588bcca740a05ae8f26e437dac3249c795f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=236975479&response-content-disposition=attachment%3B%20filename%3Den-ru.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-07-13 09:10:13--  https://github-releases.githubusercontent.com/236975479/614a5780-69f8-11eb-831b-11b2dc2c342f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210713T161122Z&X-Amz-Expires=300&X-Amz-Signature=51e70727e2cdd5ef4202bd41e3d9c588bcca740a05ae8f26e437dac3249c795f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=236975479&response-content-disposition=attachment%3B%20filename%3Den-ru.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.110.154, 185.199.111.154, 185.199.108.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.110.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 430927867 (411M) [application/octet-stream]\n",
      "Saving to: ‘./en-ru.tar.gz’\n",
      "\n",
      "en-ru.tar.gz        100%[===================>] 410.96M  62.5MB/s    in 5.2s    \n",
      "\n",
      "2021-07-13 09:10:19 (79.6 MB/s) - ‘./en-ru.tar.gz’ saved [430927867/430927867]\n",
      "\n",
      "en-ru/\n",
      "en-ru/dict-ru.gz\n",
      "en-ru/dict-en.gz\n",
      "en-ru/wordfreq-en.gz\n",
      "en-ru/porn-removal.en.ftz\n",
      "en-ru/train.en-ru\n",
      "en-ru/en-ru.yaml\n",
      "en-ru/lm.en\n",
      "en-ru/porn-removal.txt.en\n",
      "en-ru/wordfreq-ru.gz\n",
      "en-ru/lm.ru\n",
      "en-ru/en-ru.classifier\n",
      "Finished\n",
      "NOTE: This notebook does not install bicleaner from scratch, please install things yourself before running the next few cells.\n",
      "To use Bicleaner please make sure you have BOOST and associated libraries installed ...\n",
      "To install bicleaner, follow setup instructions on the repository - https://github.com/bitextor/bicleaner.\n",
      "Bicleaner also requires kenlm with support for upto 7-gram LMs. Instructions on how to build things are on the repository as well.\n"
     ]
    }
   ],
   "source": [
    "# Note: Fix commit of bicleaner when cloning\n",
    "print('Downloading En-Ru Bicleaner models.')\n",
    "!git clone https://github.com/bitextor/bicleaner\n",
    "!cd bicleaner && git checkout bicleaner-0.15 && cd ..\n",
    "!./bicleaner/utils/download-pack.sh en ru\n",
    "\n",
    "print()\n",
    "print('==============================================================================')\n",
    "print('NOTE: This notebook does not install bicleaner from scratch, please install things yourself before running the next few cells.')\n",
    "print('To use Bicleaner please make sure you have BOOST and associated libraries installed ...')\n",
    "print('To install bicleaner, follow setup instructions on the repository - https://github.com/bitextor/bicleaner.')\n",
    "print('Bicleaner also requires kenlm with support for upto 7-gram LMs. Instructions on how to build things are on the repository as well.')\n",
    "print('==============================================================================')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e8d2f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bicleaner scores ...\n",
      "2021-07-13 09:17:12,611 - INFO - Accuracy histogram: [0.5001, 0.7714543, 0.8156631, 0.8457692, 0.8728746, 0.8924785, 0.8861772, 0.8425685, 0.7294459, 0.5561112]\n",
      "2021-07-13 09:17:12,611 - INFO - Ideal threshold: 0.5\n",
      "2021-07-13 09:17:12,617 - INFO - Arguments processed.\n",
      "2021-07-13 09:17:12,617 - INFO - Executing main program...\n",
      "2021-07-13 09:17:14,028 - INFO - Start mapping\n",
      "2021-07-13 09:17:19,634 - INFO - End mapping\n",
      "2021-07-13 09:18:50,028 - INFO - Finished\n",
      "2021-07-13 09:18:50,028 - INFO - Total: 1138638 rows\n",
      "2021-07-13 09:18:50,028 - INFO - Elapsed time 97.41 s\n",
      "2021-07-13 09:18:50,028 - INFO - Troughput: 11689 rows/s\n",
      "2021-07-13 09:18:50,029 - INFO - Program finished\n"
     ]
    }
   ],
   "source": [
    "print('Generating Bicleaner scores ...')\n",
    "!gawk '{{print \"-\\t-\"}}' \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.en | \\\n",
    "    paste -d \"\\t\" - data/WikiMatrix.en-ru.langidfilter.lengthratio.en \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.ru | \\\n",
    "    bicleaner-classify - - en-ru/en-ru.yaml \\\n",
    "    > data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43059b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering based on Bicleaner scores > 0.6 ...\n",
      "-\t-\tThe glory of the Lord has risen upon thee\".\tКакую же из милостей вашего Господа вы считаете ложью?».\t0.530\n",
      "-\t-\tI think I washed his body 50 times.\"\tЯ думаю, что я омыла его тело 50 раз.»\t0.604\n",
      "-\t-\tThere has come to you clear evidence from your Lord.\tК вам пришло ясное знамение от вашего Господа.\t0.730\n",
      "-\t-\t\"15,000 attend dawn service\".\t15,000 attend dawn service (англ.).\t0.676\n",
      "-\t-\tAsk anybody, particularly the critics.\"\tСпросите кого угодно, в особенности критиков.»\t0.568\n",
      "-\t-\t\"Wiranto – survivor with iron will\".\t«Wiranto — survivor with iron will».\t0\n",
      "-\t-\t\"They Saved Lisa's Brain\".\t«They Saved Lisa’s Brain» (рус.\t0.422\n",
      "-\t-\tHowever, patients liked banknotes or coins of the Japan Bank.\tОднако пациенты предпочитали банкноты или монеты Банка Японии.\t0.764\n",
      "-\t-\tThe Flaming Blade of the Lord’s Retribution.\tЧто ниспосланное Господом наказание вразумит заблудших!\t0.284\n",
      "-\t-\tHe slept quite openly with them all.\"\tОн спал вполне открыто со всеми ними».\t0.728\n",
      "Filtering out English ...\n",
      "Filtering out Russian ...\n",
      "I think I washed his body 50 times.\"\tЯ думаю, что я омыла его тело 50 раз.»\n",
      "There has come to you clear evidence from your Lord.\tК вам пришло ясное знамение от вашего Господа.\n",
      "\"15,000 attend dawn service\".\t15,000 attend dawn service (англ.).\n",
      "However, patients liked banknotes or coins of the Japan Bank.\tОднако пациенты предпочитали банкноты или монеты Банка Японии.\n",
      "He slept quite openly with them all.\"\tОн спал вполне открыто со всеми ними».\n",
      "We need more heroines like you, Tina.\tНам нужно больше таких героинь, как Вы, Тина.\n",
      "He explained: \"This next season will be their senior year, and then that they will graduate.\tОн объяснил: «Этот следующий сезон будет их последним годом в школе и, таким образом, они станут выпускниками.\n",
      "And some of them are in their seventies.\"\tИ некоторые из них находятся в свои семьдесят».\n",
      "\"P.S. What Happened to Brian and the Ponds?\".\tP.S. What Happened to Brian and the Ponds? (англ.).\n",
      "They are beginning to ask their first questions about injustice.\tОни начинают задавать свои первые вопросы о несправедливости.\n",
      "paste: write error: Broken pipe\n",
      "paste: write error\n"
     ]
    }
   ],
   "source": [
    "print('Filtering based on Bicleaner scores > 0.6 ...')\n",
    "!head -10 data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.scores\n",
    "\n",
    "print('Filtering out English ...')\n",
    "!gawk -F \"\\t\" '{if ($5>0.6) {print $3}}' \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.scores > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.en\n",
    "\n",
    "print('Filtering out Russian ...')\n",
    "!gawk -F \"\\t\" '{if ($5>0.6) {print $4}}' \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.scores > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.ru\n",
    "\n",
    "!paste -d \"\\t\" \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.en \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.ru \\\n",
    "    | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726510c",
   "metadata": {},
   "source": [
    "## Normalize Punctuation\n",
    "\n",
    "Punctuation can vary across languages and even between ascii and unicode variants of the same punctuation marker. For example, across languages. For example, in German, quotes are often written as „ and “ while in English we typically just use \". This step normalizes such punctuation differences to use the same character everywhere.\n",
    "\n",
    "We use [moses](https://github.com/moses-smt/mosesdecoder) or [sacremoses](https://github.com/alvations/sacremoses) to normalize punctuation. The moses implementation is in perl while sacremoses is in python with a CLI interface. The perl implementation is buffered and works better for large corpora that may not fit into CPU memory all at once while sacremoses is unbuffered and multi-processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73670d6",
   "metadata": {},
   "source": [
    "### Sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "597e041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing English ...\n",
      "100%|████████████████████████████████| 905234/905234 [00:13<00:00, 66384.80it/s]\n",
      "Normalizing Russian ...\n",
      "100%|████████████████████████████████| 905234/905234 [00:14<00:00, 62549.41it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Normalizing English ...')\n",
    "!sacremoses -j 4 normalize \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.en > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.en\n",
    "\n",
    "print('Normalizing Russian ...')\n",
    "!sacremoses -j 4 normalize \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.ru > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.ru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b0a1f",
   "metadata": {},
   "source": [
    "## Moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f5adaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing English ...\n",
      "Normalizing Russian ...\n"
     ]
    }
   ],
   "source": [
    "print('Normalizing English ...')\n",
    "!perl mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.en > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.moses.norm.en\n",
    "\n",
    "print('Normalizing Russian ...')\n",
    "!perl mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.ru > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.moses.norm.ru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfad64",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c60b90",
   "metadata": {},
   "source": [
    "### Sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7bb4c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing English ...\n",
      "100%|████████████████████████████████| 905234/905234 [00:25<00:00, 35973.36it/s]\n",
      "Tokenizing Russian ...\n",
      "100%|████████████████████████████████| 905234/905234 [00:19<00:00, 47260.96it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizing English ...')\n",
    "!sacremoses -j 4 -l en tokenize -x \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.en > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.en\n",
    "\n",
    "print('Tokenizing Russian ...')\n",
    "!sacremoses -j 4 -l ru tokenize -x \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.ru > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.ru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444bebd7",
   "metadata": {},
   "source": [
    "### Moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21333e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing English ...\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 4\n",
      "Tokenizing Russian ...\n",
      "Tokenizer Version 1.1\n",
      "Language: ru\n",
      "Number of threads: 4\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizing English ...')\n",
    "!perl mosesdecoder/scripts/tokenizer/tokenizer.perl -l en -no-escape -threads 4 \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.moses.norm.en > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.moses.norm.tok.en\n",
    "\n",
    "print('Tokenizing Russian ...')\n",
    "!perl mosesdecoder/scripts/tokenizer/tokenizer.perl -l ru -no-escape -threads 4 \\\n",
    "    < data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.moses.norm.ru > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.moses.norm.tok.ru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5409d",
   "metadata": {},
   "source": [
    "## Segmenting Chinese and Japanese\n",
    "\n",
    "### Jieba segmentation for Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41b4cc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /home/sandeepsub/anaconda3/lib/python3.8/site-packages (0.42.1)\n",
      "--2021-07-13 10:00:14--  https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-zh.tsv.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 340467177 (325M) [application/gzip]\n",
      "Saving to: ‘data/WikiMatrix.en-zh.tsv.gz’\n",
      "\n",
      "data/WikiMatrix.en- 100%[===================>] 324.69M  52.8MB/s    in 6.0s    \n",
      "\n",
      "2021-07-13 10:00:20 (53.7 MB/s) - ‘data/WikiMatrix.en-zh.tsv.gz’ saved [340467177/340467177]\n",
      "\n",
      "\n",
      "-----------------------------------------\n",
      "Chinese text before segmentation ...\n",
      "-----------------------------------------\n",
      "\n",
      "這是你們的主所降示的減輕和慈恩。\n",
      "以昭事上主，闡揚天主聖教為本。\n",
      " 你的主的言辞，诚实极了，公平极了。\n",
      "祈求圣主明鉴施恩。\n",
      " 因上主之名而來的，當受讚頌!\n",
      "例如ன是ṉa（有母音a），而ன்是ṉ（沒有母音）。\n",
      "按座主，即大众一座之主，亦即住持。\n",
      "世尊知摩尼珠髻聚落主去已。\n",
      "正月，朝見其主（王）龔。\n",
      "” 念「我作证，萬物非主，唯有真主。\n",
      "\n",
      "-----------------------------------------\n",
      "Segmenting Chinese text ...\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------\n",
      "Chinese text after segmentation ...\n",
      "這是 你 們 的 主所 降示 的 減輕 和慈恩 。\n",
      "以昭事 上主 ， 闡揚 天主 聖教為 本 。\n",
      "  你 的 主 的 言辞 ， 诚实 极了 ， 公平 极了 。\n",
      "祈求 圣主 明鉴 施恩 。\n",
      "  因上 主之名 而 來 的 ， 當 受 讚 頌 !\n",
      "例如 ன 是 ṉ a （ 有 母音 a ） ， 而 ன ் 是 ṉ （ 沒有 母音 ） 。\n",
      "按座 主 ， 即 大众 一座 之主 ， 亦 即 住持 。\n",
      "世尊 知摩尼珠 髻 聚落 主去 已 。\n",
      "正月 ， 朝見 其主 （ 王 ） 龔 。\n",
      "”   念 「 我 作证 ， 萬物 非主 ， 唯有 真主 。\n",
      "-----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n",
    "import jieba\n",
    "!wget https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-zh.tsv.gz -O data/WikiMatrix.en-zh.tsv.gz\n",
    "!gunzip -k -f data/WikiMatrix.en-zh.tsv.gz\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Chinese text before segmentation ...')\n",
    "print('-----------------------------------------')\n",
    "print()\n",
    "\n",
    "!awk -F \"\\t\" '{print $3}' data/WikiMatrix.en-zh.tsv | head -10\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Segmenting Chinese text ...')\n",
    "print('-----------------------------------------')\n",
    "print()\n",
    "\n",
    "zh_lines = []\n",
    "with open('data/WikiMatrix.en-zh.tsv', 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        line = line.strip().split('\\t')[2]\n",
    "        zh_lines.append(' '.join(jieba.cut(line)))\n",
    "        if idx == 100:\n",
    "            break\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Chinese text after segmentation ...')\n",
    "print('\\n'.join(zh_lines[:10]))\n",
    "print('-----------------------------------------')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "489bd915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mecab-python3 in /home/sandeepsub/anaconda3/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: ipadic in /home/sandeepsub/anaconda3/lib/python3.8/site-packages (1.0.0)\n",
      "--2021-07-13 10:00:29--  https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ja.tsv.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 440814991 (420M) [application/gzip]\n",
      "Saving to: ‘data/WikiMatrix.en-ja.tsv.gz’\n",
      "\n",
      "data/WikiMatrix.en- 100%[===================>] 420.39M  48.4MB/s    in 8.8s    \n",
      "\n",
      "2021-07-13 10:00:38 (47.8 MB/s) - ‘data/WikiMatrix.en-ja.tsv.gz’ saved [440814991/440814991]\n",
      "\n",
      "\n",
      "-----------------------------------------\n",
      "Japanese text before segmentation ...\n",
      "-----------------------------------------\n",
      "\n",
      "主のものは以下の通り。\n",
      "（詳細はソネットを参照） Shall I compare thee to a summer’s day?\n",
      "主の為なら主にすら嘘をつく。\n",
      "一般には後主と称されている。\n",
      " 主の御名によって来られる方を讃えよ。\n",
      "魂主の手首には「主の証」として具現化する。\n",
      "主の御名を呪う者は死刑に処せられる。\n",
      "これはかれらの行いに対する、アッラーの見せしめのための懲しめである。\n",
      "で主に道外向けに同時配信。\n",
      "最期には人主に至るであろう。\n",
      "\n",
      "-----------------------------------------\n",
      "Segmenting Japanese text ...\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------\n",
      "Japanese text after segmentation ...\n",
      "這是 你 們 的 主所 降示 的 減輕 和慈恩 。\n",
      "以昭事 上主 ， 闡揚 天主 聖教為 本 。\n",
      "  你 的 主 的 言辞 ， 诚实 极了 ， 公平 极了 。\n",
      "祈求 圣主 明鉴 施恩 。\n",
      "  因上 主之名 而 來 的 ， 當 受 讚 頌 !\n",
      "例如 ன 是 ṉ a （ 有 母音 a ） ， 而 ன ் 是 ṉ （ 沒有 母音 ） 。\n",
      "按座 主 ， 即 大众 一座 之主 ， 亦 即 住持 。\n",
      "世尊 知摩尼珠 髻 聚落 主去 已 。\n",
      "正月 ， 朝見 其主 （ 王 ） 龔 。\n",
      "”   念 「 我 作证 ， 萬物 非主 ， 唯有 真主 。\n",
      "-----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3\n",
    "!pip install ipadic\n",
    "\n",
    "import MeCab\n",
    "import ipadic\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ja.tsv.gz -O data/WikiMatrix.en-ja.tsv.gz\n",
    "!gunzip -k -f data/WikiMatrix.en-ja.tsv.gz\n",
    "\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Japanese text before segmentation ...')\n",
    "print('-----------------------------------------')\n",
    "print()\n",
    "\n",
    "!awk -F \"\\t\" '{print $3}' data/WikiMatrix.en-ja.tsv | head -10\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Segmenting Japanese text ...')\n",
    "print('-----------------------------------------')\n",
    "print()\n",
    "\n",
    "mecab_tokenizer = MeCab.Tagger(ipadic.MECAB_ARGS + \" -Owakati\")\n",
    "ja_lines = []\n",
    "with open('data/WikiMatrix.en-ja.tsv', 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        line = line.strip().split('\\t')[2]\n",
    "        ja_lines.append(mecab_tokenizer.parse(line))\n",
    "        if idx == 100:\n",
    "            break\n",
    "print()\n",
    "print('-----------------------------------------')\n",
    "print('Japanese text after segmentation ...')\n",
    "print('\\n'.join(zh_lines[:10]))\n",
    "print('-----------------------------------------')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a079efe",
   "metadata": {},
   "source": [
    "## Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55d98bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "========== De-duplicate ============\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating files: 100%|██████████| 905234/905234 [00:03<00:00, 275236.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 905231 out of 905234 after deduplication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import xxhash\n",
    "\n",
    "def dedup_file(input_file_lang_1, input_file_lang_2, output_file_lang_1, output_file_lang_2):\n",
    "    print()\n",
    "    print('====================================')\n",
    "    print('========== De-duplicate ============')\n",
    "    print('====================================')\n",
    "    print()\n",
    "    num_lines = num_lines_in_file(input_file_lang_1)\n",
    "    hashes = set()\n",
    "    num_output_lines = 0\n",
    "    with open(input_file_lang_1, 'r') as f_lang1, \\\n",
    "        open(input_file_lang_2, 'r')  as f_lang2, \\\n",
    "        open(output_file_lang_1, 'w') as f_out_lang1, \\\n",
    "        open(output_file_lang_2, 'w') as f_out_lang2:\n",
    "        for line_1, line_2 in tqdm(zip(f_lang1, f_lang2), total=num_lines, desc=f\"Deduplicating files\"):\n",
    "            parallel_hash = xxhash.xxh64((line_1.strip() + '\\t' + line_2.strip()).encode('utf-8')).hexdigest()\n",
    "            if parallel_hash not in hashes:\n",
    "                hashes.add(parallel_hash)\n",
    "                f_out_lang1.write(line_1.strip() + '\\n')\n",
    "                f_out_lang2.write(line_2.strip() + '\\n')\n",
    "                num_output_lines += 1\n",
    "\n",
    "    print(f\"Kept {num_output_lines} out of {num_lines} after deduplication\")\n",
    "\n",
    "dedup_file(\n",
    "    'data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.en',\n",
    "    'data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.ru',\n",
    "    'data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.en',\n",
    "    'data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.ru'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c181a",
   "metadata": {},
   "source": [
    "## Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "413734bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is an effective method for countries where identification documents for citizens are not always standardised or institutionalised .\tЭто эффективный способ для стран , где удостоверяющие личность документы для граждан не стандартизированы или необязательны .\r\n",
      "He claimed they there were 74 persons dead and 345 persons injured .\tОн утверждал , что в результате этих действий 74 человек погибли и 345 были ранены .\r\n",
      "Its test version was released on 20 April 2006 , and within three weeks the encyclopedia had grown to more than 90,000 articles , surpassing the number in Chinese Wikipedia .\tТестовая версия появилась 20 апреля 2006 года , и по прошествии трёх недель энциклопедия содержала более 90 тысяч статей , превзойдя по этому показателю китайскую Википедию .\r\n",
      "These theatres are smaller than Broadway theatres .\tПо своим размерам эти театры меньше бродвейских .\r\n",
      "In October 2010 , he was made a Distinguished Supporter of the British Humanist Association .\tВ октябре 2010 года он был удостоен звания Почётного члена Британской гуманистической ассоциации .\r\n",
      "Diesel Car magazine said of the BX \" We can think of no other car currently on sale in the UK that comes anywhere near approaching the BX Turbo 's combination of performance , accommodation and economy . \"\tЖурнал Diesel Car говорил о BX \" мы не можем придумать никакого другого автомобиля , в настоящее время продаваемого в Великобритании , находящийся близко к исполнению BX Turbo \" В 1989 году , среди моделей BX прошли незначительные изменения и усовершенствования , включая новые колпаки колес и интерьерные ткани .\r\n",
      "A stopped-flow instrument is a rapid mixing device used to study the chemical kinetics of fast reactions in solution .\tИнструмент для метода остановленной струи это устройство для быстрого смешивания , которое используется для изучения химической кинетики быстрых реакций в растворе .\r\n",
      "Six German battalions landed in Hanko and helped the Finnish army to drive the Red Guards from most of their strongholds .\tШесть немецких батальонов высадились в Ханко и помогли финской армии вытеснить красногвардейцев из большей части опорных пунктов .\r\n",
      "Larroca and Claremont had a three-year-long run on the title .\tУ Ларрока и Клермонта был трехлетний , долгосрочный перспектив на название .\r\n",
      "Vagner began his career with União Barbarense and in spring 2006 he joined Darida .\tВагнер начал свою карьеру с União Barbarense , и в апреле 2006 присоединился к Дариде .\r\n",
      "paste: write error: Broken pipe\r\n",
      "paste: write error\r\n"
     ]
    }
   ],
   "source": [
    "!shuf --random-source=data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.en \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.en > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.en\n",
    "\n",
    "!shuf --random-source=data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.en \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.ru > \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.ru\n",
    "\n",
    "!paste -d \"\\t\" \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.en \\\n",
    "    data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.ru \\\n",
    "    | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "097e3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/tarred_dataset_en_ru_8k_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a9f26",
   "metadata": {},
   "source": [
    "## Tarred Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b045df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-07-13 10:35:50 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:35:52 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:35:52 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:35:52 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:35:52 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:35:52 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-07-13 10:35:52 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2021-07-13 10:35:52 preproc_mt_data:823] Encoder tokenizer model data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model not found. Training tokenizer model.\n",
      "Training parameters\n",
      "  input: data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.en\n",
      "  model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model\n",
      "  vocab_size: 32000\n",
      "  n_threads: 8\n",
      "  character_coverage: 0.999\n",
      "  pad: 0\n",
      "  unk: 1\n",
      "  bos: 2\n",
      "  eos: 3\n",
      "\n",
      "reading file...\n",
      "learning bpe...\n",
      "number of unique characters in the training data: 3417\n",
      "number of deleted characters: 3343\n",
      "number of unique characters left: 74\n",
      "id: 1000=82+28                freq: 9914        subword: ink=in+k\n",
      "id: 2000=1498+1080            freq: 4000        subword: ▁episode=▁epis+ode\n",
      "id: 3000=150+144              freq: 2356        subword: ▁Bet=▁B+et\n",
      "id: 4000=245+1428             freq: 1627        subword: ▁extended=▁ex+tended\n",
      "id: 5000=732+282              freq: 1200        subword: ▁charge=▁char+ge\n",
      "id: 6000=1950+2544            freq: 932         subword: ▁encounter=▁enc+ounter\n",
      "id: 7000=198+101              freq: 751         subword: oses=os+es\n",
      "id: 8000=5572+85              freq: 624         subword: ▁Lebanon=▁Leban+on\n",
      "id: 9000=207+830              freq: 527         subword: ▁Jason=▁J+ason\n",
      "id: 10000=5959+324            freq: 451         subword: ▁procedure=▁proced+ure\n",
      "id: 11000=7667+112            freq: 387         subword: ▁monitoring=▁monitor+ing\n",
      "id: 12000=115+1509            freq: 336         subword: ▁tours=▁to+urs\n",
      "id: 13000=1392+9433           freq: 297         subword: ▁legitimate=▁leg+itimate\n",
      "id: 14000=6922+1659           freq: 264         subword: ▁undertaken=▁undert+aken\n",
      "id: 15000=13215+26            freq: 235         subword: .m.=.m+.\n",
      "id: 16000=2328+335            freq: 213         subword: ▁Jackie=▁Jack+ie\n",
      "id: 17000=2876+1417           freq: 193         subword: ▁happiness=▁happ+iness\n",
      "id: 18000=8042+271            freq: 176         subword: ▁Literary=▁Liter+ary\n",
      "id: 19000=16350+281           freq: 161         subword: ▁mystical=▁myst+ical\n",
      "id: 20000=658+166             freq: 148         subword: ▁Shot=▁Sh+ot\n",
      "id: 21000=2509+6421           freq: 137         subword: ▁favoured=▁fav+oured\n",
      "id: 22000=4197+21462          freq: 126         subword: ▁cardiovascular=▁card+iovascular\n",
      "id: 23000=2602+128            freq: 117         subword: ▁superst=▁super+st\n",
      "id: 24000=129+572             freq: 109         subword: omics=om+ics\n",
      "id: 25000=4227+901            freq: 102         subword: ▁Younger=▁Youn+ger\n",
      "id: 26000=2197+17             freq: 95          subword: -gu=-g+u\n",
      "id: 27000=150+1549            freq: 89          subword: ▁Bott=▁B+ott\n",
      "id: 28000=1781+11361          freq: 84          subword: ▁watercol=▁water+col\n",
      "id: 29000=5050+37             freq: 79          subword: -99=-9+9\n",
      "id: 30000=6250+4102           freq: 75          subword: opolitan=opol+itan\n",
      "id: 31000=591+10709           freq: 71          subword: ▁reprise=▁rep+rise\n",
      "model saved to: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model\n",
      "[NeMo I 2021-07-13 10:35:54 preproc_mt_data:862] Decoder tokenizer model data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model not found. Training tokenizer model.\n",
      "Training parameters\n",
      "  input: data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.ru\n",
      "  model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model\n",
      "  vocab_size: 32000\n",
      "  n_threads: 8\n",
      "  character_coverage: 0.999\n",
      "  pad: 0\n",
      "  unk: 1\n",
      "  bos: 2\n",
      "  eos: 3\n",
      "\n",
      "reading file...\n",
      "learning bpe...\n",
      "number of unique characters in the training data: 3344\n",
      "number of deleted characters: 3217\n",
      "number of unique characters left: 127\n",
      "id: 1000=14+139               freq: 11967       subword: вра=в+ра\n",
      "id: 2000=1903+508             freq: 5095        subword: ▁необходи=▁необ+ходи\n",
      "id: 3000=162+160              freq: 3051        subword: тари=та+ри\n",
      "id: 4000=360+559              freq: 2078        subword: ствовал=ство+вал\n",
      "id: 5000=2755+238             freq: 1544        subword: ▁Париже=▁Пари+же\n",
      "id: 6000=366+151              freq: 1205        subword: филь=фи+ль\n",
      "id: 7000=2443+2297            freq: 986         subword: ▁недостато=▁недо+стато\n",
      "id: 8000=674+48               freq: 828         subword: ▁42=▁4+2\n",
      "id: 9000=2688+272             freq: 710         subword: ▁оценку=▁оцен+ку\n",
      "id: 10000=200+164             freq: 618         subword: емле=ем+ле\n",
      "id: 11000=161+1796            freq: 543         subword: ▁нашли=▁на+шли\n",
      "id: 12000=941+1608            freq: 487         subword: ▁Виктор=▁Ви+ктор\n",
      "id: 13000=849+184             freq: 438         subword: ▁Сици=▁Си+ци\n",
      "id: 14000=302+7               freq: 397         subword: туи=ту+и\n",
      "id: 15000=152+672             freq: 362         subword: ▁поход=▁по+ход\n",
      "id: 16000=9582+3315           freq: 331         subword: ▁Holly=▁Hol+ly\n",
      "id: 17000=1280+5              freq: 306         subword: ложено=ложен+о\n",
      "id: 18000=3553+4359           freq: 283         subword: ▁выдающиеся=▁выда+ющиеся\n",
      "id: 19000=14458+440           freq: 262         subword: ▁Узбе=▁Уз+бе\n",
      "id: 20000=171+1742            freq: 243         subword: китай=ки+тай\n",
      "id: 21000=1562+437            freq: 228         subword: ▁Сандер=▁Сан+дер\n",
      "id: 22000=281+740             freq: 214         subword: ▁мотор=▁мо+тор\n",
      "id: 23000=17705+533           freq: 200         subword: ▁Словении=▁Слов+ении\n",
      "id: 24000=10102+19            freq: 189         subword: ▁сферу=▁сфер+у\n",
      "id: 25000=258+155             freq: 178         subword: шины=ши+ны\n",
      "id: 26000=11841+35            freq: 169         subword: ▁хеш=▁хе+ш\n",
      "id: 27000=145+14261           freq: 160         subword: ▁окис=▁о+кис\n",
      "id: 28000=456+1002            freq: 152         subword: ственному=ствен+ному\n",
      "id: 29000=66+605              freq: 144         subword: uis=u+is\n",
      "id: 30000=195+18              freq: 137         subword: скоп=ско+п\n",
      "id: 31000=9219+944            freq: 131         subword: ylus=yl+us\n",
      "model saved to: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-07-13 10:35:57 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:35:57 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo W 2021-07-13 10:35:59 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:35:59 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:00 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2021-07-13 10:36:01 preproc_mt_data:406] Found 905231 source lines and  905231 target lines.\n",
      "[NeMo I 2021-07-13 10:36:01 preproc_mt_data:411] Found 10 fragments to parallelize over.\n",
      "[NeMo W 2021-07-13 10:36:03 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:03 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:03 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:03 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:04 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:04 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:04 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:04 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:04 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:04 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "################################################################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /home/sandeepsub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-07-13 10:36:05 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmpmsh9esk6...\n",
      "Tokenizing sentence: 0it [00:00, ?it/s]\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmpzr_42mmu...\n",
      "Tokenizing sentence: 0it [00:00, ?it/s]\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmp_s32gzel...\n",
      "Tokenizing sentence:   0%|                           | 0/100000 [00:00<?, ?it/s][NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmpfxypi1sb...\n",
      "Tokenizing sentence:   0%|                           | 0/100000 [00:00<?, ?it/s][NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmp_k884w3j...\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmp6d8nz36k...\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "Tokenizing sentence:   0%|                           | 0/100000 [00:00<?, ?it/s][NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.encoder.32000.BPE.model with r2l: False.\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmpi7z3lo3h...\n",
      "Tokenizing sentence:   4%|▌            | 4182/100000 [00:00<00:02, 41811.17it/s][NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmpr1s5b8ff...\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmps2fmxg5r...\n",
      "[NeMo I 2021-07-13 10:36:06 data_preprocessing:379] Tokenizing dataset /tmp/tmp06w414t1...\n",
      "Tokenizing sentence:   0%|                           | 0/100000 [00:00<?, ?it/s][NeMo I 2021-07-13 10:36:06 tokenizer_utils:136] Getting YouTokenToMeTokenizer with model: data/tarred_dataset_en_ru_8k_tokens/tokenizer.decoder.32000.BPE.model with r2l: False.\n",
      "Tokenizing sentence:   4%|▌            | 4229/100000 [00:00<00:02, 42283.59it/s][NeMo I 2021-07-13 10:36:07 data_preprocessing:379] Tokenizing dataset /tmp/tmpi9l7i9q0...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41781.64it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmpldbeju29...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41219.72it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmp7u31aa7_...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41610.15it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmpw31_duin...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41449.11it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmp3y29rgcp...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41626.26it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmp1tx33m3h...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41706.88it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmpeipdmynq...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41754.71it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmpg8xrun10...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41375.47it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmpjf4yxhhl...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41769.12it/s]\n",
      "[NeMo I 2021-07-13 10:36:09 data_preprocessing:379] Tokenizing dataset /tmp/tmpaom90y9o...\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 40509.99it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41010.81it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 40449.35it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 40608.73it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41044.88it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41037.04it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 41026.73it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 40403.50it/s]\n",
      "Tokenizing sentence: 100%|███████████| 100000/100000 [00:02<00:00, 40954.88it/s]\n",
      "[NeMo I 2021-07-13 10:36:13 preproc_mt_data:470] Number of batches discarded: 3, total batches kept: 8260\n"
     ]
    }
   ],
   "source": [
    "!python /home/sandeepsub/code/NeMo/examples/nlp/machine_translation/create_tarred_parallel_dataset.py \\\n",
    "    --src_fname data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.en \\\n",
    "    --tgt_fname data/WikiMatrix.en-ru.langidfilter.lengthratio.bicleaner.60.sacremoses.norm.tok.dedup.shuf.ru \\\n",
    "    --out_dir data/tarred_dataset_en_ru_8k_tokens \\\n",
    "    --clean \\\n",
    "    --encoder_tokenizer_name yttm \\\n",
    "    --encoder_tokenizer_vocab_size 32000 \\\n",
    "    --encoder_tokenizer_coverage 0.999 \\\n",
    "    --encoder_tokenizer_bpe_dropout 0.1 \\\n",
    "    --decoder_tokenizer_name yttm \\\n",
    "    --decoder_tokenizer_vocab_size 32000 \\\n",
    "    --decoder_tokenizer_coverage 0.999 \\\n",
    "    --decoder_tokenizer_bpe_dropout 0.1 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --min_seq_length 1 \\\n",
    "    --tokens_in_batch 8000 \\\n",
    "    --lines_per_dataset_fragment 100000 \\\n",
    "    --num_batches_per_tarfile 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "990265e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.tokens.8000.json\t      parallel.batches.tokens.8000.287.tar\r\n",
      "parallel.batches.tokens.8000.0.tar    parallel.batches.tokens.8000.288.tar\r\n",
      "parallel.batches.tokens.8000.100.tar  parallel.batches.tokens.8000.289.tar\r\n",
      "parallel.batches.tokens.8000.101.tar  parallel.batches.tokens.8000.28.tar\r\n",
      "parallel.batches.tokens.8000.102.tar  parallel.batches.tokens.8000.290.tar\r\n",
      "parallel.batches.tokens.8000.103.tar  parallel.batches.tokens.8000.291.tar\r\n",
      "parallel.batches.tokens.8000.104.tar  parallel.batches.tokens.8000.292.tar\r\n",
      "parallel.batches.tokens.8000.105.tar  parallel.batches.tokens.8000.293.tar\r\n",
      "parallel.batches.tokens.8000.106.tar  parallel.batches.tokens.8000.294.tar\r\n",
      "parallel.batches.tokens.8000.107.tar  parallel.batches.tokens.8000.295.tar\r\n",
      "parallel.batches.tokens.8000.108.tar  parallel.batches.tokens.8000.296.tar\r\n",
      "parallel.batches.tokens.8000.109.tar  parallel.batches.tokens.8000.297.tar\r\n",
      "parallel.batches.tokens.8000.10.tar   parallel.batches.tokens.8000.298.tar\r\n",
      "parallel.batches.tokens.8000.110.tar  parallel.batches.tokens.8000.299.tar\r\n",
      "parallel.batches.tokens.8000.111.tar  parallel.batches.tokens.8000.29.tar\r\n",
      "parallel.batches.tokens.8000.112.tar  parallel.batches.tokens.8000.2.tar\r\n",
      "parallel.batches.tokens.8000.113.tar  parallel.batches.tokens.8000.300.tar\r\n",
      "parallel.batches.tokens.8000.114.tar  parallel.batches.tokens.8000.301.tar\r\n",
      "parallel.batches.tokens.8000.115.tar  parallel.batches.tokens.8000.302.tar\r\n",
      "parallel.batches.tokens.8000.116.tar  parallel.batches.tokens.8000.303.tar\r\n",
      "parallel.batches.tokens.8000.117.tar  parallel.batches.tokens.8000.304.tar\r\n",
      "parallel.batches.tokens.8000.118.tar  parallel.batches.tokens.8000.305.tar\r\n",
      "parallel.batches.tokens.8000.119.tar  parallel.batches.tokens.8000.306.tar\r\n",
      "parallel.batches.tokens.8000.11.tar   parallel.batches.tokens.8000.307.tar\r\n",
      "parallel.batches.tokens.8000.120.tar  parallel.batches.tokens.8000.308.tar\r\n",
      "parallel.batches.tokens.8000.121.tar  parallel.batches.tokens.8000.309.tar\r\n",
      "parallel.batches.tokens.8000.122.tar  parallel.batches.tokens.8000.30.tar\r\n",
      "parallel.batches.tokens.8000.123.tar  parallel.batches.tokens.8000.310.tar\r\n",
      "parallel.batches.tokens.8000.124.tar  parallel.batches.tokens.8000.311.tar\r\n",
      "parallel.batches.tokens.8000.125.tar  parallel.batches.tokens.8000.312.tar\r\n",
      "parallel.batches.tokens.8000.126.tar  parallel.batches.tokens.8000.313.tar\r\n",
      "parallel.batches.tokens.8000.127.tar  parallel.batches.tokens.8000.314.tar\r\n",
      "parallel.batches.tokens.8000.128.tar  parallel.batches.tokens.8000.315.tar\r\n",
      "parallel.batches.tokens.8000.129.tar  parallel.batches.tokens.8000.316.tar\r\n",
      "parallel.batches.tokens.8000.12.tar   parallel.batches.tokens.8000.317.tar\r\n",
      "parallel.batches.tokens.8000.130.tar  parallel.batches.tokens.8000.318.tar\r\n",
      "parallel.batches.tokens.8000.131.tar  parallel.batches.tokens.8000.319.tar\r\n",
      "parallel.batches.tokens.8000.132.tar  parallel.batches.tokens.8000.31.tar\r\n",
      "parallel.batches.tokens.8000.133.tar  parallel.batches.tokens.8000.320.tar\r\n",
      "parallel.batches.tokens.8000.134.tar  parallel.batches.tokens.8000.321.tar\r\n",
      "parallel.batches.tokens.8000.135.tar  parallel.batches.tokens.8000.322.tar\r\n",
      "parallel.batches.tokens.8000.136.tar  parallel.batches.tokens.8000.323.tar\r\n",
      "parallel.batches.tokens.8000.137.tar  parallel.batches.tokens.8000.324.tar\r\n",
      "parallel.batches.tokens.8000.138.tar  parallel.batches.tokens.8000.325.tar\r\n",
      "parallel.batches.tokens.8000.139.tar  parallel.batches.tokens.8000.326.tar\r\n",
      "parallel.batches.tokens.8000.13.tar   parallel.batches.tokens.8000.327.tar\r\n",
      "parallel.batches.tokens.8000.140.tar  parallel.batches.tokens.8000.328.tar\r\n",
      "parallel.batches.tokens.8000.141.tar  parallel.batches.tokens.8000.329.tar\r\n",
      "parallel.batches.tokens.8000.142.tar  parallel.batches.tokens.8000.32.tar\r\n",
      "parallel.batches.tokens.8000.143.tar  parallel.batches.tokens.8000.330.tar\r\n",
      "parallel.batches.tokens.8000.144.tar  parallel.batches.tokens.8000.331.tar\r\n",
      "parallel.batches.tokens.8000.145.tar  parallel.batches.tokens.8000.332.tar\r\n",
      "parallel.batches.tokens.8000.146.tar  parallel.batches.tokens.8000.333.tar\r\n",
      "parallel.batches.tokens.8000.147.tar  parallel.batches.tokens.8000.334.tar\r\n",
      "parallel.batches.tokens.8000.148.tar  parallel.batches.tokens.8000.335.tar\r\n",
      "parallel.batches.tokens.8000.149.tar  parallel.batches.tokens.8000.336.tar\r\n",
      "parallel.batches.tokens.8000.14.tar   parallel.batches.tokens.8000.337.tar\r\n",
      "parallel.batches.tokens.8000.150.tar  parallel.batches.tokens.8000.338.tar\r\n",
      "parallel.batches.tokens.8000.151.tar  parallel.batches.tokens.8000.339.tar\r\n",
      "parallel.batches.tokens.8000.152.tar  parallel.batches.tokens.8000.33.tar\r\n",
      "parallel.batches.tokens.8000.153.tar  parallel.batches.tokens.8000.340.tar\r\n",
      "parallel.batches.tokens.8000.154.tar  parallel.batches.tokens.8000.341.tar\r\n",
      "parallel.batches.tokens.8000.155.tar  parallel.batches.tokens.8000.342.tar\r\n",
      "parallel.batches.tokens.8000.156.tar  parallel.batches.tokens.8000.343.tar\r\n",
      "parallel.batches.tokens.8000.157.tar  parallel.batches.tokens.8000.344.tar\r\n",
      "parallel.batches.tokens.8000.158.tar  parallel.batches.tokens.8000.345.tar\r\n",
      "parallel.batches.tokens.8000.159.tar  parallel.batches.tokens.8000.346.tar\r\n",
      "parallel.batches.tokens.8000.15.tar   parallel.batches.tokens.8000.347.tar\r\n",
      "parallel.batches.tokens.8000.160.tar  parallel.batches.tokens.8000.348.tar\r\n",
      "parallel.batches.tokens.8000.161.tar  parallel.batches.tokens.8000.349.tar\r\n",
      "parallel.batches.tokens.8000.162.tar  parallel.batches.tokens.8000.34.tar\r\n",
      "parallel.batches.tokens.8000.163.tar  parallel.batches.tokens.8000.350.tar\r\n",
      "parallel.batches.tokens.8000.164.tar  parallel.batches.tokens.8000.351.tar\r\n",
      "parallel.batches.tokens.8000.165.tar  parallel.batches.tokens.8000.352.tar\r\n",
      "parallel.batches.tokens.8000.166.tar  parallel.batches.tokens.8000.353.tar\r\n",
      "parallel.batches.tokens.8000.167.tar  parallel.batches.tokens.8000.354.tar\r\n",
      "parallel.batches.tokens.8000.168.tar  parallel.batches.tokens.8000.355.tar\r\n",
      "parallel.batches.tokens.8000.169.tar  parallel.batches.tokens.8000.356.tar\r\n",
      "parallel.batches.tokens.8000.16.tar   parallel.batches.tokens.8000.357.tar\r\n",
      "parallel.batches.tokens.8000.170.tar  parallel.batches.tokens.8000.358.tar\r\n",
      "parallel.batches.tokens.8000.171.tar  parallel.batches.tokens.8000.359.tar\r\n",
      "parallel.batches.tokens.8000.172.tar  parallel.batches.tokens.8000.35.tar\r\n",
      "parallel.batches.tokens.8000.173.tar  parallel.batches.tokens.8000.360.tar\r\n",
      "parallel.batches.tokens.8000.174.tar  parallel.batches.tokens.8000.361.tar\r\n",
      "parallel.batches.tokens.8000.175.tar  parallel.batches.tokens.8000.362.tar\r\n",
      "parallel.batches.tokens.8000.176.tar  parallel.batches.tokens.8000.363.tar\r\n",
      "parallel.batches.tokens.8000.177.tar  parallel.batches.tokens.8000.364.tar\r\n",
      "parallel.batches.tokens.8000.178.tar  parallel.batches.tokens.8000.365.tar\r\n",
      "parallel.batches.tokens.8000.179.tar  parallel.batches.tokens.8000.366.tar\r\n",
      "parallel.batches.tokens.8000.17.tar   parallel.batches.tokens.8000.367.tar\r\n",
      "parallel.batches.tokens.8000.180.tar  parallel.batches.tokens.8000.368.tar\r\n",
      "parallel.batches.tokens.8000.181.tar  parallel.batches.tokens.8000.369.tar\r\n",
      "parallel.batches.tokens.8000.182.tar  parallel.batches.tokens.8000.36.tar\r\n",
      "parallel.batches.tokens.8000.183.tar  parallel.batches.tokens.8000.370.tar\r\n",
      "parallel.batches.tokens.8000.184.tar  parallel.batches.tokens.8000.371.tar\r\n",
      "parallel.batches.tokens.8000.185.tar  parallel.batches.tokens.8000.372.tar\r\n",
      "parallel.batches.tokens.8000.186.tar  parallel.batches.tokens.8000.373.tar\r\n",
      "parallel.batches.tokens.8000.187.tar  parallel.batches.tokens.8000.374.tar\r\n",
      "parallel.batches.tokens.8000.188.tar  parallel.batches.tokens.8000.375.tar\r\n",
      "parallel.batches.tokens.8000.189.tar  parallel.batches.tokens.8000.376.tar\r\n",
      "parallel.batches.tokens.8000.18.tar   parallel.batches.tokens.8000.377.tar\r\n",
      "parallel.batches.tokens.8000.190.tar  parallel.batches.tokens.8000.378.tar\r\n",
      "parallel.batches.tokens.8000.191.tar  parallel.batches.tokens.8000.379.tar\r\n",
      "parallel.batches.tokens.8000.192.tar  parallel.batches.tokens.8000.37.tar\r\n",
      "parallel.batches.tokens.8000.193.tar  parallel.batches.tokens.8000.380.tar\r\n",
      "parallel.batches.tokens.8000.194.tar  parallel.batches.tokens.8000.381.tar\r\n",
      "parallel.batches.tokens.8000.195.tar  parallel.batches.tokens.8000.382.tar\r\n",
      "parallel.batches.tokens.8000.196.tar  parallel.batches.tokens.8000.383.tar\r\n",
      "parallel.batches.tokens.8000.197.tar  parallel.batches.tokens.8000.384.tar\r\n",
      "parallel.batches.tokens.8000.198.tar  parallel.batches.tokens.8000.385.tar\r\n",
      "parallel.batches.tokens.8000.199.tar  parallel.batches.tokens.8000.386.tar\r\n",
      "parallel.batches.tokens.8000.19.tar   parallel.batches.tokens.8000.387.tar\r\n",
      "parallel.batches.tokens.8000.1.tar    parallel.batches.tokens.8000.388.tar\r\n",
      "parallel.batches.tokens.8000.200.tar  parallel.batches.tokens.8000.389.tar\r\n",
      "parallel.batches.tokens.8000.201.tar  parallel.batches.tokens.8000.38.tar\r\n",
      "parallel.batches.tokens.8000.202.tar  parallel.batches.tokens.8000.390.tar\r\n",
      "parallel.batches.tokens.8000.203.tar  parallel.batches.tokens.8000.391.tar\r\n",
      "parallel.batches.tokens.8000.204.tar  parallel.batches.tokens.8000.392.tar\r\n",
      "parallel.batches.tokens.8000.205.tar  parallel.batches.tokens.8000.393.tar\r\n",
      "parallel.batches.tokens.8000.206.tar  parallel.batches.tokens.8000.394.tar\r\n",
      "parallel.batches.tokens.8000.207.tar  parallel.batches.tokens.8000.395.tar\r\n",
      "parallel.batches.tokens.8000.208.tar  parallel.batches.tokens.8000.396.tar\r\n",
      "parallel.batches.tokens.8000.209.tar  parallel.batches.tokens.8000.397.tar\r\n",
      "parallel.batches.tokens.8000.20.tar   parallel.batches.tokens.8000.398.tar\r\n",
      "parallel.batches.tokens.8000.210.tar  parallel.batches.tokens.8000.399.tar\r\n",
      "parallel.batches.tokens.8000.211.tar  parallel.batches.tokens.8000.39.tar\r\n",
      "parallel.batches.tokens.8000.212.tar  parallel.batches.tokens.8000.3.tar\r\n",
      "parallel.batches.tokens.8000.213.tar  parallel.batches.tokens.8000.400.tar\r\n",
      "parallel.batches.tokens.8000.214.tar  parallel.batches.tokens.8000.401.tar\r\n",
      "parallel.batches.tokens.8000.215.tar  parallel.batches.tokens.8000.402.tar\r\n",
      "parallel.batches.tokens.8000.216.tar  parallel.batches.tokens.8000.403.tar\r\n",
      "parallel.batches.tokens.8000.217.tar  parallel.batches.tokens.8000.404.tar\r\n",
      "parallel.batches.tokens.8000.218.tar  parallel.batches.tokens.8000.405.tar\r\n",
      "parallel.batches.tokens.8000.219.tar  parallel.batches.tokens.8000.406.tar\r\n",
      "parallel.batches.tokens.8000.21.tar   parallel.batches.tokens.8000.407.tar\r\n",
      "parallel.batches.tokens.8000.220.tar  parallel.batches.tokens.8000.408.tar\r\n",
      "parallel.batches.tokens.8000.221.tar  parallel.batches.tokens.8000.409.tar\r\n",
      "parallel.batches.tokens.8000.222.tar  parallel.batches.tokens.8000.40.tar\r\n",
      "parallel.batches.tokens.8000.223.tar  parallel.batches.tokens.8000.410.tar\r\n",
      "parallel.batches.tokens.8000.224.tar  parallel.batches.tokens.8000.411.tar\r\n",
      "parallel.batches.tokens.8000.225.tar  parallel.batches.tokens.8000.412.tar\r\n",
      "parallel.batches.tokens.8000.226.tar  parallel.batches.tokens.8000.41.tar\r\n",
      "parallel.batches.tokens.8000.227.tar  parallel.batches.tokens.8000.42.tar\r\n",
      "parallel.batches.tokens.8000.228.tar  parallel.batches.tokens.8000.43.tar\r\n",
      "parallel.batches.tokens.8000.229.tar  parallel.batches.tokens.8000.44.tar\r\n",
      "parallel.batches.tokens.8000.22.tar   parallel.batches.tokens.8000.45.tar\r\n",
      "parallel.batches.tokens.8000.230.tar  parallel.batches.tokens.8000.46.tar\r\n",
      "parallel.batches.tokens.8000.231.tar  parallel.batches.tokens.8000.47.tar\r\n",
      "parallel.batches.tokens.8000.232.tar  parallel.batches.tokens.8000.48.tar\r\n",
      "parallel.batches.tokens.8000.233.tar  parallel.batches.tokens.8000.49.tar\r\n",
      "parallel.batches.tokens.8000.234.tar  parallel.batches.tokens.8000.4.tar\r\n",
      "parallel.batches.tokens.8000.235.tar  parallel.batches.tokens.8000.50.tar\r\n",
      "parallel.batches.tokens.8000.236.tar  parallel.batches.tokens.8000.51.tar\r\n",
      "parallel.batches.tokens.8000.237.tar  parallel.batches.tokens.8000.52.tar\r\n",
      "parallel.batches.tokens.8000.238.tar  parallel.batches.tokens.8000.53.tar\r\n",
      "parallel.batches.tokens.8000.239.tar  parallel.batches.tokens.8000.54.tar\r\n",
      "parallel.batches.tokens.8000.23.tar   parallel.batches.tokens.8000.55.tar\r\n",
      "parallel.batches.tokens.8000.240.tar  parallel.batches.tokens.8000.56.tar\r\n",
      "parallel.batches.tokens.8000.241.tar  parallel.batches.tokens.8000.57.tar\r\n",
      "parallel.batches.tokens.8000.242.tar  parallel.batches.tokens.8000.58.tar\r\n",
      "parallel.batches.tokens.8000.243.tar  parallel.batches.tokens.8000.59.tar\r\n",
      "parallel.batches.tokens.8000.244.tar  parallel.batches.tokens.8000.5.tar\r\n",
      "parallel.batches.tokens.8000.245.tar  parallel.batches.tokens.8000.60.tar\r\n",
      "parallel.batches.tokens.8000.246.tar  parallel.batches.tokens.8000.61.tar\r\n",
      "parallel.batches.tokens.8000.247.tar  parallel.batches.tokens.8000.62.tar\r\n",
      "parallel.batches.tokens.8000.248.tar  parallel.batches.tokens.8000.63.tar\r\n",
      "parallel.batches.tokens.8000.249.tar  parallel.batches.tokens.8000.64.tar\r\n",
      "parallel.batches.tokens.8000.24.tar   parallel.batches.tokens.8000.65.tar\r\n",
      "parallel.batches.tokens.8000.250.tar  parallel.batches.tokens.8000.66.tar\r\n",
      "parallel.batches.tokens.8000.251.tar  parallel.batches.tokens.8000.67.tar\r\n",
      "parallel.batches.tokens.8000.252.tar  parallel.batches.tokens.8000.68.tar\r\n",
      "parallel.batches.tokens.8000.253.tar  parallel.batches.tokens.8000.69.tar\r\n",
      "parallel.batches.tokens.8000.254.tar  parallel.batches.tokens.8000.6.tar\r\n",
      "parallel.batches.tokens.8000.255.tar  parallel.batches.tokens.8000.70.tar\r\n",
      "parallel.batches.tokens.8000.256.tar  parallel.batches.tokens.8000.71.tar\r\n",
      "parallel.batches.tokens.8000.257.tar  parallel.batches.tokens.8000.72.tar\r\n",
      "parallel.batches.tokens.8000.258.tar  parallel.batches.tokens.8000.73.tar\r\n",
      "parallel.batches.tokens.8000.259.tar  parallel.batches.tokens.8000.74.tar\r\n",
      "parallel.batches.tokens.8000.25.tar   parallel.batches.tokens.8000.75.tar\r\n",
      "parallel.batches.tokens.8000.260.tar  parallel.batches.tokens.8000.76.tar\r\n",
      "parallel.batches.tokens.8000.261.tar  parallel.batches.tokens.8000.77.tar\r\n",
      "parallel.batches.tokens.8000.262.tar  parallel.batches.tokens.8000.78.tar\r\n",
      "parallel.batches.tokens.8000.263.tar  parallel.batches.tokens.8000.79.tar\r\n",
      "parallel.batches.tokens.8000.264.tar  parallel.batches.tokens.8000.7.tar\r\n",
      "parallel.batches.tokens.8000.265.tar  parallel.batches.tokens.8000.80.tar\r\n",
      "parallel.batches.tokens.8000.266.tar  parallel.batches.tokens.8000.81.tar\r\n",
      "parallel.batches.tokens.8000.267.tar  parallel.batches.tokens.8000.82.tar\r\n",
      "parallel.batches.tokens.8000.268.tar  parallel.batches.tokens.8000.83.tar\r\n",
      "parallel.batches.tokens.8000.269.tar  parallel.batches.tokens.8000.84.tar\r\n",
      "parallel.batches.tokens.8000.26.tar   parallel.batches.tokens.8000.85.tar\r\n",
      "parallel.batches.tokens.8000.270.tar  parallel.batches.tokens.8000.86.tar\r\n",
      "parallel.batches.tokens.8000.271.tar  parallel.batches.tokens.8000.87.tar\r\n",
      "parallel.batches.tokens.8000.272.tar  parallel.batches.tokens.8000.88.tar\r\n",
      "parallel.batches.tokens.8000.273.tar  parallel.batches.tokens.8000.89.tar\r\n",
      "parallel.batches.tokens.8000.274.tar  parallel.batches.tokens.8000.8.tar\r\n",
      "parallel.batches.tokens.8000.275.tar  parallel.batches.tokens.8000.90.tar\r\n",
      "parallel.batches.tokens.8000.276.tar  parallel.batches.tokens.8000.91.tar\r\n",
      "parallel.batches.tokens.8000.277.tar  parallel.batches.tokens.8000.92.tar\r\n",
      "parallel.batches.tokens.8000.278.tar  parallel.batches.tokens.8000.93.tar\r\n",
      "parallel.batches.tokens.8000.279.tar  parallel.batches.tokens.8000.94.tar\r\n",
      "parallel.batches.tokens.8000.27.tar   parallel.batches.tokens.8000.95.tar\r\n",
      "parallel.batches.tokens.8000.280.tar  parallel.batches.tokens.8000.96.tar\r\n",
      "parallel.batches.tokens.8000.281.tar  parallel.batches.tokens.8000.97.tar\r\n",
      "parallel.batches.tokens.8000.282.tar  parallel.batches.tokens.8000.98.tar\r\n",
      "parallel.batches.tokens.8000.283.tar  parallel.batches.tokens.8000.99.tar\r\n",
      "parallel.batches.tokens.8000.284.tar  parallel.batches.tokens.8000.9.tar\r\n",
      "parallel.batches.tokens.8000.285.tar  tokenizer.decoder.32000.BPE.model\r\n",
      "parallel.batches.tokens.8000.286.tar  tokenizer.encoder.32000.BPE.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/tarred_dataset_en_ru_8k_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc5e123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"num_batches\": 8260, \"tar_files\": [\"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.6.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.258.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.318.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.203.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.39.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.50.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.381.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.277.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.240.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.243.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.41.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.152.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.190.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.332.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.382.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.262.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.232.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.47.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.295.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.253.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.156.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.328.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.252.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.19.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.65.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.313.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.51.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.406.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.4.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.412.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.98.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.177.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.289.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.324.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.375.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.378.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.274.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.89.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.31.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.273.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.155.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.163.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.331.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.62.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.214.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.197.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.183.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.187.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.20.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.222.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.356.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.400.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.81.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.97.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.329.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.212.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.131.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.21.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.399.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.59.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.339.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.355.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.143.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.40.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.407.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.334.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.260.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.393.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.365.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.319.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.320.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.259.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.267.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.263.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.100.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.130.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.335.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.294.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.302.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.36.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.327.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.37.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.411.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.394.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.49.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.373.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.272.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.223.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.286.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.323.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.403.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.145.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.191.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.88.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.138.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.401.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.314.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.250.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.213.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.108.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.157.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.239.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.85.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.140.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.236.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.211.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.1.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.338.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.242.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.268.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.15.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.110.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.171.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.204.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.386.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.372.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.384.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.175.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.402.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.189.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.300.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.228.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.363.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.169.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.28.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.209.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.134.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.111.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.220.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.91.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.359.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.306.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.226.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.34.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.336.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.193.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.325.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.124.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.311.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.316.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.139.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.257.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.84.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.397.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.105.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.224.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.174.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.79.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.73.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.126.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.161.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.142.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.135.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.215.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.2.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.284.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.281.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.10.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.58.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.151.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.404.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.221.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.317.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.230.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.309.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.122.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.22.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.99.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.116.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.132.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.276.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.225.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.194.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.117.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.127.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.376.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.389.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.129.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.377.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.374.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.35.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.234.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.205.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.176.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.198.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.246.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.354.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.61.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.32.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.353.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.0.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.60.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.241.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.9.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.245.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.86.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.255.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.285.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.128.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.72.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.201.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.369.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.366.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.170.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.350.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.43.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.269.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.261.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.326.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.266.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.115.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.64.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.202.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.340.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.184.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.206.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.154.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.395.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.38.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.44.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.248.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.383.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.146.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.144.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.109.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.63.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.342.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.219.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.264.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.112.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.391.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.24.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.364.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.133.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.11.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.247.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.74.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.405.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.147.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.291.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.346.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.114.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.235.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.388.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.56.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.305.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.165.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.125.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.218.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.119.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.296.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.80.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.231.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.330.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.315.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.361.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.25.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.308.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.390.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.208.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.103.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.298.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.195.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.181.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.368.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.17.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.136.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.304.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.27.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.158.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.303.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.164.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.90.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.94.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.270.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.322.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.69.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.160.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.371.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.48.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.7.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.168.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.271.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.188.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.102.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.106.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.182.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.301.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.141.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.237.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.57.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.283.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.278.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.18.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.410.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.70.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.199.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.13.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.210.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.348.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.87.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.398.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.396.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.345.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.279.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.337.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.113.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.227.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.29.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.192.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.282.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.360.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.92.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.82.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.293.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.196.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.149.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.380.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.150.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.3.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.95.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.409.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.216.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.23.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.385.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.290.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.287.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.200.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.185.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.297.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.107.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.101.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.137.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.180.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.53.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.167.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.76.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.5.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.54.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.8.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.153.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.321.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.16.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.55.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.358.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.96.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.14.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.249.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.26.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.310.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.83.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.392.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.349.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.120.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.254.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.207.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.275.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.162.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.148.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.238.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.172.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.362.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.66.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.173.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.104.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.45.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.312.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.408.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.347.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.179.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.251.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.30.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.288.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.387.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.71.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.166.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.186.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.118.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.52.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.77.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.256.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.33.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.42.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.217.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.299.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.123.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.280.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.357.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.68.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.233.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.343.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.370.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.333.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.379.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.265.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.75.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.46.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.12.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.244.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.344.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.159.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.367.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.352.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.341.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.67.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.178.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.121.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.229.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.93.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.78.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.351.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.292.tar\", \"data/tarred_dataset_en_ru_8k_tokens/parallel.batches.tokens.8000.307.tar\"]}"
     ]
    }
   ],
   "source": [
    "!cat data/tarred_dataset_en_ru_8k_tokens/metadata.tokens.8000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b004170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
