{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d3d4d3",
   "metadata": {},
   "source": [
    "# Speaker Clustering: Torch Scripted Module "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a331b6",
   "metadata": {},
   "source": [
    "Provide the NeMo path to `NEMO_BRANCH_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d90716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check NeMo PATH: ['/home/taejinp/projects/diar_torch/NeMo/nemo']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# NEMO_BRANCH_PATH = '/your/path/to/diar_torch/NeMo/'\n",
    "NEMO_BRANCH_PATH = '/home/taejinp/projects/diar_torch/NeMo/'\n",
    "sys.path.insert(0, NEMO_BRANCH_PATH)\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b45939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc61cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-25 13:51:57 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-07-25 13:51:58 nemo_logging:349] /home/taejinp/anaconda3/envs/t01/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.parts.utils.nmesc_clustering_export import SpeakerClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd924952",
   "metadata": {},
   "source": [
    "Download an example input dictionary file `uniq_embs_and_timestamps`:\n",
    "https://drive.google.com/file/d/1249CTH6FgFbioBY1KkPOuA996LfbgqP_/view?usp=sharing\n",
    "\n",
    "Please save it to your local path such as:   \n",
    "`example_file_path = \"/home/taejinp/Downloads/uniq_embs_and_timestamps.pt\"`\n",
    "\n",
    "This file has been created using scale lengths of :   \n",
    "`[1.5, 1.25, 1.0, 0.75, 0.5]`  \n",
    "and shift length of :  \n",
    "`[0.75, 0.75, 0.5, 0.375, 0.25]`  \n",
    "Scale indexes are:   \n",
    "`[0, 1, 2, 3, 4]`  \n",
    "\n",
    "Base scale is the finest (shortest) scale which is also the unit of decision.   \n",
    "In this example, base scale index is `4`.  \n",
    "In this example, base scale has length of 0.5 second and shift length (hop length) of 0.25 second.  \n",
    "\n",
    "`multiscale_segment_counts` variable is needed for splitting into multiscale tensors.  \n",
    "`embeddings_in_scales` contains concatenated embeddings of 5 scales.   \n",
    "`timestamps_in_scales` contains concatenated timestamps of 5 scales.   \n",
    "\n",
    "jit scripted module `speaker_clustering` splits the input tensors into `scale_n` number of tensors before run clustering. \n",
    "Each of scale contains torch.tensors with different sizes. Check out the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481e1c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment counts of 5 scales\n",
      "tensor([ 553,  609,  788, 1045, 1577])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([192])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([192])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "example_file_path = \"/home/taejinp/Downloads/uniq_embs_and_timestamps.pt\"\n",
    "uniq_embs_and_timestamps = torch.load(example_file_path)\n",
    "\n",
    "# Python Dictionary indexed (keys) by integer, values are torch.Tensor\n",
    "multiscale_segment_counts = uniq_embs_and_timestamps['multiscale_segment_counts']\n",
    "embeddings_in_scales = uniq_embs_and_timestamps['embeddings']\n",
    "timestamps_in_scales = uniq_embs_and_timestamps['time_stamps']\n",
    "\n",
    "# Multiscale segment counts for each scale\n",
    "# Dimension: (Number of index-2 (3rd) scale segmensts) x 2 (start and end time stamps)\n",
    "print(f\"Segment counts of {len(multiscale_segment_counts)} scales\")\n",
    "print(multiscale_segment_counts)\n",
    "\n",
    "# Dimension: (Number of index-4 (5th) scale segmensts) x (embedding dimension, 192 in this case)\n",
    "print(type(embeddings_in_scales[4]))\n",
    "print(embeddings_in_scales[4].shape)\n",
    "\n",
    "# Dimension: (Number of index-4 (5th) scale segmensts) x 2 (start and end time stamps)\n",
    "print(type(timestamps_in_scales[4]))\n",
    "print(timestamps_in_scales[4].shape)\n",
    "\n",
    "\n",
    "# Dimension: (Number of index-2 (3rd) scale segmensts) x (embedding dimension, 192 in this case)\n",
    "print(type(embeddings_in_scales[2]))\n",
    "print(embeddings_in_scales[2].shape)\n",
    "\n",
    "# Dimension: (Number of index-2 (3rd) scale segmensts) x 2 (start and end time stamps)\n",
    "print(type(timestamps_in_scales[2]))\n",
    "print(timestamps_in_scales[2].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b16060f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup a multiscale weight vector. Equal weights\n",
    "device = torch.device(\"cuda\")\n",
    "multiscale_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0]).unsqueeze(0).to(device)\n",
    "multiscale_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13be2fd",
   "metadata": {},
   "source": [
    "Now, create a `SpeakerClustering` class instance and convert it to torch.jit.script module. This will create recursive script module since all the sub-fucntions used in this class is all torch.jit.script-decorated. \n",
    "\n",
    "First module is non-parallelized instance and the second one is parallelized. `parallelism=False` or `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3a168e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_search_volume=30\n",
    "max_num_speaker=8\n",
    "max_rp_threshold=0.15\n",
    "\n",
    "# Single thread, WITHOUT parallelism for searching the p-value parameter.\n",
    "speaker_clustering_singthrd = SpeakerClustering(\n",
    "            max_num_speaker=max_num_speaker,\n",
    "            max_rp_threshold=max_rp_threshold,\n",
    "            sparse_search_volume=sparse_search_volume,\n",
    "            multiscale_weights=multiscale_weights,\n",
    "            parallelism=False,\n",
    "            cuda=True)\n",
    "scripted_singthrd = torch.jit.script(speaker_clustering_singthrd).to(device)\n",
    "\n",
    "# Multi thread, WITH parallelism for searching the p-value parameter.\n",
    "speaker_clustering_multhrd = SpeakerClustering(\n",
    "            max_num_speaker=max_num_speaker,\n",
    "            max_rp_threshold=max_rp_threshold,\n",
    "            sparse_search_volume=sparse_search_volume,\n",
    "            multiscale_weights=multiscale_weights,\n",
    "            parallelism=True,\n",
    "            cuda=True)\n",
    "scripted_multhrd = torch.jit.script(speaker_clustering_multhrd).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda11e5c",
   "metadata": {},
   "source": [
    "Now, run the speaker clustering model with the following line. \n",
    "\n",
    "- Input is all `torch.tensor` type. It will be split into each scale in the scripted module.\n",
    "- You can check the clustered labels and estimated number of speakers. The output is also torch.tensor format.\n",
    "\n",
    "Check out the speed gain from parallelism=True. The bigger the size of `sparse_search_volume`, the more the speed gain is. For example `sparse_search_volume=30`, it has approximately 30% speed gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fde3c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Thread ETA: 3.255 sec\n",
      "cluster labels: tensor([0, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "Set of speakers {0, 1}\n",
      "\n",
      "Multi Thread ETA: 1.953 sec\n",
      "cluster labels: tensor([0, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "Set of speakers {0, 1}\n"
     ]
    }
   ],
   "source": [
    "start1 = time.time()\n",
    "cluster_labels = scripted_singthrd.forward(\n",
    "    embeddings_in_scales,\n",
    "    timestamps_in_scales,\n",
    "    multiscale_segment_counts,\n",
    "    oracle_num_speakers=-1,\n",
    "    )\n",
    "print(f\"\\nSingle Thread ETA: {(time.time()-start1):.3f} sec\")\n",
    "print(\"cluster labels:\", cluster_labels)\n",
    "print(\"Set of speakers\", set(cluster_labels.cpu().numpy().tolist()))\n",
    "\n",
    "start2 = time.time()\n",
    "cluster_labels = scripted_multhrd.forward(\n",
    "    embeddings_in_scales,\n",
    "    timestamps_in_scales,\n",
    "    multiscale_segment_counts,\n",
    "    oracle_num_speakers=-1,\n",
    "    )\n",
    "print(f\"\\nMulti Thread ETA: {(time.time()-start2):.3f} sec\")\n",
    "\n",
    "print(\"cluster labels:\", cluster_labels)\n",
    "print(\"Set of speakers\", set(cluster_labels.cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b1c1e",
   "metadata": {},
   "source": [
    "Save torch.jit.script module with `torch.jit.save`. Check out [this page](https://pytorch.org/docs/stable/generated/torch.jit.save.html) where `torch.jit.save` is explained as below.\n",
    "\n",
    "> Save an offline version of this module for use in a separate process. The saved module serializes all of the methods, submodules, parameters, and attributes of this module. It can be loaded into the C++ API using torch::jit::load(filename) or into the Python API with torch.jit.load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4e2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(scripted_multhrd, 'speaker_clustering_multithread.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26cccad",
   "metadata": {},
   "source": [
    "# Speaker Clustering: Speed Test for Torch Scripted Module "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d545667",
   "metadata": {},
   "source": [
    "`addAnchorEmb()` Is a function that creates dummy speaker embedding. Let's create dummy embeddings to measure the speed of speaker clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c41a07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings:\n",
      " torch.Size([8192, 192])\n",
      "embeddings_in_scales_gen.shape \n",
      " torch.Size([8192, 192])\n",
      "tensor([[-0.0975, -0.6189,  0.0506,  ...,  0.2752, -0.0342, -1.0346],\n",
      "        [-0.1126, -0.6138,  0.0459,  ...,  0.2954,  0.0058, -1.1020],\n",
      "        [-0.0951, -0.6133,  0.0118,  ...,  0.3197, -0.0104, -1.0503],\n",
      "        ...,\n",
      "        [ 0.5204, -0.1872, -0.3354,  ..., -0.5635, -0.0084,  1.6196],\n",
      "        [ 0.5335, -0.2014, -0.3348,  ..., -0.5357,  0.0120,  1.6273],\n",
      "        [ 0.4883, -0.1984, -0.3263,  ..., -0.6130,  0.0127,  1.6214]])\n",
      "tensor([[0.0000e+00, 1.0000e+00],\n",
      "        [5.0000e-01, 1.5000e+00],\n",
      "        [1.0000e+00, 2.0000e+00],\n",
      "        ...,\n",
      "        [4.0945e+03, 4.0955e+03],\n",
      "        [4.0950e+03, 4.0960e+03],\n",
      "        [4.0955e+03, 4.0965e+03]])\n",
      "timestamps_in_scales_gen.shape \n",
      " torch.Size([8192, 2])\n",
      "tensor([8192])\n"
     ]
    }
   ],
   "source": [
    "def addAnchorEmb(emb: torch.Tensor, anchor_sample_n: int, anchor_spk_n: int, sigma: float):\n",
    "    emb_dim = 192\n",
    "    std_org = torch.std(emb, dim=0)\n",
    "    new_emb_list = []\n",
    "    for _ in range(anchor_spk_n):\n",
    "        emb_m = torch.tile(torch.randn(1, emb_dim), (anchor_sample_n, 1))\n",
    "        emb_noise = torch.randn(anchor_sample_n, emb_dim).T\n",
    "        emb_noise = torch.matmul(\n",
    "            torch.diag(std_org), emb_noise / torch.max(torch.abs(emb_noise), dim=0)[0].unsqueeze(0)\n",
    "        ).T\n",
    "        emb_gen = emb_m + sigma * emb_noise\n",
    "        new_emb_list.append(emb_gen)\n",
    "\n",
    "#     new_emb_list.append(emb)\n",
    "    new_emb_np = torch.vstack(new_emb_list)\n",
    "    return new_emb_np\n",
    "\n",
    "scale_n = 1\n",
    "mat_size = 2**13\n",
    "\n",
    "anchor_spk_n = 2\n",
    "\n",
    "embeddings = addAnchorEmb(embeddings_in_scales, anchor_sample_n=int(mat_size/anchor_spk_n), anchor_spk_n=anchor_spk_n, sigma=1)\n",
    "print(\"embeddings:\\n\", embeddings.shape)\n",
    "embeddings_in_scales_gen = embeddings.tile((scale_n, 1))\n",
    "print(\"embeddings_in_scales_gen.shape \\n\", embeddings_in_scales_gen.shape)\n",
    "\n",
    "timestamps = torch.tensor([[float(stt/2), float(stt/2+1)] for stt in range(mat_size)])\n",
    "timestamps_in_scales_gen = timestamps.tile((scale_n, 1))\n",
    "print(embeddings_in_scales_gen)\n",
    "print(timestamps_in_scales_gen)\n",
    "print(\"timestamps_in_scales_gen.shape \\n\", timestamps_in_scales_gen.shape)\n",
    "multiscale_segment_counts_gen = torch.tensor([mat_size for x in range(scale_n)])\n",
    "print(multiscale_segment_counts_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78162cc8",
   "metadata": {},
   "source": [
    "Measure the speed of speaker clustering jit scripted module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ba5e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running segment volume with: \n",
      " tensor([16])\n",
      "\n",
      "Single Thread ETA: 0.375 sec\n",
      "\n",
      "Multi Thread ETA: 0.035 sec\n",
      "\n",
      "\n",
      "\n",
      "Running segment volume with: \n",
      " tensor([32])\n",
      "\n",
      "Single Thread ETA: 0.142 sec\n",
      "\n",
      "Multi Thread ETA: 0.123 sec\n",
      "\n",
      "\n",
      "\n",
      "Running segment volume with: \n",
      " tensor([64])\n",
      "\n",
      "Single Thread ETA: 0.327 sec\n",
      "\n",
      "Multi Thread ETA: 0.192 sec\n",
      "\n",
      "\n",
      "\n",
      "Running segment volume with: \n",
      " tensor([128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m start_gen1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat_n):\n\u001b[0;32m---> 50\u001b[0m     cluster_labels \u001b[38;5;241m=\u001b[39m \u001b[43mscripted_singthrd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings_in_scales_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestamps_in_scales_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultiscale_segment_counts_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43moracle_num_speakers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m eta1 \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_gen1)\u001b[38;5;241m/\u001b[39mrepeat_n\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSingle Thread ETA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meta1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup a multiscale weight vector. Equal weights\n",
    "device = torch.device(\"cuda\")\n",
    "multiscale_weights = torch.tensor([1.0 for _ in range(scale_n)]).unsqueeze(0).to(device)\n",
    "\n",
    "sparse_search_volume=10\n",
    "max_num_speaker=8\n",
    "max_rp_threshold=0.25\n",
    "scale_n = 1\n",
    "anchor_spk_n = 2\n",
    "repeat_n = 10\n",
    "eta_list_single = []\n",
    "eta_list_mutli = []\n",
    "mat_size_list = []\n",
    "size_list=[4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "for pos in size_list:\n",
    "    mat_size = int(2 ** pos)\n",
    "    mat_size_list.append(mat_size)\n",
    "    embeddings = addAnchorEmb(embeddings_in_scales, anchor_sample_n=int(mat_size/anchor_spk_n), anchor_spk_n=anchor_spk_n, sigma=1)\n",
    "    embeddings_in_scales_gen = embeddings.tile((scale_n, 1))\n",
    "    timestamps = torch.tensor([[float(stt/2), float(stt/2+1)] for stt in range(mat_size)])\n",
    "    timestamps_in_scales_gen = timestamps.tile((scale_n, 1))\n",
    "    multiscale_segment_counts_gen = torch.tensor([mat_size for x in range(scale_n)])\n",
    "\n",
    "\n",
    "    # Single thread, WITHOUT parallelism for searching the p-value parameter.\n",
    "    speaker_clustering_singthrd = SpeakerClustering(\n",
    "                max_num_speaker=max_num_speaker,\n",
    "                max_rp_threshold=max_rp_threshold,\n",
    "                sparse_search_volume=sparse_search_volume,\n",
    "                multiscale_weights=multiscale_weights,\n",
    "                parallelism=False,\n",
    "                cuda=True)\n",
    "    scripted_singthrd = torch.jit.script(speaker_clustering_singthrd).to(device)\n",
    "\n",
    "    print(\"Running segment volume with: \\n\", multiscale_segment_counts_gen)\n",
    "    # Multi thread, WITH parallelism for searching the p-value parameter.\n",
    "    speaker_clustering_multhrd = SpeakerClustering(\n",
    "                max_num_speaker=max_num_speaker,\n",
    "                max_rp_threshold=max_rp_threshold,\n",
    "                sparse_search_volume=sparse_search_volume,\n",
    "                multiscale_weights=multiscale_weights,\n",
    "                parallelism=True,\n",
    "                cuda=True)\n",
    "    scripted_multhrd = torch.jit.script(speaker_clustering_multhrd).to(device)\n",
    "\n",
    "\n",
    "    start_gen1 = time.time()\n",
    "    for _ in range(repeat_n):\n",
    "        cluster_labels = scripted_singthrd.forward(\n",
    "            embeddings_in_scales_gen,\n",
    "            timestamps_in_scales_gen,\n",
    "            multiscale_segment_counts_gen,\n",
    "            oracle_num_speakers=-1,\n",
    "            )\n",
    "    eta1 = (time.time()-start_gen1)/repeat_n\n",
    "    print(f\"\\nSingle Thread ETA: {eta1:.3f} sec\")\n",
    "    eta_list_single.append(eta1)\n",
    "    \n",
    "    start_gen2 = time.time()\n",
    "    for _ in range(repeat_n):\n",
    "        cluster_labels = scripted_multhrd.forward(\n",
    "            embeddings_in_scales_gen,\n",
    "            timestamps_in_scales_gen,\n",
    "            multiscale_segment_counts_gen,\n",
    "            oracle_num_speakers=-1,\n",
    "            )\n",
    "    eta2 = (time.time()-start_gen2)/repeat_n\n",
    "    print(f\"\\nMulti Thread ETA: {eta2:.3f} sec\")\n",
    "    eta_list_mutli.append(eta2)\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\"eta_list_single\\n\")\n",
    "print(eta_list_single)\n",
    "print(\"eta_list_multi\\n\")\n",
    "print(eta_list_mutli)\n",
    "print(\"mat_size_list\\n\")\n",
    "print(mat_size_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9980663",
   "metadata": {},
   "source": [
    "Save the result in csv file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b8ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "   \n",
    "# field names \n",
    "fields = ['Matrix size', 'Single Thread', 'Multi Thread']\n",
    "    \n",
    "# data rows of csv file \n",
    "mat_size_list.insert(0,fields[0])\n",
    "eta_list_single.insert(0, fields[1])\n",
    "eta_list_mutli.insert(0, fields[2])\n",
    "\n",
    "rows = [mat_size_list,\n",
    "        eta_list_single,\n",
    "        eta_list_mutli]\n",
    "print(rows)\n",
    "print(mat_size_list, eta_list_single, eta_list_mutli)\n",
    "path = f\"/home/taejinp/gdrive/result_data/clustering_speed_searchvol{sparse_search_volume}.csv\"\n",
    "\n",
    "with open(path, 'w') as f:\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935d151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce91cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
