{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fdd695",
   "metadata": {},
   "source": [
    "# Speaker Clustering: Torch Scripted Module Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a331b6",
   "metadata": {},
   "source": [
    "Provide the NeMo path to `NEMO_BRANCH_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d90716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check NeMo PATH: ['/home/taejinp/projects/diar_torch/NeMo/nemo']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# NEMO_BRANCH_PATH = '/your/path/to/diar_torch/NeMo/'\n",
    "NEMO_BRANCH_PATH = '/home/taejinp/projects/diar_torch/NeMo/'\n",
    "sys.path.insert(0, NEMO_BRANCH_PATH)\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b45939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc61cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-14 18:04:49 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-07-14 18:04:50 nemo_logging:349] /home/taejinp/anaconda3/envs/t01/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.parts.utils.nmesc_clustering_export import SpeakerClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd924952",
   "metadata": {},
   "source": [
    "Download an example input dictionary file `uniq_scale_dict`:\n",
    "https://drive.google.com/file/d/1gQ7pqKnHk4v9zt52ECkJBeQL-aN38pEI/view?usp=sharing\n",
    "\n",
    "Please save it to your local path such as:   \n",
    "`example_file_path = \"/home/taejinp/Downloads/uniq_scale_dict.pt\"`\n",
    "\n",
    "This file has been created using scale lengths of :   \n",
    "`[1.5, 1.25, 1.0, 0.75, 0.5]`  \n",
    "and shift length of :  \n",
    "`[0.75, 0.75, 0.5, 0.375, 0.25]`  \n",
    "\n",
    "Scale indexes are:   \n",
    "`[0, 1, 2, 3, 4]`  \n",
    "\n",
    "Base scale is the finest (shortest) scale which is also the unit of decision.   \n",
    "In this example, base scale index is `4`.  \n",
    "In this example, base scale has length of 0.5 second and shift length (hop length) of 0.25 second.  \n",
    "\n",
    "Each of scale contains torch.tensors with different sizes. Check out the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481e1c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniq_scale_dict keys:\n",
      " dict_keys([0, 1, 2, 3, 4])\n",
      "Base scale contents\n",
      " dict_keys(['embeddings', 'time_stamps'])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1577, 192])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1577, 2])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([788, 192])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([788, 2])\n"
     ]
    }
   ],
   "source": [
    "example_file_path = \"/home/taejinp/Downloads/uniq_scale_dict.pt\"\n",
    "uniq_scale_dict = torch.load(example_file_path)\n",
    "\n",
    "\n",
    "# It contains integer scale indexes\n",
    "print(\"uniq_scale_dict keys:\\n\", uniq_scale_dict.keys())\n",
    "\n",
    "# Each scale key contains (1) embeddings (2) time_stamps\n",
    "print(\"Base scale contents\\n\", uniq_scale_dict[4].keys())\n",
    "\n",
    "# Dimension: (Number of index-4 (5th) scale segmensts) x (embedding dimension, 192 in this case)\n",
    "print(type(uniq_scale_dict[4]['embeddings']))\n",
    "print(uniq_scale_dict[4]['embeddings'].shape)\n",
    "\n",
    "# Dimension: (Number of index-4 (5th) scale segmensts) x 2 (start and end time stamps)\n",
    "print(type(uniq_scale_dict[4]['time_stamps']))\n",
    "print(uniq_scale_dict[4]['time_stamps'].shape)\n",
    "\n",
    "\n",
    "# Dimension: (Number of index-2 (3rd) scale segmensts) x (embedding dimension, 192 in this case)\n",
    "print(type(uniq_scale_dict[2]['embeddings']))\n",
    "print(uniq_scale_dict[2]['embeddings'].shape)\n",
    "\n",
    "# Dimension: (Number of index-2 (3rd) scale segmensts) x 2 (start and end time stamps)\n",
    "print(type(uniq_scale_dict[2]['time_stamps']))\n",
    "print(uniq_scale_dict[2]['time_stamps'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b16060f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup a multiscale weight vector. Equal weights\n",
    "device = torch.device(\"cuda\")\n",
    "multiscale_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0]).unsqueeze(0).to(device)\n",
    "multiscale_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13be2fd",
   "metadata": {},
   "source": [
    "Now, create a `SpeakerClustering` class instance and convert it to torch.jit.script module. This will create recursive script module since all the sub-fucntions used in this class is all torch.jit.script-decorated. \n",
    "\n",
    "First module is non-parallelized instance and the second one is parallelized. `parallelism=False` or `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3a168e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "sparse_search_volume=30\n",
    "max_num_speaker=8\n",
    "max_rp_threshold=0.15\n",
    "\n",
    "# Single thread, WITHOUT parallelism for searching the p-value parameter.\n",
    "speaker_clustering_singthrd = SpeakerClustering(\n",
    "            max_num_speaker=max_num_speaker,\n",
    "            max_rp_threshold=max_rp_threshold,\n",
    "            sparse_search_volume=sparse_search_volume,\n",
    "            multiscale_weights=multiscale_weights,\n",
    "            parallelism=False,\n",
    "            cuda=True)\n",
    "scripted_singthrd = torch.jit.script(speaker_clustering_singthrd)\n",
    "\n",
    "# Multi thread, WITH parallelism for searching the p-value parameter.\n",
    "speaker_clustering_multhrd = SpeakerClustering(\n",
    "            max_num_speaker=max_num_speaker,\n",
    "            max_rp_threshold=max_rp_threshold,\n",
    "            sparse_search_volume=sparse_search_volume,\n",
    "            multiscale_weights=multiscale_weights,\n",
    "            parallelism=True,\n",
    "            cuda=True)\n",
    "scripted_multhrd = torch.jit.script(speaker_clustering_multhrd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda11e5c",
   "metadata": {},
   "source": [
    "Now, run the speaker clustering model with the following line. You can check the clustered labels and estimated number of speakers. The output is also torch.tensor format.\n",
    "\n",
    "Check out the speed gain from parallelism=True. The bigger the size of `sparse_search_volume`, the more the speed gain is. For example `sparse_search_volume=30`, it has approximately 30% speed gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fde3c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Thread ETA: 4.728 sec\n",
      "cluster labels: tensor([0, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "Set of speakers {0, 1}\n",
      "\n",
      "Multi Thread ETA: 2.555 sec\n",
      "cluster labels: tensor([0, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "Set of speakers {0, 1}\n"
     ]
    }
   ],
   "source": [
    "start1 = time.time()\n",
    "cluster_labels = scripted_singthrd.forward(\n",
    "    uniq_scale_dict,\n",
    "    oracle_num_speakers=-1,\n",
    "    )\n",
    "print(f\"\\nSingle Thread ETA: {(time.time()-start1):.3f} sec\")\n",
    "print(\"cluster labels:\", cluster_labels)\n",
    "print(\"Set of speakers\", set(cluster_labels.cpu().numpy().tolist()))\n",
    "\n",
    "start2 = time.time()\n",
    "cluster_labels = scripted_multhrd.forward(\n",
    "    uniq_scale_dict,\n",
    "    oracle_num_speakers=-1,\n",
    "    )\n",
    "print(f\"\\nMulti Thread ETA: {(time.time()-start2):.3f} sec\")\n",
    "\n",
    "print(\"cluster labels:\", cluster_labels)\n",
    "print(\"Set of speakers\", set(cluster_labels.cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838278d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
