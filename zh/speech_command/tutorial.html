

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>教程 &mdash; nemo 0.10.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="数据集" href="datasets.html" />
    <link rel="prev" title="语音指令" href="intro.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.10.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">从这里开始</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">快速训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">语音识别</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">语音指令</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">教程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">入门</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">数据准备</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">训练</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">混合精度训练</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu">多 GPU 训练</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id9">完整的训练案例</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning">微调 (Fine-tuning)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">评估</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="models.html">模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/intro.html">自然语言处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tts/intro.html">语音合成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">语音指令</a> &raquo;</li>
        
      <li>教程</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/speech_command/tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>教程<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>请首先安装 <code class="docutils literal notranslate"><span class="pre">nemo</span></code> 和 <code class="docutils literal notranslate"><span class="pre">nemo_asr</span></code> 集合。
具体的安装步骤请参阅 <a class="reference internal" href="../index.html#installation"><span class="std std-ref">开始吧</span></a> 章节。
另外，本教程还使用 python 包 <cite>torchaudio</cite> 进行语音特征提取。</p>
<div class="section" id="id2">
<h2>入门<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>本教程基于 QuartzNet <a class="bibtex reference internal" href="#speech-recognition-tut-kriman2019quartznet" id="id3">[SPEECH-RECOGNITION-ALL-TUT1]</a> 模型。其中的解码器部分做了些许修改以适配分类任务。</p>
<ol class="arabic simple">
<li><p>音频处理（特征提取）：包括信号归一化，滑窗处理，频谱转换（或者是梅尔频谱 MFCC ）</p></li>
<li><p>使用 SpecAugment <a class="bibtex reference internal" href="#speech-recognition-tut-park2019" id="id4">[SPEECH-RECOGNITION-ALL-TUT2]</a> 进行数据增强，同时这一方法也能增加数据量。</p></li>
<li><p>创建一个小型的神经网络模型进行训练。</p></li>
</ol>
</div>
<div class="section" id="id5">
<h2>数据准备<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>我们使用开源的谷歌语音指令数据集 Google Speech Commands Dataset 。目前，我们使用的是第一版数据集。如果想使用第二版，还需要一些简单的修改。下面的命令可以下载数据并进行相应的格式转换。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir data
<span class="c1"># process_speech_commands_data.py script is located under &lt;nemo_git_repo_root&gt;/scripts</span>
<span class="c1"># The `--rebalance` flag will duplicate elements in the train set so that all classes</span>
<span class="c1"># have the same number of elements. It is not mandatory to add this flag.</span>
python process_speech_commands_data.py --data_root<span class="o">=</span>data --data_version<span class="o">=</span><span class="m">1</span> --rebalance
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>如果使用第一版数据集 <code class="docutils literal notranslate"><span class="pre">--data_version=1</span></code> ，至少需要 4GB 的硬盘空间。如果使用第二版 <code class="docutils literal notranslate"><span class="pre">--data_version=2</span></code> 至少需要 16 GB 的硬盘空间。另外，下载和处理的过程均需要一些时间。</p>
</div>
<p>下载和处理完成后，你会得到一个名为 <cite>data</cite> 文件夹，其中包含另一个文件夹名称为 <cite>google_speech_recognition_v{1/2}</cite> 。
在这个文件夹中会有多个子目录包含很多 wav 文件和三个 json 文件，分别是</p>
<ul class="simple">
<li><p><cite>train_manifest.json</cite></p></li>
<li><p><cite>validation_manifest.json</cite></p></li>
<li><p><cite>test_manifest.json</cite></p></li>
</ul>
<p>json文件的每一行代表一条训练数据。其中， <cite>audio_filepath</cite> 属性是音频文件的地址， <cite>duration</cite> 是音频时间长度， <cite>command</cite> 是音频的标注。</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;absolute path to dataset&gt;/two/8aa35b0c_nohash_0.wav&quot;</span><span class="p">,</span> <span class="nt">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="nt">&quot;command&quot;</span><span class="p">:</span> <span class="s2">&quot;two&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;absolute path to dataset&gt;/two/ec5ab5d5_nohash_2.wav&quot;</span><span class="p">,</span> <span class="nt">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="nt">&quot;command&quot;</span><span class="p">:</span> <span class="s2">&quot;two&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h2>训练<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>我们使用的是 QuartzNet 模型 <a class="bibtex reference internal" href="#speech-recognition-tut-kriman2019quartznet" id="id7">[SPEECH-RECOGNITION-ALL-TUT1]</a> 。
相比于 Jasper 模型, QuartzNet 模型中使用了可分离卷积 (Separable Convolutions) ，大幅减少了参数数量。</p>
<p>QuartzNet 模型使用一种固定的模型定义模式： QuartzNet-[BxR], 其中 B 是模块的数量，R 是卷积子模块的数量。每个子模块包含一个 1D 掩码卷积，批归一化， ReLU 激活和 dropout 。</p>
<blockquote>
<div><img alt="quartznet model" class="align-center" src="../_images/quartz_vertical1.png" />
</div></blockquote>
<p>本教程中我们使用的是 QuartzNet [3x1] 模型。
接下来的脚本会在一个 GPU 上进行训练和评估。</p>
<blockquote>
<div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>借助 Jupyter 笔记本一步一步地运行这个脚本。</p>
</div>
</div></blockquote>
<p><strong>训练脚本</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some utility functions</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">ruamel.yaml</span> <span class="kn">import</span> <span class="n">YAML</span>

<span class="c1"># NeMo&#39;s &quot;core&quot; package</span>
<span class="kn">import</span> <span class="nn">nemo</span>
<span class="c1"># NeMo&#39;s ASR collection</span>
<span class="kn">import</span> <span class="nn">nemo.collections.asr</span> <span class="k">as</span> <span class="nn">nemo_asr</span>
<span class="c1"># NeMo&#39;s learning rate policy</span>
<span class="kn">from</span> <span class="nn">nemo.utils.lr_policies</span> <span class="kn">import</span> <span class="n">CosineAnnealing</span>
<span class="kn">from</span> <span class="nn">nemo.collections.asr.helpers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">monitor_classification_training_progress</span><span class="p">,</span>
    <span class="n">process_classification_evaluation_batch</span><span class="p">,</span>
    <span class="n">process_classification_evaluation_epoch</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logging</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">logging</span>

<span class="c1"># Lets define some hyper parameters</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># Create a Neural Factory</span>
<span class="c1"># It creates log files and tensorboard writers for us among other functions</span>
<span class="n">neural_factory</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">(</span>
    <span class="n">log_dir</span><span class="o">=</span><span class="s1">&#39;./quartznet-3x1-v1&#39;</span><span class="p">,</span>
    <span class="n">create_tb_writer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tb_writer</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">tb_writer</span>

<span class="c1"># Path to our training manifest</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="s2">&quot;&lt;path_to_where_you_put_data&gt;/train_manifest.json&quot;</span>

<span class="c1"># Path to our validation manifest</span>
<span class="n">eval_datasets</span> <span class="o">=</span> <span class="s2">&quot;&lt;path_to_where_you_put_data&gt;/test_manifest.json&quot;</span>

<span class="c1"># Here we will be using separable convolutions</span>
<span class="c1"># with 3 blocks (k=3 repeated once r=1 from the picture above)</span>
<span class="n">yaml</span> <span class="o">=</span> <span class="n">YAML</span><span class="p">(</span><span class="n">typ</span><span class="o">=</span><span class="s2">&quot;safe&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;&lt;nemo_git_repo_root&gt;/examples/asr/configs/quartznet_speech_commands_3x1_v1.yaml&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">jasper_params</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="c1"># Pre-define a set of labels that this model must learn to predict</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">jasper_params</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span>

<span class="c1"># Get the sampling rate of the data</span>
<span class="n">sample_rate</span> <span class="o">=</span> <span class="n">jasper_params</span><span class="p">[</span><span class="s1">&#39;sample_rate&#39;</span><span class="p">]</span>

<span class="c1"># Check if data augmentation such as white noise and time shift augmentation should be used</span>
<span class="n">audio_augmentor</span> <span class="o">=</span> <span class="n">jasper_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;AudioAugmentor&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="c1"># Build the input data layer and the preprocessing layers for the train set</span>
<span class="n">train_data_layer</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">AudioToSpeechLabelDataLayer</span><span class="p">(</span>
    <span class="n">manifest_filepath</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="n">sample_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span>
    <span class="n">augmentor</span><span class="o">=</span><span class="n">audio_augmentor</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

 <span class="c1"># Build the input data layer and the preprocessing layers for the test set</span>
<span class="n">eval_data_layer</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">AudioToSpeechLabelDataLayer</span><span class="p">(</span>
    <span class="n">manifest_filepath</span><span class="o">=</span><span class="n">eval_datasets</span><span class="p">,</span>
    <span class="n">sample_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># We will convert the raw audio data into MFCC Features to feed as input to our model</span>
<span class="n">data_preprocessor</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">AudioToMFCCPreprocessor</span><span class="p">(</span>
    <span class="n">sample_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span> <span class="o">**</span><span class="n">jasper_params</span><span class="p">[</span><span class="s2">&quot;AudioToMFCCPreprocessor&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Compute the total number of samples and the number of training steps per epoch</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data_layer</span><span class="p">)</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">))</span>

<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Steps per epoch : </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">steps_per_epoch</span><span class="p">))</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Have </span><span class="si">{0}</span><span class="s1"> examples to train on.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>

<span class="c1"># Here we begin defining all of the augmentations we want</span>
<span class="c1"># We will pad the preprocessed spectrogram image to have a certain number of timesteps</span>
<span class="c1"># This centers the generated spectrogram and adds black boundaries to either side</span>
<span class="c1"># of the padded image.</span>
<span class="n">crop_pad_augmentation</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">CropOrPadSpectrogramAugmentation</span><span class="p">(</span><span class="n">audio_length</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># We also optionally add `SpecAugment` augmentations based on the config file</span>
<span class="c1"># SpecAugment has various possible augmentations to the generated spectrogram</span>
<span class="c1"># 1) Frequency band masking</span>
<span class="c1"># 2) Time band masking</span>
<span class="c1"># 3) Rectangular cutout</span>
<span class="n">spectr_augment_config</span> <span class="o">=</span> <span class="n">jasper_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;SpectrogramAugmentation&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="k">if</span> <span class="n">spectr_augment_config</span><span class="p">:</span>
    <span class="n">data_spectr_augmentation</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">SpectrogramAugmentation</span><span class="p">(</span><span class="o">**</span><span class="n">spectr_augment_config</span><span class="p">)</span>

<span class="c1"># Build the QuartzNet Encoder model</span>
<span class="c1"># The config defines the layers as a list of dictionaries</span>
<span class="c1"># The first and last two blocks are not considered when we say QuartzNet-[BxR]</span>
<span class="c1"># B is counted as the number of blocks after the first layer and before the penultimate layer.</span>
<span class="c1"># R is defined as the number of repetitions of each block in B.</span>
<span class="c1"># Note: We can scale the convolution kernels size by the float parameter `kernel_size_factor`</span>
<span class="n">jasper_encoder</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">JasperEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">jasper_params</span><span class="p">[</span><span class="s2">&quot;JasperEncoder&quot;</span><span class="p">])</span>

<span class="c1"># We then define the QuartzNet decoder.</span>
<span class="c1"># This decoder head is specialized for the task for classification, such that it</span>
<span class="c1"># accepts a set of `N-feat` per timestep of the model, and averages these features</span>
<span class="c1"># over all the timesteps, before passing a Linear classification layer on those features.</span>
<span class="n">jasper_decoder</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">JasperDecoderForClassification</span><span class="p">(</span>
    <span class="n">feat_in</span><span class="o">=</span><span class="n">jasper_params</span><span class="p">[</span><span class="s2">&quot;JasperEncoder&quot;</span><span class="p">][</span><span class="s2">&quot;jasper&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;filters&quot;</span><span class="p">],</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span>
    <span class="o">**</span><span class="n">jasper_params</span><span class="p">[</span><span class="s1">&#39;JasperDecoderForClassification&#39;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># We can easily apply cross entropy loss to train this model</span>
<span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">CrossEntropyLossNM</span><span class="p">()</span>

<span class="c1"># Lets print out the number of parameters of this model</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;================================&#39;</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters in encoder: </span><span class="si">{jasper_encoder.num_weights}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters in decoder: </span><span class="si">{jasper_decoder.num_weights}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Total number of parameters in model: &quot;</span> <span class="sa">f</span><span class="s2">&quot;{jasper_decoder.num_weights + jasper_encoder.num_weights}&quot;</span>
<span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;================================&#39;</span><span class="p">)</span>

<span class="c1"># Now we have all of the components that are required to build the NeMo execution graph!</span>
<span class="c1">## Build the training data loaders and preprocessors first</span>
<span class="n">audio_signal</span><span class="p">,</span> <span class="n">audio_signal_len</span><span class="p">,</span> <span class="n">commands</span><span class="p">,</span> <span class="n">command_len</span> <span class="o">=</span> <span class="n">train_data_layer</span><span class="p">()</span>
<span class="n">processed_signal</span><span class="p">,</span> <span class="n">processed_signal_len</span> <span class="o">=</span> <span class="n">data_preprocessor</span><span class="p">(</span><span class="n">input_signal</span><span class="o">=</span><span class="n">audio_signal</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">audio_signal_len</span><span class="p">)</span>
<span class="n">processed_signal</span><span class="p">,</span> <span class="n">processed_signal_len</span> <span class="o">=</span> <span class="n">crop_pad_augmentation</span><span class="p">(</span>
    <span class="n">input_signal</span><span class="o">=</span><span class="n">processed_signal</span><span class="p">,</span>
    <span class="n">length</span><span class="o">=</span><span class="n">audio_signal_len</span>
<span class="p">)</span>

<span class="c1">## Augment the dataset for training</span>
<span class="k">if</span> <span class="n">spectr_augment_config</span><span class="p">:</span>
    <span class="n">processed_signal</span> <span class="o">=</span> <span class="n">data_spectr_augmentation</span><span class="p">(</span><span class="n">input_spec</span><span class="o">=</span><span class="n">processed_signal</span><span class="p">)</span>

<span class="c1">## Define the model</span>
<span class="n">encoded</span><span class="p">,</span> <span class="n">encoded_len</span> <span class="o">=</span> <span class="n">jasper_encoder</span><span class="p">(</span><span class="n">audio_signal</span><span class="o">=</span><span class="n">processed_signal</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">processed_signal_len</span><span class="p">)</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">jasper_decoder</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">=</span><span class="n">encoded</span><span class="p">)</span>

<span class="c1">## Obtain the train loss</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">ce_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">decoded</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">commands</span><span class="p">)</span>

<span class="c1"># Now we build the test graph in a similar way, reusing the above components</span>
<span class="c1">## Build the test data loader and preprocess same way as train graph</span>
<span class="c1">## But note, we do not add the spectrogram augmentation to the test graph !</span>
<span class="n">test_audio_signal</span><span class="p">,</span> <span class="n">test_audio_signal_len</span><span class="p">,</span> <span class="n">test_commands</span><span class="p">,</span> <span class="n">test_command_len</span> <span class="o">=</span> <span class="n">eval_data_layer</span><span class="p">()</span>
<span class="n">test_processed_signal</span><span class="p">,</span> <span class="n">test_processed_signal_len</span> <span class="o">=</span> <span class="n">data_preprocessor</span><span class="p">(</span>
    <span class="n">input_signal</span><span class="o">=</span><span class="n">test_audio_signal</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">test_audio_signal_len</span>
<span class="p">)</span>
<span class="n">test_processed_signal</span><span class="p">,</span> <span class="n">test_processed_signal_len</span> <span class="o">=</span> <span class="n">crop_pad_augmentation</span><span class="p">(</span>
    <span class="n">input_signal</span><span class="o">=</span><span class="n">test_processed_signal</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">test_processed_signal_len</span>
<span class="p">)</span>

<span class="c1"># Pass the test data through the model encoder and decoder</span>
<span class="n">test_encoded</span><span class="p">,</span> <span class="n">test_encoded_len</span> <span class="o">=</span> <span class="n">jasper_encoder</span><span class="p">(</span>
    <span class="n">audio_signal</span><span class="o">=</span><span class="n">test_processed_signal</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">test_processed_signal_len</span>
<span class="p">)</span>
<span class="n">test_decoded</span> <span class="o">=</span> <span class="n">jasper_decoder</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">=</span><span class="n">test_encoded</span><span class="p">)</span>

<span class="c1"># Compute test loss for visualization</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="n">ce_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">test_decoded</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">test_commands</span><span class="p">)</span>

<span class="c1"># Now that we have our training and evaluation graphs built,</span>
<span class="c1"># we can focus on a few callbacks to help us save the model checkpoints</span>
<span class="c1"># during training, as well as display train and test metrics</span>

<span class="c1"># Callbacks needed to print train info to console and Tensorboard</span>
<span class="n">train_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">SimpleLossLoggerCallback</span><span class="p">(</span>
    <span class="c1"># Notice that we pass in loss, predictions, and the labels.</span>
    <span class="c1"># Of course we would like to see our training loss, but we need the</span>
    <span class="c1"># other arguments to calculate the accuracy.</span>
    <span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">commands</span><span class="p">],</span>
    <span class="c1"># The print_func defines what gets printed.</span>
    <span class="n">print_func</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">monitor_classification_training_progress</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">get_tb_values</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])],</span>
    <span class="n">tb_writer</span><span class="o">=</span><span class="n">neural_factory</span><span class="o">.</span><span class="n">tb_writer</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Callbacks needed to print test info to console and Tensorboard</span>
<span class="n">tagname</span> <span class="o">=</span> <span class="s1">&#39;TestSet&#39;</span>
<span class="n">eval_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">EvaluatorCallback</span><span class="p">(</span>
    <span class="n">eval_tensors</span><span class="o">=</span><span class="p">[</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_decoded</span><span class="p">,</span> <span class="n">test_commands</span><span class="p">],</span>
    <span class="n">user_iter_callback</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">process_classification_evaluation_batch</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">user_epochs_done_callback</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">process_classification_evaluation_epoch</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="n">tagname</span><span class="p">),</span>
    <span class="n">eval_step</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>  <span class="c1"># How often we evaluate the model on the test set</span>
    <span class="n">tb_writer</span><span class="o">=</span><span class="n">neural_factory</span><span class="o">.</span><span class="n">tb_writer</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Callback to save model checkpoints</span>
<span class="n">chpt_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">CheckpointCallback</span><span class="p">(</span>
    <span class="n">folder</span><span class="o">=</span><span class="n">neural_factory</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span>
    <span class="n">step_freq</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Prepare a list of checkpoints to pass to the engine</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_callback</span><span class="p">,</span> <span class="n">eval_callback</span><span class="p">,</span> <span class="n">chpt_callback</span><span class="p">]</span>

<span class="c1"># Now we have all the components required to train the model</span>
<span class="c1"># Lets define a learning rate schedule</span>

<span class="c1"># Define a learning rate schedule</span>
<span class="n">lr_policy</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span>
    <span class="n">total_steps</span><span class="o">=</span><span class="n">num_epochs</span> <span class="o">*</span> <span class="n">steps_per_epoch</span><span class="p">,</span>
    <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using `</span><span class="si">{lr_policy}</span><span class="s2">` Learning Rate Scheduler&quot;</span><span class="p">)</span>

<span class="c1"># Finally, lets train this model !</span>
<span class="n">neural_factory</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">tensors_to_optimize</span><span class="o">=</span><span class="p">[</span><span class="n">train_loss</span><span class="p">],</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="n">lr_policy</span><span class="o">=</span><span class="n">lr_policy</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;novograd&quot;</span><span class="p">,</span>
    <span class="n">optimization_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="n">num_epochs</span><span class="p">,</span>
        <span class="s2">&quot;max_steps&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span>
        <span class="s2">&quot;momentum&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="s2">&quot;betas&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.98</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="n">weight_decay</span><span class="p">,</span>
        <span class="s2">&quot;grad_norm_clip&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">batches_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>整个训练过程大概需要 100 个 epoch ，在 GTX 1080 GPU 上大概需要 4-5 小时。</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<dl class="simple">
<dt>想要进一步提升准确率，可以尝试下列方法：</dt><dd><ol class="arabic simple">
<li><p>更长时间的训练 (200-300 epochs)</p></li>
<li><p>使用更多的数据</p></li>
<li><p>选择更大的模型</p></li>
<li><p>使用多个 GPU 或者使用混合精度训练</p></li>
<li><p>使用一个预训练的模型</p></li>
</ol>
</dd>
</dl>
</div>
</div>
<div class="section" id="id8">
<h2>混合精度训练<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>可以借助英伟达的 <a class="reference external" href="https://github.com/NVIDIA/apex">APEX 工具包</a> 进行混合精度训练和分布式训练。
要进行混合精度训练，你只需要设置 <cite>optimization_level</cite> 选项为 <cite>nemo.core.Optimization.mxprO1</cite> 。例如：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nf</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">(</span>
    <span class="n">backend</span><span class="o">=</span><span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">PyTorch</span><span class="p">,</span>
    <span class="n">local_rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">,</span>
    <span class="n">optimization_level</span><span class="o">=</span><span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Optimization</span><span class="o">.</span><span class="n">mxprO1</span><span class="p">,</span>
    <span class="n">placement</span><span class="o">=</span><span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">AllGpu</span><span class="p">,</span>
    <span class="n">cudnn_benchmark</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="gpu">
<h2>多 GPU 训练<a class="headerlink" href="#gpu" title="Permalink to this headline">¶</a></h2>
<p>在 NeMo 中进行多 GPU 训练也非常容易：</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>将 <cite>NeuralModuleFactory</cite> 类的 <cite>placement</cite> 选项设置为 <cite>nemo.core.DeviceType.AllGpu</cite></p></li>
<li><p>添加命令行选项 <cite>local_rank</cite> : <cite>parser.add_argument(“–local_rank”, default=None, type=int)</cite></p></li>
<li><p>导入 <cite>torch.distributed.launch</cite> 包并且使用如下的方式运行脚本：</p></li>
</ol>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span>&lt;num_gpus&gt; &lt;nemo_git_repo_root&gt;/examples/asr/quartznet_speech_commands.py ...
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>混合精度训练依赖于 Tensor Cores 硬件单元，所以当前只支持英伟达 Volta 和 Turing 架构 GPU</p>
</div>
<div class="section" id="id9">
<h3>完整的训练案例<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>更详细的一个训练案例请参阅文件 <cite>&lt;nemo_git_repo_root&gt;/examples/asr/quartznet_speech_commands.py</cite> 。
在这个案例中，我们分别构建了训练，评估和测试的计算图。
下面的这条命令会启动8个 GPU 并进行混合精度训练。其中的 json 文件指定了数据集信息。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span>&lt;num_gpus&gt; &lt;nemo_git_repo_root&gt;/examples/asr/quartznet_speech_commands.py --model_config <span class="s2">&quot;&lt;nemo_git_repo_root&gt;/examples/asr/configs/quartznet_speech_commands_3x1_v1.yaml&quot;</span> <span class="se">\</span>
  --train_dataset<span class="o">=</span><span class="s2">&quot;&lt;absolute path to dataset&gt;/train_manifest.json&quot;</span> --eval_datasets <span class="s2">&quot;&lt;absolute path to dataset&gt;/validation_manifest.json&quot;</span> <span class="s2">&quot;&lt;absolute path to dataset&gt;/test_manifest.json&quot;</span> <span class="se">\</span>
  --num_epochs<span class="o">=</span><span class="m">200</span> --batch_size<span class="o">=</span><span class="m">128</span> --eval_batch_size<span class="o">=</span><span class="m">128</span> --eval_freq<span class="o">=</span><span class="m">200</span> --lr<span class="o">=</span><span class="m">0</span>.05 --min_lr<span class="o">=</span><span class="m">0</span>.001 <span class="se">\</span>
  --optimizer<span class="o">=</span><span class="s2">&quot;novograd&quot;</span> --weight_decay<span class="o">=</span><span class="m">0</span>.001 --amp_opt_level<span class="o">=</span><span class="s2">&quot;O1&quot;</span> --warmup_ratio<span class="o">=</span><span class="m">0</span>.05 --hold_ratio<span class="o">=</span><span class="m">0</span>.45 <span class="se">\</span>
  --checkpoint_dir<span class="o">=</span><span class="s2">&quot;./checkpoints/quartznet_speech_commands_checkpoints_3x1_v1/&quot;</span> <span class="se">\</span>
  --exp_name<span class="o">=</span><span class="s2">&quot;./results/quartznet_speech_classification-quartznet-3x1_v1/&quot;</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>你还可以同时输入多个 json 文件，以便在多个数据集上进行训练。例如： <cite>–train_manifest=/manifests/&lt;first dataset&gt;.json,/manifests/&lt;second dataset&gt;.json</cite></p>
</div>
</div>
</div>
<div class="section" id="fine-tuning">
<h2>微调 (Fine-tuning)<a class="headerlink" href="#fine-tuning" title="Permalink to this headline">¶</a></h2>
<p>如果使用一个预训练好的模型，那么训练时间可以被大大缩短：
1. 准备一个预训练模型，包含 jasper_encoder, jasper_decoder 和配置文件。
2. 载入模型权重，使用类似于下面这样的代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">jasper_encoder</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span><span class="s2">&quot;&lt;path_to_checkpoints&gt;/JasperEncoder-STEP-89000.pt&quot;</span><span class="p">)</span>
<span class="n">jasper_decoder</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span><span class="s2">&quot;&lt;path_to_checkpoints&gt;/JasperDecoderForClassification-STEP-89000.pt&quot;</span><span class="p">)</span>
<span class="c1"># in case of distributed training add args.local_rank</span>
<span class="n">jasper_decoder</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span><span class="s2">&quot;&lt;path_to_checkpoints&gt;/JasperDecoderForClassification-STEP-89000.pt&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>微调的时候，最好降低学习率。</p>
</div>
</div>
<div class="section" id="id10">
<h2>评估<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>我们可以下载预训练模型，并用它在谷歌语音指令数据集上检验分类准确率。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>如果你想亲自听一下数据集中的音频，你可以在 notebook 里运行下面的这份代码。</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets add some generic imports.</span>
<span class="c1"># Please note that you will need to install `librosa` for this code</span>
<span class="c1"># To install librosa : Run `!pip install librosa` from the notebook itself.</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">librosa</span>
<span class="kn">import</span> <span class="nn">librosa.display</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">IPython.display</span> <span class="k">as</span> <span class="nn">ipd</span>
<span class="kn">from</span> <span class="nn">ruamel.yaml</span> <span class="kn">import</span> <span class="n">YAML</span>

<span class="c1"># Import nemo and asr collections</span>
<span class="kn">import</span> <span class="nn">nemo</span>
<span class="kn">import</span> <span class="nn">nemo.collections.asr</span> <span class="k">as</span> <span class="nn">nemo_asr</span>

<span class="n">logging</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">logging</span>

<span class="c1"># We add some</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;&lt;path to the data directory&gt;&#39;</span>
<span class="n">data_version</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="s1">&#39;&lt;path to the config file for this model&gt;&#39;</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;&lt;path to the checkpoint directory for this model&gt;&#39;</span>

<span class="n">test_manifest</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;test_manifest.json&quot;</span><span class="p">)</span>

<span class="c1"># Parse the config file provided to us</span>
<span class="c1"># Parse config and pass to model building function</span>
<span class="n">yaml</span> <span class="o">=</span> <span class="n">YAML</span><span class="p">(</span><span class="n">typ</span><span class="o">=</span><span class="s1">&#39;safe&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;******</span><span class="se">\n</span><span class="s2">Loaded config file.</span><span class="se">\n</span><span class="s2">******&quot;</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span>  <span class="c1"># Vocab of tokens</span>
<span class="n">sample_rate</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;sample_rate&#39;</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># Build the evaluation graph</span>
<span class="c1"># Create our NeuralModuleFactory, which will oversee the neural modules.</span>
<span class="n">neural_factory</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">(</span>
    <span class="n">log_dir</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;v</span><span class="si">{data_version}</span><span class="s1">/eval_results/&#39;</span><span class="p">)</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">logger</span>

<span class="n">test_data_layer</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">AudioToSpeechLabelDataLayer</span><span class="p">(</span>
    <span class="n">manifest_filepath</span><span class="o">=</span><span class="n">test_manifest</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="n">sample_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">crop_pad_augmentation</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">CropOrPadSpectrogramAugmentation</span><span class="p">(</span>
    <span class="n">audio_length</span><span class="o">=</span><span class="mi">128</span>
<span class="p">)</span>
<span class="n">data_preprocessor</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">AudioToMFCCPreprocessor</span><span class="p">(</span>
    <span class="n">sample_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span>
    <span class="o">**</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;AudioToMFCCPreprocessor&#39;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Create the Jasper_3x1 encoder as specified, and a classification decoder</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">JasperEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;JasperEncoder&#39;</span><span class="p">])</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">JasperDecoderForClassification</span><span class="p">(</span>
    <span class="n">feat_in</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;JasperEncoder&#39;</span><span class="p">][</span><span class="s1">&#39;jasper&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;filters&#39;</span><span class="p">],</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span>
    <span class="o">**</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;JasperDecoderForClassification&#39;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">CrossEntropyLossNM</span><span class="p">()</span>

<span class="c1"># Assemble the DAG components</span>
<span class="n">test_audio_signal</span><span class="p">,</span> <span class="n">test_audio_signal_len</span><span class="p">,</span> <span class="n">test_commands</span><span class="p">,</span> <span class="n">test_command_len</span> <span class="o">=</span> <span class="n">test_data_layer</span><span class="p">()</span>

<span class="n">test_processed_signal</span><span class="p">,</span> <span class="n">test_processed_signal_len</span> <span class="o">=</span> <span class="n">data_preprocessor</span><span class="p">(</span>
    <span class="n">input_signal</span><span class="o">=</span><span class="n">test_audio_signal</span><span class="p">,</span>
    <span class="n">length</span><span class="o">=</span><span class="n">test_audio_signal_len</span>
<span class="p">)</span>

<span class="c1"># --- Crop And Pad Augment --- #</span>
<span class="n">test_processed_signal</span><span class="p">,</span> <span class="n">test_processed_signal_len</span> <span class="o">=</span> <span class="n">crop_pad_augmentation</span><span class="p">(</span>
    <span class="n">input_signal</span><span class="o">=</span><span class="n">test_processed_signal</span><span class="p">,</span>
    <span class="n">length</span><span class="o">=</span><span class="n">test_processed_signal_len</span>
<span class="p">)</span>

<span class="n">test_encoded</span><span class="p">,</span> <span class="n">test_encoded_len</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span>
    <span class="n">audio_signal</span><span class="o">=</span><span class="n">test_processed_signal</span><span class="p">,</span>
    <span class="n">length</span><span class="o">=</span><span class="n">test_processed_signal_len</span>
<span class="p">)</span>

<span class="n">test_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
    <span class="n">encoder_output</span><span class="o">=</span><span class="n">test_encoded</span>
<span class="p">)</span>

<span class="n">test_loss</span> <span class="o">=</span> <span class="n">ce_loss</span><span class="p">(</span>
    <span class="n">logits</span><span class="o">=</span><span class="n">test_decoded</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">test_commands</span>
<span class="p">)</span>

<span class="c1"># We import the classification accuracy metric to compute Top-1 accuracy</span>
<span class="kn">from</span> <span class="nn">nemo.collections.asr.metrics</span> <span class="kn">import</span> <span class="n">classification_accuracy</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># --- Inference Only --- #</span>
<span class="c1"># We&#39;ve already built the inference DAG above, so all we need is to call infer().</span>
<span class="n">evaluated_tensors</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span>
    <span class="c1"># These are the tensors we want to get from the model.</span>
    <span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_decoded</span><span class="p">,</span> <span class="n">test_commands</span><span class="p">],</span>
    <span class="c1"># checkpoint_dir specifies where the model params are loaded from.</span>
    <span class="n">checkpoint_dir</span><span class="o">=</span><span class="n">model_path</span>
    <span class="p">)</span>

<span class="c1"># Let us count the total number of incorrect classifications by this model</span>
<span class="n">correct_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">evaluated_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">evaluated_tensors</span><span class="p">[</span><span class="mi">2</span><span class="p">])):</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">classification_accuracy</span><span class="p">(</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Select top 1 accuracy only</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Since accuracy here is &quot;per batch&quot;, we simply denormalize it by multiplying</span>
    <span class="c1"># by batch size to recover the count of correct samples.</span>
    <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">acc</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">total_count</span> <span class="o">+=</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total correct / Total count : </span><span class="si">{correct_count}</span><span class="s2"> / </span><span class="si">{total_count}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final accuracy : {correct_count / float(total_count)}&quot;</span><span class="p">)</span>

<span class="c1"># Let us now filter out the incorrectly labeled samples from the total set of samples in the test set</span>

<span class="c1"># First lets create a utility class to remap the integer class labels to actual string label</span>
<span class="k">class</span> <span class="nc">ReverseMapLabel</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_layer</span><span class="p">:</span> <span class="n">nemo_asr</span><span class="o">.</span><span class="n">AudioToSpeechLabelDataLayer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label2id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">data_layer</span><span class="o">.</span><span class="n">_dataset</span><span class="o">.</span><span class="n">label2id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">data_layer</span><span class="o">.</span><span class="n">_dataset</span><span class="o">.</span><span class="n">id2label</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred_idx</span><span class="p">,</span> <span class="n">label_idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">pred_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">label_idx</span><span class="p">]</span>

<span class="c1"># Next, lets get the indices of all the incorrectly labeled samples</span>
<span class="n">sample_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">incorrect_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rev_map</span> <span class="o">=</span> <span class="n">ReverseMapLabel</span><span class="p">(</span><span class="n">test_data_layer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">evaluated_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">evaluated_tensors</span><span class="p">[</span><span class="mi">2</span><span class="p">])):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">probas</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">incorrect_ids</span> <span class="o">=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">!=</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">incorrect_ids</span><span class="p">:</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">probas</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">label</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">sample_idx</span>

        <span class="n">incorrect_preds</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="o">*</span><span class="n">rev_map</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">),</span> <span class="n">proba</span><span class="p">))</span>

    <span class="n">sample_idx</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Num test samples : </span><span class="si">{total_count}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Num errors : {len(incorrect_preds)}&quot;</span><span class="p">)</span>

<span class="c1"># First lets sort by confidence of prediction</span>
<span class="n">incorrect_preds</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">incorrect_preds</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Lets print out the (test id, predicted label, ground truth label, confidence)</span>
<span class="c1"># tuple of first 20 incorrectly labeled samples</span>
<span class="k">for</span> <span class="n">incorrect_sample</span> <span class="ow">in</span> <span class="n">incorrect_preds</span><span class="p">[:</span><span class="mi">20</span><span class="p">]:</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">incorrect_sample</span><span class="p">))</span>

<span class="c1"># Lets define a threshold below which we designate a model&#39;s prediction as &quot;low confidence&quot;</span>
<span class="c1"># and then filter out how many such samples exist</span>
<span class="n">low_confidence_threshold</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="n">count_low_confidence</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">low_confidence_threshold</span><span class="p">,</span> <span class="n">incorrect_preds</span><span class="p">)))</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of low confidence predictions : </span><span class="si">{count_low_confidence}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># One interesting observation is to actually listen to these samples whose predicted labels were incorrect</span>
<span class="c1"># Note: The following requires the use of a Notebook environment</span>

<span class="c1"># First lets create a helper function to parse the manifest files</span>
<span class="k">def</span> <span class="nf">parse_manifest</span><span class="p">(</span><span class="n">manifest</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">manifest</span><span class="p">:</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="c1"># Now lets load the test manifest into memory</span>
<span class="n">test_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">test_manifest</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">test_f</span><span class="p">:</span>
    <span class="n">test_samples</span> <span class="o">=</span> <span class="n">test_f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>

<span class="n">test_samples</span> <span class="o">=</span> <span class="n">parse_manifest</span><span class="p">(</span><span class="n">test_samples</span><span class="p">)</span>

<span class="c1"># Next, lets create a helper function to actually listen to certain samples</span>
<span class="k">def</span> <span class="nf">listen_to_file</span><span class="p">(</span><span class="n">sample_id</span><span class="p">,</span> <span class="n">pred</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">proba</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Load the audio waveform using librosa</span>
    <span class="n">filepath</span> <span class="o">=</span> <span class="n">test_samples</span><span class="p">[</span><span class="n">sample_id</span><span class="p">][</span><span class="s1">&#39;audio_filepath&#39;</span><span class="p">]</span>
    <span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span> <span class="o">=</span> <span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">label</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">proba</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample : </span><span class="si">{sample_id}</span><span class="s2"> Prediction : </span><span class="si">{pred}</span><span class="s2"> Label : </span><span class="si">{label}</span><span class="s2"> Confidence = </span><span class="si">{proba: 0.4f}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample : </span><span class="si">{sample_id}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ipd</span><span class="o">.</span><span class="n">Audio</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">)</span>

<span class="c1"># Finally, lets listen to all the audio samples where the model made a mistake</span>
<span class="c1"># Note: This list of incorrect samples may be quite large, so you may choose to subsample `incorrect_preds`</span>
<span class="k">for</span> <span class="n">sample_id</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">proba</span> <span class="ow">in</span> <span class="n">incorrect_preds</span><span class="p">:</span>
    <span class="n">ipd</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">listen_to_file</span><span class="p">(</span><span class="n">sample_id</span><span class="p">,</span> <span class="n">pred</span><span class="o">=</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">proba</span><span class="o">=</span><span class="n">proba</span><span class="p">))</span>  <span class="c1"># Needs to be run in a notebook environment</span>
</pre></div>
</div>
</div>
<div class="section" id="id11">
<h2>参考<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-speech_command/tutorial-0"><dl class="citation">
<dt class="bibtex label" id="speech-recognition-tut-kriman2019quartznet"><span class="brackets">SPEECH-RECOGNITION-ALL-TUT1</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, and Yang Zhang. Quartznet: deep automatic speech recognition with 1d time-channel separable convolutions. <em>arXiv preprint arXiv:1910.10261</em>, 2019.</p>
</dd>
<dt class="bibtex label" id="speech-recognition-tut-park2019"><span class="brackets"><a class="fn-backref" href="#id4">SPEECH-RECOGNITION-ALL-TUT2</a></span></dt>
<dd><p>Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. <em>arXiv e-prints</em>, 2019. <a class="reference external" href="https://arxiv.org/abs/1904.08779">arXiv:1904.08779</a>.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="datasets.html" class="btn btn-neutral float-right" title="数据集" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="intro.html" class="btn btn-neutral float-left" title="语音指令" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>